{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6e5af8",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing the SARSA(λ) algorithm, which is an on-policy temporal difference method that utilizes the concept of averaging over all $n$-step returns. Every $n$-step return from $0~to~\\infty$ is considered, and the returns are incrementally weighted by a factor of $\\lambda$ each time-step, normalized by $(1-\\lambda)$.\n",
    "\n",
    "Similar to other implementations, this will be done using the Frozen Lake environment offered through Gymnasium, which is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6ed90",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96eb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import these packages:\n",
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1f253",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02596a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2474448226.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef __init__(self, env: gym.Env, gamma: float, alpha: float, lambda: float, beta: float, es: bool, rs: bool):\u001b[39m\n                                                                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# SARSA(λ)-Agent Class:\n",
    "class SARSA_L_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, env: gym.Env, gamma: float, alpha: float, lamb: float, beta: float, es: bool, rs: bool):\n",
    "        \"\"\"\n",
    "        this is the constructor for the agent. this agent is a TD-based agent, implementing SARSA(λ), meaning that the policy\n",
    "        is evaluated and improved every time-step by examining all n-step returns\n",
    "\n",
    "        env:    a gymnasium environment\n",
    "        gamma:  a float value indicating the discount factor\n",
    "        alpha:  a float value indicating the learning rate\n",
    "        lamb: a float value indicating the trace decay rate, λ\n",
    "        beta:   a float value indicating the decay rate of ε\n",
    "        es:     a boolean value indicating whether to use exploring starts or not\n",
    "        rs:     a boolean value indicating whether to use reward shaping or not\n",
    "                    if true:\n",
    "                        goal_value: +10.0\n",
    "                        hole_value: -1.0\n",
    "                    else:\n",
    "                        goal_value: +1.0\n",
    "                        hole_value: 0.0 (sparsely defined)\n",
    "        Q:      the estimate of the action-value function q, initialized as zeroes over all states and actions\n",
    "        E:      the eligibility trace, initialized as zeroes over all states and actions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lamb = lamb\n",
    "        self.beta = beta\n",
    "        self.es = es\n",
    "        self.rs = rs\n",
    "\n",
    "        # set the reward shaping:\n",
    "        if self.rs:\n",
    "            self.goal_value = 10.0\n",
    "            self.hole_value = -1.0\n",
    "        else:\n",
    "            self.goal_value = 1.0\n",
    "            self.hole_value = 0.0\n",
    "\n",
    "        # get the number of states, number of actions:\n",
    "        nS, nA = env.observation_space.n, env.action_space.n\n",
    "\n",
    "        # get the terminal spaces of the current map:\n",
    "        desc = env.unwrapped.desc.astype(\"U1\")\n",
    "        chars = desc.flatten()\n",
    "        self.terminal_states = [i for i, c in enumerate(chars) if c in (\"H\", \"G\")]\n",
    "\n",
    "        # tabular Q values:\n",
    "        self.Q = np.zeros((nS, nA))\n",
    "\n",
    "        # eligibility trace:\n",
    "        self.E = np.zeros((nS, nA))\n",
    "\n",
    "        # return to the user the metrics about the environment:\n",
    "        print(f\"Action Space is: {env.action_space}\")\n",
    "        print(f\"Observation Space is: {env.observation_space}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
