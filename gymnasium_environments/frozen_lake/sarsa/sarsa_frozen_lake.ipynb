{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f62dab6",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing the base SARSA algorithm, an on-policy temporal difference method. Similar to other implementations, this will be done using the Frozen Lake environment offered through Gymnasium. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API. The `Env` class is leveraged within Gymnasium to encapsulate an environment and the Markov Decision Process (MDP) that runs behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb93cfe",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4c914",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA-Agent Class:\n",
    "class SARSA_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, env: gym.Env, gamma: float, alpha: float, es: bool, rs: bool):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent is a TD-based agent, implementing SARSA, which means that the \n",
    "        policy is evaluated and improved every time-step\n",
    "\n",
    "        env:    a gymnasium environment\n",
    "        gamma:  a float value indicating the discount factor\n",
    "        alpha:  a float value indicating the learning rate\n",
    "        es:     a boolean value indicating whether to use exploring starts or not\n",
    "        rs:     a boolean value indicating whether to use reward shaping or not\n",
    "                    if true:\n",
    "                        goal_value: +10.0\n",
    "                        hole_value: -1.0\n",
    "                    else:\n",
    "                        goal_value: +1.0\n",
    "                        hole_value: 0.0 (sparsely defined)\n",
    "        Q:      the estimate of the action-value function q, initialized as zeros over all states and actions\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.es = es\n",
    "        self.rs = rs\n",
    "\n",
    "        # set the reward shaping:\n",
    "        if self.rs:\n",
    "            self.goal_value = 10.0\n",
    "            self.hole_value = -1.0\n",
    "        else:\n",
    "            self.goal_value = 1.0\n",
    "            self.hole_value = 0.0\n",
    "\n",
    "        # get the number of states, number of actions:\n",
    "        nS, nA = env.observation_space.n, env.action_space.n\n",
    "\n",
    "        # get the terminal spaces of the current map:\n",
    "        desc = env.unwrapped.desc.astype(\"U1\")\n",
    "        chars = desc.flatten()\n",
    "        self.terminal_states = [i for i, c in enumerate(chars) if c in (\"H\", \"G\")]\n",
    "\n",
    "        # tabular Q-values:\n",
    "        self.Q = np.zeros((nS, nA))\n",
    "\n",
    "        # return to the user the metrics about the environment:\n",
    "        print(f\"Action Space is: {env.action_space}\")\n",
    "        print(f\"Observation Space is: {env.observation_space}\")\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # function to perform ε-greedy probability assignment\n",
    "    def get_action_probs(self, Q):\n",
    "        \"\"\" \n",
    "        this function does the ε-greedy probability assignment for the actions available in a given state\n",
    "\n",
    "        Q:          a np.ndarray corresponding to the action-values of the actions available in a given state\n",
    "        returns:    probability of selecting each action\n",
    "\n",
    "        \"\"\"\n",
    "        # get the number of available actions:\n",
    "        m = len(Q)\n",
    "\n",
    "        # assign each action a base probability of ε/m\n",
    "        p = np.ones(m)*(self.epsilon/m)\n",
    "\n",
    "        # find the index of the best Q value\n",
    "        best = np.argmax(Q)\n",
    "\n",
    "        # give that one more probability by an amount equal to (1 - ε):\n",
    "        p[best] += 1.0 - self.epsilon\n",
    "\n",
    "        # this way the \"best\" action has a probability of ε/m + (1-ε), meaning it will be chosen more often\n",
    "        # whereas the others have a probability of ε/m, so there is a probability that exploratory actions will be selected\n",
    "\n",
    "        # return the probability of selecting each action:\n",
    "        return p\n",
    "\n",
    "    # ε-greedy policy function:\n",
    "    def policy(self, state):\n",
    "        \"\"\" \n",
    "        this is the ε-greedy policy itself, where it chooses an action based on the ε-greedy probabilities of each action\n",
    "\n",
    "        state:      an int representing the current state\n",
    "        returns:    a randomly selected action\n",
    "\n",
    "        \"\"\"\n",
    "        probs = self.get_action_probs(self.Q[state])    # for a given state, or row in Q\n",
    "        return np.random.choice(len(probs), p = probs)  # pick an action from the probabilities of each action\n",
    "\n",
    "    # GPI function using SARSA rule and ε-greedy policy:\n",
    "    def GPI(self, num_episodes):\n",
    "        \"\"\"\n",
    "        this function performs the generalized policy iteration using SARSA as the evaluation modality and\n",
    "        ε-greedy policy improvement to improve the policy\n",
    "\n",
    "        num_episodes:   number of desired episodes to train the agent on\n",
    "        returns:        the updated Q values\n",
    "\n",
    "        \"\"\"\n",
    "        for k in tqdm(range(num_episodes)):\n",
    "            # decay ε:\n",
    "            self.epsilon = 1.0 / (0.005*k + 1)\n",
    "\n",
    "            # if exploring starts:\n",
    "            if self.es:\n",
    "                non_terminals = [s for s in range(self.env.observation_space.n) if s not in self.terminal_states]\n",
    "                starting_state = np.random.choice(non_terminals)\n",
    "\n",
    "                # force env into starting state:\n",
    "                _, _ = self.env.reset()\n",
    "                self.env.unwrapped.s = starting_state\n",
    "                obs = starting_state\n",
    "            else:\n",
    "                obs = self.env.reset()\n",
    "            \n",
    "            # take an initial ε-greedy action:\n",
    "            action = self.policy(obs)\n",
    "\n",
    "            # flag for finishing\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                next_obs, r, term, trunc = self.env.step(action)    # take an action, get a new state and a reward\n",
    "                next_action = self.policy(next_obs)                 # pick next action based on ε-greedy policy\n",
    "\n",
    "                # if reward shaping:\n",
    "                if self.rs:\n",
    "                    if term and r == 0:\n",
    "                        r = self.hole_value     # fell in a hole\n",
    "                    elif term and r == 1:\n",
    "                        r = self.goal_value     # reached goal\n",
    "\n",
    "                # update Q using SARSA update rule:\n",
    "                self.Q[obs, action] += self.alpha * (r + self.gamma*self.Q[next_obs, next_action] - self.Q[obs, action])\n",
    "\n",
    "                # advance state and action indices:\n",
    "                obs, action = next_obs, next_action\n",
    "\n",
    "                # check for completion:\n",
    "                done = term or trunc\n",
    "\n",
    "        return self.Q\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
