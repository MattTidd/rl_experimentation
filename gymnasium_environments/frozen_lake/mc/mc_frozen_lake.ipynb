{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a431a9c5",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a Monte-Carlo reinforcement learning method on the Frozen Lake environment offered through a Gymnasium environment. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API. There are four key functions to Gymnasium, namely: ```make()```, ```Env.reset()```, ```Env.step()```, and ```Env.render()```.\n",
    "\n",
    "As per its [introductory documentation](https://gymnasium.farama.org/introduction/basic_usage/), the core of Gymnasium lies in the high-level Python class ```Env```, which approximately represents a Markov Decision Process (MDP) from reinforcement learning theory. This class allows users of Gymnasium to start new episodes, take actions, and visualize the agent's current state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48474e",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9cc2a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inclusions:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47facdbe",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83987ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space is: Discrete(4)\n",
      "Observation Space is: Discrete(16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.625, 0.125, 0.125, 0.125])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MC-Agent Class:\n",
    "class GLIE_MC_Agent:\n",
    "        # constructor:\n",
    "        def __init__(self, env: gym.Env, epsilon: float, gamma: float):\n",
    "                \"\"\"\n",
    "                this is the constructor for the agent. this agent is a monte-carlo agent, meaning that it averages the returns\n",
    "                for each Q(s,a) at the end of the episode.\n",
    "\n",
    "                env: a gymnasium environment\n",
    "                epsilon: a float value indicating the probability of action selection\n",
    "                gamma: a float value indicating the discounting factor\n",
    "                Q: the estimate of the action-value function q, initialized as zeros over all states and actions\n",
    "                \n",
    "                \"\"\"\n",
    "                # object parameters:\n",
    "                self.env = env\n",
    "                self.epsilon = epsilon\n",
    "                self.gamma = gamma\n",
    "\n",
    "                # get the number of states, number of actions:\n",
    "                nS, nA = env.observation_space.n, env.action_space.n\n",
    "\n",
    "                # tabular Q-values, and counter N(s,a):\n",
    "                self.Q = np.zeros((nS, nA))\n",
    "                self.returns_count = np.zeros((nS, nA), dtype = int)         # how many times I have been to a state, and taken an action         \n",
    "\n",
    "                # return to the user to metrics about the environment:\n",
    "                print(f\"Action Space is: {env.action_space}\")\n",
    "                print(f\"Observation Space is: {env.observation_space}\\n\")\n",
    "\n",
    "        def get_action_probs(self, Q):\n",
    "                # get the number of available actions:\n",
    "                m = Q.shape[1]\n",
    "\n",
    "                # assign each action a base probability of e/m\n",
    "                p = np.ones(m)*(self.epsilon/m)\n",
    "\n",
    "                # find the index of the best Q value\n",
    "                best = np.argmax(Q)\n",
    "\n",
    "                # give that one more probability by an amount equal to (1 - e):\n",
    "                p[best] += 1.0 - self.epsilon\n",
    "\n",
    "                # return the probability of selecting each action:\n",
    "                return p\n",
    "        \n",
    "        def policy(self, state):\n",
    "                probs = self.get_action_probs(self.Q[state])\n",
    "\n",
    "# create training environment:\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode = \"human\")\n",
    "agent = GLIE_MC_Agent(env = env, epsilon = 0.5, gamma = 0.99)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
