{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a431a9c5",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a Monte-Carlo reinforcement learning method on the Frozen Lake environment offered through the Gymnasium environment. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API. There are four key functions to Gymnasium, namely: ```make()```, ```Env.reset()```, ```Env.step()```, and ```Env.render()```.\n",
    "\n",
    "As per its [introductory documentation](https://gymnasium.farama.org/introduction/basic_usage/), the core of Gymnasium lies in the high-level Python class ```Env```, which approximately represents a Markov Decision Process (MDP) from reinforcement learning theory. This class allows users of Gymnasium to start new episodes, take actions, and visualize the agent's current state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48474e",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cc2a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47facdbe",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83987ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC-Agent Class:\n",
    "class GLIE_MC_Agent:\n",
    "        ####################### INITIALIZATION #######################\n",
    "        # constructor:\n",
    "        def __init__(self, env: gym.Env, gamma: float, beta: float, es : bool, rs: bool):\n",
    "                \"\"\"\n",
    "                this is the constructor for the agent. this agent is a monte-carlo agent, meaning that it averages the returns\n",
    "                for each Q(s,a) at the end of the episode\n",
    "\n",
    "                env:    a gymnasium environment\n",
    "                gamma:  a float value indicating the discounting factor\n",
    "                beta:   a float value indicating the decay rate of ε\n",
    "                es:     a boolean value indicating whether to use exploring starts or not\n",
    "                rs:     a boolean value indicating whether to use custom rewards or not\n",
    "                                if true:\n",
    "                                        goal_value:     +10.0\n",
    "                                        hole_value:     -1.0\n",
    "                                else:\n",
    "                                        goal_value:     +1.0\n",
    "                                        hole_value:     0.0 (sparsely defined)\n",
    "                Q:      the estimate of the action-value function q, initialized as zeros over all states and actions\n",
    "                \n",
    "                \"\"\"\n",
    "                # object parameters:\n",
    "                self.env = env\n",
    "                self.gamma = gamma\n",
    "                self.beta = beta\n",
    "                self.es = es\n",
    "                self.rs = rs\n",
    "\n",
    "                # reward shaping:\n",
    "                if self.rs:\n",
    "                        self.goal_value = 10.0\n",
    "                        self.hole_value = -1.0\n",
    "                else:\n",
    "                        self.goal_value = 1.0\n",
    "                        self.hole_value = 0.0\n",
    "\n",
    "                # get the number of states, number of actions:\n",
    "                nS, nA = env.observation_space.n, env.action_space.n\n",
    "\n",
    "                # get the terminal spaces of the current map:\n",
    "                desc = env.unwrapped.desc.astype('U1')\n",
    "                chars = desc.flatten()\n",
    "                self.terminal_states = [i for i, c in enumerate(chars) if c in ('H','G')]\n",
    "\n",
    "                # tabular Q-values, and counter N(s,a):\n",
    "                self.Q = np.zeros((nS, nA))\n",
    "                self.visits = np.zeros((nS, nA), dtype = int)         # how many times I have been to a state, and taken an action         \n",
    "\n",
    "                # return to the user the metrics about the environment:\n",
    "                print(f\"Action Space is: {env.action_space}\")\n",
    "                print(f\"Observation Space is: {env.observation_space}\\n\")\n",
    "        \n",
    "        ####################### TRAINING #######################\n",
    "        # function to perform ε-greedy probability assignment:\n",
    "        def get_action_probs(self, Q):\n",
    "                \"\"\" \n",
    "                this function does the ε-greedy probability assignment for the actions available in a given state\n",
    "\n",
    "                Q:         a np.ndarray corresponding to the action-values of the actions available in a given state\n",
    "                returns:   probability of selecting each action\n",
    "                \n",
    "                \"\"\"\n",
    "                # get the number of available actions:\n",
    "                m = len(Q)\n",
    "\n",
    "                # assign each action a base probability of ε/m\n",
    "                p = np.ones(m)*(self.epsilon/m)\n",
    "\n",
    "                # find the index of the best Q value\n",
    "                best = np.argmax(Q)\n",
    "\n",
    "                # give that one more probability by an amount equal to (1 - ε):\n",
    "                p[best] += 1.0 - self.epsilon\n",
    "\n",
    "                # this way the \"best\" action has a probability of ε/m + (1-ε), meaning it will be chosen more often\n",
    "                # whereas the others have a probability of ε/m, so there is a probability that exploratory actions will be selected\n",
    "\n",
    "                # return the probability of selecting each action:\n",
    "                return p\n",
    "        \n",
    "        # ε-greedy policy function:\n",
    "        def policy(self, state):\n",
    "                \"\"\" \n",
    "                this is the ε-greedy policy itself, where it chooses an action based on the ε-greedy probabilities of each action\n",
    "\n",
    "                state:   an int representing the current state\n",
    "                returns: a randomly selected action\n",
    "\n",
    "                \"\"\"\n",
    "                probs = self.get_action_probs(self.Q[state])    # for a given state, or row in Q\n",
    "                return np.random.choice(len(probs), p = probs)  # pick an action from the probabilities of each action\n",
    "        \n",
    "        # episode generation function:\n",
    "        def generate_episode(self):\n",
    "                \"\"\" \n",
    "                this function is used to generate and run through episodes\n",
    "\n",
    "                returns: a list of (obs, a, r) tuples\n",
    "\n",
    "                \"\"\"\n",
    "                episode = []    # empty list for returns\n",
    "\n",
    "                # exploring starts:\n",
    "                if self.es:\n",
    "                        non_terminals = [s for s in range(self.env.observation_space.n) if s not in self.terminal_states]\n",
    "                        starting_state = np.random.choice(non_terminals)\n",
    "\n",
    "                        # force env into starting state:\n",
    "                        _, _ = self.env.reset()\n",
    "                        self.env.unwrapped.s = starting_state \n",
    "                        obs = starting_state\n",
    "                else:\n",
    "                        obs, _ = self.env.reset()\n",
    "\n",
    "                # flag for when to stop episode\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                        a = self.policy(obs)    # select an action based on the current state\n",
    "\n",
    "                        next_obs, r, term, trunc, _ = self.env.step(a)    # take the action\n",
    "\n",
    "                        # custom reward shaping:\n",
    "                        if self.rs:\n",
    "                                if term and r == 0:\n",
    "                                        r = self.hole_value  # fell in hole\n",
    "                                elif term and r == 1:\n",
    "                                        r = self.goal_value  # reached goal\n",
    "\n",
    "                        episode.append((obs, a, r))     # trajectories are given by {S_1, A_1, R_2, ... , S_T} \n",
    "                        obs = next_obs          # advance the state\n",
    "                        done = term or trunc    # set done to True if term (terminal state) or trunc (environment cut episode short)\n",
    "\n",
    "                # return episode information for GPI\n",
    "                return episode\n",
    "        \n",
    "        def update_Q(self, episode):\n",
    "                \"\"\" \n",
    "                this function updates the Q estimation using incremental every-visit MC at the end of an episode\n",
    "\n",
    "                episode: the (s, a, r) list\n",
    "\n",
    "                \"\"\"\n",
    "                g = 0   # initial return value\n",
    "                for (s, a, r) in reversed(episode):\n",
    "                        g = r + self.gamma * g  # get the return for that state\n",
    "                        self.visits[s, a] += 1  # increment visits\n",
    "                        n = self.visits[s, a]   # visit counter is used in the update rule\n",
    "\n",
    "                        # update Q(s, a) based on update rule:\n",
    "                        self.Q[s, a] += (g - self.Q[s, a]) / n\n",
    "        \n",
    "        # actual policy iteration:\n",
    "        def GPI(self, num_episodes):\n",
    "                \"\"\" \n",
    "                this function performs the generalized policy iteration, using GLIE evaluation and ε-greedy policy improvement\n",
    "\n",
    "                num_episode: number of episodes to play out\n",
    "                returns:     the updated Q values\n",
    "                \n",
    "                \"\"\"\n",
    "                for k in tqdm(range(num_episodes), colour = \"#33FF00\", ncols = 100):\n",
    "                        # GLIE uses a decaying ε schedule:\n",
    "                        self.epsilon = np.exp(-self.beta*k)\n",
    "\n",
    "                        # 1) play out an entire episode:\n",
    "                        episode = self.generate_episode()\n",
    "\n",
    "                        # 2) perform incremental, every-visit MC after the episode to approximate Q(s,a):\n",
    "                        self.update_Q(episode)\n",
    "\n",
    "                return self.Q\n",
    "        \n",
    "        ####################### EVALUATION #######################\n",
    "        # average return per episode:\n",
    "        def average_return(self, agent, num_episodes):\n",
    "                \"\"\" \n",
    "                this function computes the average return per episode for a given amount of episodes\n",
    "\n",
    "                agent:          the agent that has been trained\n",
    "                num_episode:    number of episodes to play out\n",
    "                returns:        the average return per episode\n",
    "                \n",
    "                \"\"\"\n",
    "                # initialize the total return received over the evaluation:\n",
    "                total_return = 0\n",
    "\n",
    "                # for every episode:\n",
    "                for _ in tqdm(range(num_episodes), colour = \"#33FF00\", ncols = 100):\n",
    "                        obs, _ = agent.env.reset()      # must reset before an episode\n",
    "                        done = False                    # flag is set to False initially\n",
    "                        episode_return = 0              # reset return for the episode\n",
    "\n",
    "                        # while False:\n",
    "                        while not done:\n",
    "                                a = np.argmax(agent.Q[obs])                     # pick best action from policy\n",
    "                                obs, r, term, trunc, _ = agent.env.step(a)      # step that action\n",
    "                                episode_return += r     # increment the episode return by that return\n",
    "                                done = term or trunc    # set to True if term or trunc\n",
    "                        \n",
    "                        total_return += episode_return  # increment total return by episode return\n",
    "                \n",
    "                return round(total_return / num_episodes, 3)      # average return accross all episodes\n",
    "        \n",
    "        # success rate:\n",
    "        def success_rate(self, agent, num_episodes):\n",
    "                \"\"\" \n",
    "                this function computes the success rate for a given amount of episodes\n",
    "\n",
    "                agent:          the agent that has been trained\n",
    "                num_episode:    number of episodes to play out\n",
    "                returns:        the success rate for that stretch of episodes\n",
    "                \n",
    "                \"\"\"\n",
    "                # initialize number of successes:\n",
    "                success = 0\n",
    "\n",
    "                # for every episode:\n",
    "                for _ in tqdm(range(num_episodes), colour = \"#33FF00\", ncols = 100):\n",
    "                        obs, _ = agent.env.reset()      # must reset before an episode\n",
    "                        done = False                    # flag is set to False initially\n",
    "\n",
    "                        # while False:\n",
    "                        while not done:\n",
    "                                a = np.argmax(agent.Q[obs])                     # pick best action from policy\n",
    "                                obs, r, term, trunc, _ = agent.env.step(a)      # step that action\n",
    "                                done = term or trunc    # set to True if term or trunc\n",
    "\n",
    "                        # if at the goal pose\n",
    "                        if r == self.goal_value:\n",
    "                                success += 1    # increment the success counter\n",
    "\n",
    "                return round((success / num_episodes) * 100, 3)   # return success rate\n",
    "        \n",
    "        # average episode length:\n",
    "        def average_length(self, agent, num_episodes):\n",
    "                \"\"\" \n",
    "                this function computes the average episode length for a given amount of episodes\n",
    "\n",
    "                agent:          the agent that has been trained\n",
    "                num_episodes:   number of episodes to play out\n",
    "                returns:        the average episode length for that stretch of episodes\n",
    "                \n",
    "                \"\"\"\n",
    "                # initialize the total number of steps over the evaluation:\n",
    "                total_steps = 0\n",
    "                \n",
    "                # for every episode:\n",
    "                for _ in tqdm(range(num_episodes), colour = \"#33FF00\", ncols = 100):\n",
    "                        obs, _ = agent.env.reset()      # must reset before an episode\n",
    "                        done = False                    # flag is set to False initially\n",
    "                        episode_steps = 0               # reset steps for the episode\n",
    "\n",
    "                        # while False:\n",
    "                        while not done:\n",
    "                                a = np.argmax(agent.Q[obs])                     # pick best action from policy\n",
    "                                obs, _, term, trunc, _ = agent.env.step(a)      # step that action\n",
    "                                episode_steps += 1                              # increment episode steps\n",
    "\n",
    "                                done = term or trunc    # set to True if term or trunc\n",
    "\n",
    "                        total_steps += episode_steps    # increment total steps by steps taken in episode\n",
    "                \n",
    "                # return the average steps per episode to the user:\n",
    "                return round(total_steps / num_episodes, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168a863",
   "metadata": {},
   "source": [
    "# **Using the Environment: Deterministic**\n",
    "\n",
    "This section utilizes the above object-oriented approach to create an environment and train an agent, using the deterministic case where ``is_slippery = False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efeda89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space is: Discrete(4)\n",
      "Observation Space is: Discrete(16)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m███████████████████████████████████████████████████\u001b[0m| 2500000/2500000 [26:54<00:00, 1548.22it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# create training environment:\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery = False)\n",
    "\n",
    "# instantiate the agent:\n",
    "agent_det = GLIE_MC_Agent(env = env, gamma = 0.99, beta = 1e-5, es = False, rs = False)\n",
    "num_episodes = 250000\n",
    "\n",
    "# learn value function using MC:\n",
    "q = agent_det.GPI(num_episodes = num_episodes).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0bb65",
   "metadata": {},
   "source": [
    "### **Benchmark the Learned Policy**\n",
    "\n",
    "This section uses the evaluation functions defined within the class to check how the learned policy fares over a set of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a55691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m█████████████████████████████████████████████████████\u001b[0m| 100000/100000 [00:22<00:00, 4462.03it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average return per episode is: 0.73\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m█████████████████████████████████████████████████████\u001b[0m| 100000/100000 [00:22<00:00, 4501.41it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average episode length is: 41.242 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m█████████████████████████████████████████████████████\u001b[0m| 100000/100000 [00:21<00:00, 4584.01it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The success rate is: 72.857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set num_episodes to desired testing length:\n",
    "num_episodes = 100000\n",
    "\n",
    "# evaluate and print to the user:\n",
    "avg_return = agent_det.average_return(agent_det, num_episodes)\n",
    "print(f\"The average return per episode for the deterministic case is: {avg_return}\\n\")\n",
    "\n",
    "avg_length = agent_det.average_length(agent_det, num_episodes)\n",
    "print(f\"The average episode length for the deterministic case is: {avg_length} steps\\n\")\n",
    "\n",
    "success_rate = agent_det.success_rate(agent_det, num_episodes)\n",
    "print(f\"The success rate for the deterministic case is: {success_rate}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b896d9e",
   "metadata": {},
   "source": [
    "### **Visualize the Learned Policy**\n",
    "\n",
    "This section visualizes the final policy that was learned and overlays a heatmap of the action-value function $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb661dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define mapping of actions to directions:\n",
    "action_map = {0: '←', 1: '↓', 2: '→', 3: '↑', 4: 'Terminal', 5 : 'Goal'}\n",
    "size = int(np.sqrt(agent_det.env.observation_space.n))\n",
    "grid = np.zeros(agent_det.env.observation_space.n, dtype = object)\n",
    "\n",
    "# for every state:\n",
    "for s in range(agent_det.env.observation_space.n):\n",
    "    if s in agent_det.terminal_states:\n",
    "        if s == 15:\n",
    "            action = 5\n",
    "        else:\n",
    "            action = 4\n",
    "    else:\n",
    "        action = np.argmax(q[s, :])\n",
    "\n",
    "    grid[s] = action_map[action]\n",
    "\n",
    "# reshape to look like map:\n",
    "grid = grid.reshape(size, size)\n",
    "\n",
    "# plot stuff:\n",
    "fig, ax = plt.subplots(figsize = (2*size, 2*size))\n",
    "\n",
    "# heatmap stuff:\n",
    "heatmap_value = np.max(q, axis = 1).reshape((4,4))\n",
    "heatmap = ax.imshow(heatmap_value, cmap = 'coolwarm', interpolation = 'nearest')\n",
    "\n",
    "# overlay text:\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        text = grid[i, j]\n",
    "        ax.text(j, i, text, ha = 'center', va = 'center', color = 'black', fontsize = 16)\n",
    "\n",
    "# formatting:\n",
    "fig.colorbar(heatmap, ax = ax, label = 'Q')\n",
    "ax.set_title('ε-greedy policy - deterministic')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44bff7",
   "metadata": {},
   "source": [
    "# **Using the Environment: Stochastic**\n",
    "\n",
    "This section utilizes the object-oriented approach to create an environment and train an agent, using the stochastic case where ``is_slippery = True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb412e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training environment:\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery = True)\n",
    "\n",
    "# instantiate the agent:\n",
    "agent_sto = GLIE_MC_Agent(env = env, gamma = 0.99, beta = 1e-5, es = False, rs = False)\n",
    "num_episodes = 250000\n",
    "\n",
    "# learn value function using MC:\n",
    "q = agent_sto.GPI(num_episodes = num_episodes).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd017f71",
   "metadata": {},
   "source": [
    "### **Benchmark the Learned Policy**\n",
    "\n",
    "This section uses the evaluation functions defined within the class to check how the learned policy fares over a set of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set num_episodes to desired testing length:\n",
    "num_episodes = 100000\n",
    "\n",
    "# evaluate and print to the user:\n",
    "avg_return = agent_sto.average_return(agent_sto, num_episodes)\n",
    "print(f\"The average return per episode for the stochastic case is: {avg_return}\\n\")\n",
    "\n",
    "avg_length = agent_sto.average_length(agent_sto, num_episodes)\n",
    "print(f\"The average episode length for the stochastic case is: {avg_length} steps\\n\")\n",
    "\n",
    "success_rate = agent_sto.success_rate(agent_sto, num_episodes)\n",
    "print(f\"The success rate for the stochastic case is: {success_rate}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f662eb",
   "metadata": {},
   "source": [
    "### **Visualize the Learned Policy**\n",
    "\n",
    "This section visualizes the final policy that was learned and overlays a heatmap of the action-value function $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mapping of actions to directions:\n",
    "action_map = {0: '←', 1: '↓', 2: '→', 3: '↑', 4: 'Terminal', 5 : 'Goal'}\n",
    "size = int(np.sqrt(agent_sto.env.observation_space.n))\n",
    "grid = np.zeros(agent_sto.env.observation_space.n, dtype = object)\n",
    "\n",
    "# for every state:\n",
    "for s in range(agent_sto.env.observation_space.n):\n",
    "    if s in agent_sto.terminal_states:\n",
    "        if s == 15:\n",
    "            action = 5\n",
    "        else:\n",
    "            action = 4\n",
    "    else:\n",
    "        action = np.argmax(q[s, :])\n",
    "\n",
    "    grid[s] = action_map[action]\n",
    "\n",
    "# reshape to look like map:\n",
    "grid = grid.reshape(size, size)\n",
    "\n",
    "# plot stuff:\n",
    "fig, ax = plt.subplots(figsize = (2*size, 2*size))\n",
    "\n",
    "# heatmap stuff:\n",
    "heatmap_value = np.max(q, axis = 1).reshape((4,4))\n",
    "heatmap = ax.imshow(heatmap_value, cmap = 'coolwarm', interpolation = 'nearest')\n",
    "\n",
    "# overlay text:\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        text = grid[i, j]\n",
    "        ax.text(j, i, text, ha = 'center', va = 'center', color = 'black', fontsize = 16)\n",
    "\n",
    "# formatting:\n",
    "fig.colorbar(heatmap, ax = ax, label = 'Q')\n",
    "ax.set_title('ε-greedy policy - stochastic')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
