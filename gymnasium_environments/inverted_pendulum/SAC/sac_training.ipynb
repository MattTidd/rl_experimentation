{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826d79c2",
   "metadata": {},
   "source": [
    "# **Introduction** \n",
    "\n",
    "This notebook serves as an implementation of the Soft-Actor Critic (SAC) algorithm developed by Haarnoja et al. in the following papers [[1]](https://arxiv.org/abs/1801.01290)[[2]](https://arxiv.org/abs/1812.05905)[[3]](https://arxiv.org/abs/1812.11103). SAC is an off-policy actor-critic algorithm that is based on the maximum entropy reinforcement learning framework.\n",
    "\n",
    "The maximum entropy framework sees the actor attempting to simultaneously maximize both the expected return and the expected entropy of the policy. This leads to improvements in both exploration and robustness. The three key components of the SAC architecture are:\n",
    "\n",
    "1. an actor-critic architecture, separating policy and value function into two distinct networks,\n",
    "2. an off-policy formulation allowing the use of a replay buffer, and\n",
    "3. the use of entropy maximization to encourage both stability and exploration.\n",
    "\n",
    "A fourth feature was added in [[2]](https://arxiv.org/abs/1812.05905), which includes automatically adjusting the entropy such that the *temperature* parameter $\\alpha$ is learned.\n",
    "\n",
    "This implementation was done using the `InvertedPendulum` environment offered through `Gymnasium`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ffec7",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "9c63f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a6b11",
   "metadata": {},
   "source": [
    "function for making neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "cc69b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model based on user inputs:\n",
    "def make_model(rate : float, \n",
    "               layers : int, \n",
    "               neurons : int, \n",
    "               input_shape : int, \n",
    "               output_shape : int, \n",
    "               loss_function : str, \n",
    "               output_activation : str):\n",
    "    \"\"\" \n",
    "    this is a function for making simple Keras sequential models. these models do not have any protection\n",
    "    against vanishing or exploding gradients (lacking batch_normalization and dropout layers, namely) and are\n",
    "    simply fully connected, nonlinearly activated feedforward neural networks.\n",
    "\n",
    "    rate:                   a float representing the learning rate of the optimizer used, which is Adam\n",
    "    layers:                 an int representing the number of layers in the network\n",
    "    neurons:                an int representing the number of neurons in each layer of the network\n",
    "    input_shape:            an int representing the shape of the input data (input_shape, )\n",
    "    output_shape:           an int representing the number of outputs of the network\n",
    "    loss_function:          a string representing the desired loss function to be used in the optimizer, which is Adam\n",
    "    output_activation:      a string representing the activation function of the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "        else: \n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    # return to user:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92662a",
   "metadata": {},
   "source": [
    "object oriented function for making SAC agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29968fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class:\n",
    "class SAC_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                lr_a: float,\n",
    "                lr_c: float,\n",
    "                alpha: float,\n",
    "                gamma: float,\n",
    "                layers: int,\n",
    "                neurons: int,\n",
    "                batch_size: int,\n",
    "                buffer_size: int,\n",
    "                gradient_steps: int,\n",
    "                polyak_coefficient: float,\n",
    "                target_update_interval: int,\n",
    "                ):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the soft actor-critic (SAC) algorithm to learn an optimal policy. \n",
    "        the theory behind this implementation is derived from entropy maximization reinforcement learning, which seeks to improve the robustness and the \n",
    "        exploratory nature of the agent by changing the learning objective to both maximize the expected return and the entropy. \n",
    "\n",
    "        the base SAC implementation in [1] is brittle with respect to the temperature. this is because the SAC algorithm is very sensitive to the scaling of the \n",
    "        rewards, and the reward scaling is inversely proportional to temperature, which determines the relative importance of the entropy term versus the reward.\n",
    "\n",
    "        the modified implementation in [2] addresses this delicate need to tune the temperature by having the network automatically learn the temperature. basically,\n",
    "        the learning objective is modified to include an expected entropy constraint. the learned stochastic policy therefore attempts to achieve maximal expected return, \n",
    "        satisfying a minimum expected entropy constraint. \n",
    "\n",
    "        env:                        a gymnasium environment\n",
    "        lr_a:                       a float value representing the learning rate of the actor, α_a\n",
    "        lr_c:                       a float value representing the learning rate of the critic, α_c\n",
    "        alpha:                      a float value representing the initial temperature, α\n",
    "        gamma:                      a float value representing the discount factor, γ\n",
    "        layers:                     an int value indicating the number of layers in a given network\n",
    "        neurons:                    an int value indicating the number of neurons in a given network\n",
    "        batch_size:                 an int value indicating the number of samples to sample from the replay buffer\n",
    "        buffer_size:                an int value indicating the size of the replay buffer\n",
    "        gradient_steps:             an int value indicating how many gradient steps to apply\n",
    "        polyak_coefficient:         a float value indicating the target smooth coefficient (polyak coefficient)\n",
    "        target_update_interval:     an int value indicating how often to apply the smooth target network update\n",
    "\n",
    "        nS:                 an int representing the number of states observed from the continuous state space\n",
    "        nA:                 an int representing the number of actions observed from the continuous action space\n",
    "        actor:              a Keras sequential neural network representing the actor network\n",
    "        critic_1:           a Keras sequential neural network representing the first critic network\n",
    "        critic_2:           a Keras sequential neural network representing the second critic network\n",
    "        experience:         an empty deque used to hold the experience history of the agent, limited by 'buffer_size'\n",
    "        entropy_target:     an int value representing the desired entropy target\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        self.gamma = gamma\n",
    "        self.layers = layers\n",
    "        self.neurons = neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gradient_steps = gradient_steps\n",
    "        self.polyak_coefficient = polyak_coefficient\n",
    "        self.target_update_interval = target_update_interval\n",
    "\n",
    "        # set temperature as learnable parameter:\n",
    "        self.log_alpha = tf.Variable(np.log(alpha), dtype = tf.float32)     # log alpha used to maintain positive entropy\n",
    "        self.alpha = tf.exp(self.log_alpha)\n",
    "        self.temp_optimizer = Adam(learning_rate = 1e-3)\n",
    "\n",
    "        # get the environmental dimensions (number of states and number of actions):\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.shape[0]\n",
    "        self.entropy_target = -self.nA      # see appendix D in [2]\n",
    "\n",
    "        # create networks:\n",
    "        # ACTOR NETWORK:\n",
    "        x = Input(shape = (self.nS, ))\n",
    "        for i in range(self.layers):\n",
    "            if i == 0:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(x)\n",
    "            else:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(h)\n",
    "        \n",
    "        # make heads:\n",
    "        mu = Dense(self.nA, activation = \"tanh\")(h)\n",
    "        log_sigma = Dense(self.nA, activation = \"linear\")(h)\n",
    "\n",
    "        self.actor = keras.Model(inputs = x, outputs = [mu, log_sigma])\n",
    "        self.actor.compile(optimizer = Adam(self.lr_a), loss = lambda y_true, y_pred: 0.0)\n",
    "        \n",
    "        # NETWORK PAIR 1:\n",
    "        self.critic_1 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS + self.nA,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_1 = keras.models.clone_model(self.critic_1)\n",
    "        self.target_1.set_weights(self.critic_1.get_weights())\n",
    "        \n",
    "        # NETWORK PAIR 2:\n",
    "        self.critic_2 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS + self.nA,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_2 = keras.models.clone_model(self.critic_2)\n",
    "        self.target_2.set_weights(self.critic_2.get_weights())\n",
    "        \n",
    "        # initialize the experience buffer:\n",
    "        self.experience = deque(maxlen = self.buffer_size)\n",
    "\n",
    "        # initialize the step counter:\n",
    "        self.step_counter = 0\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # critic updating function:\n",
    "    @tf.function\n",
    "    def critic_update(self, states, rewards, actions, next_states, dones):\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # 1) get fresh actions from the policy:\n",
    "            means, log_stds = self.actor(next_states, training = False)             # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 2)                           # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                                 # convert from log_std to std\n",
    "            zs = tf.random.normal(shape = tf.shape(means))                          # sample the standard normal distribution\n",
    "            pre_tanh = means + stds*zs                                              # use location-scaling to go from N(0,1) to N(μ, σ^2)\n",
    "            next_actions = tf.tanh(pre_tanh)  * self.env.action_space.high      # squash action using tanh, scale action into action bounds\n",
    "\n",
    "            # 2) need to pass these actions along with the next states into target networks:\n",
    "            merged_target_input = tf.concat([next_states, next_actions], axis = 1)      # shape (bS, nS+nA)\n",
    "            target_output_1 = self.target_1(merged_target_input, training = False)      # shape (bS, 1) \n",
    "            target_output_2 = self.target_2(merged_target_input, training = False)      # shape (bS, 1) \n",
    "            target_output = tf.math.minimum(target_output_1, target_output_2)           # shape (bS, 1) \n",
    "\n",
    "            # 3) compute the log of the policy with the fresh action and next state:\n",
    "            # basically computing log(pi(a~'|s')) using the same zs from fresh action via calcuating log-prob\n",
    "            # of transformed univariate gaussian (me thinks)\n",
    "            logp = -0.5 * (zs**2 + 2*log_stds + np.log(2*np.pi))\n",
    "            \n",
    "            # however when the actions are bounded, this changes the probability density\n",
    "            # need to correct for this\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "            term = self.alpha * logp\n",
    "\n",
    "            # 4) compute the target:\n",
    "            target = rewards + self.gamma*(1-dones)*(target_output - term)\n",
    "\n",
    "            # 5) compute the critic loss:\n",
    "            merged_input = tf.concat([states, actions], axis = 1)\n",
    "            critic_output_1 = self.critic_1(merged_input, training = True)\n",
    "            critic_output_2 = self.critic_2(merged_input, training = True)\n",
    "\n",
    "            critic_loss_1 = tf.reduce_mean(tf.square(critic_output_1 - target))\n",
    "            critic_loss_2 = tf.reduce_mean(tf.square(critic_output_2 - target))\n",
    "\n",
    "        # 6) compute gradients:\n",
    "        critic_grad_1 = critic_tape.gradient(critic_loss_1, self.critic_1.trainable_variables)\n",
    "        critic_grad_2 = critic_tape.gradient(critic_loss_2, self.critic_2.trainable_variables)\n",
    "\n",
    "        # 7) backpropagate and update critic networks:\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_grad_1, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_grad_2, self.critic_2.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def actor_update(self, states):\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            # 1) reparameterize the action:\n",
    "            means, log_stds = self.actor(states, training = False)      # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 2)               # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                     # convert from log_std to std\n",
    "            zs = tf.random.normal(shape = tf.shape(means))              # sample the standard normal distribution\n",
    "            pre_tanh = means + stds*zs                                  # use location-scaling to go from N(0,1) to N(μ, σ^2)                                 \n",
    "            actions = tf.tanh(pre_tanh) * self.env.action_space.high    # squash action using tanh, scale action into action bounds\n",
    "\n",
    "            # 2) pass s_t and a~_t into Q-networks:\n",
    "            merged_input = tf.concat([states, actions], axis = 1)               # shape (bS, nS + nA)\n",
    "            critic_output_1 = self.critic_1(merged_input)                       # shape (bS, 1)\n",
    "            critic_output_2 = self.critic_2(merged_input)                       # shape (bS, 1)\n",
    "            critic_output = tf.math.minimum(critic_output_1, critic_output_2)   # shape (bS, 1)\n",
    "\n",
    "            # 3) need to get the log of the policy:\n",
    "            logp = -0.5 * (zs**2 + 2*log_stds + np.log(2*np.pi))\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "            term = self.alpha * logp\n",
    "\n",
    "            # 4) need to compute the actor loss:\n",
    "            actor_loss = -tf.reduce_mean(critic_output - term)\n",
    "        \n",
    "        # 5) compute gradients:\n",
    "        actor_grad = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "\n",
    "        # 6) backpropagate and update actor network:\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def temp_update(self, states):\n",
    "        with tf.GradientTape() as temp_tape:\n",
    "            # 1) reparameterize the action:\n",
    "            means, log_stds = self.actor(states, training = False)      # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 2)               # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                     # convert from log_std to std\n",
    "            zs = tf.random.normal(shape = tf.shape(means))              # sample the standard normal distribution\n",
    "            pre_tanh = means + stds*zs                                  # use location-scaling to go from N(0,1) to N(μ, σ^2)\n",
    "\n",
    "            # 2) compute the log of the policy:\n",
    "            logp = -0.5 * (zs**2 + 2*log_stds + np.log(2*np.pi))\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "            # 3) compute temperature loss:\n",
    "            temp_loss = -tf.reduce_mean(self.log_alpha * (logp + self.entropy_target))\n",
    "        \n",
    "        # 4) compute gradient:\n",
    "        temp_grad = temp_tape.gradient(temp_loss, [self.log_alpha])\n",
    "\n",
    "        # 5) backpropagate and update:\n",
    "        self.temp_optimizer.apply_gradients(zip(temp_grad, [self.log_alpha]))\n",
    "        self.alpha.assign(tf.exp(self.log_alpha))\n",
    "\n",
    "    @tf.function\n",
    "    def polyak_update(self):\n",
    "        # update the first network:\n",
    "        for target_param, param in zip(self.target_1.variables, self.critic_1.variables):\n",
    "            target_param.assign(self.polyak_coefficient * target_param + (1 - self.polyak_coefficient) * param)\n",
    "\n",
    "        # update the second network:\n",
    "        for target_param, param in zip(self.target_2.variables, self.critic_2.variables):\n",
    "            target_param.assign(self.polyak_coefficient * target_param + (1 - self.polyak_coefficient) * param)\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length):\n",
    "        \"\"\" \n",
    "        this is the function that executes the main SAC loop, using gymnasium to interact with the environment\n",
    "        and collect reward. this function also calls upon the tensorflow decorated training functions to update the \n",
    "        networks.\n",
    "\n",
    "        training_length:        desired number of episodes to train the agent for\n",
    "        return:                 reward_history, which is the history of reward collected over all episodes\n",
    "        \n",
    "        \"\"\"\n",
    "        # initialize reward history:\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "            # 0) INITIALIZATION:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while False:\n",
    "            while not done:\n",
    "                # convert obs to tensor:\n",
    "                obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "\n",
    "                # 1) GET ACTION:\n",
    "                mean, log_std = self.actor(obs_tensor)              # get network output\n",
    "                log_std = tf.clip_by_value(log_std, -20, 2)         # clamp the log-std to prevent huge/small stds\n",
    "                std = tf.exp(log_std)                               # convert from log_std to std\n",
    "                z = tf.random.normal(shape = tf.shape(mean))        # sample the standard normal distribution\n",
    "                action = mean + std*z                               # use location-scaling to go from N(0,1) to N(μ, σ^2)\n",
    "                action = tf.tanh(action)[0]                         # squash action using tanh function\n",
    "                action = action * self.env.action_space.high        # scale action into action bounds\n",
    "\n",
    "                # 2) EXECUTE ACTION ON ENVIRONMENT:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "\n",
    "                # 3) STORE TRANSITION IN REPLAY BUFFER:\n",
    "                self.experience.append((obs.copy(), action.numpy(), reward, next_obs.copy(), done))\n",
    "\n",
    "                # advance:\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # 4) RESET ENVIRONMENT STATE IF TERMINAL:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # 5) IF TIME TO UPDATE:\n",
    "                if len(self.experience) >= self.batch_size:\n",
    "                    # 6) FOR AS MANY UPDATES AS REQUIRED:\n",
    "                    for _ in range(self.gradient_steps):\n",
    "                        # 7) SAMPLE A BATCH:\n",
    "                        batch = random.sample(list(self.experience), self.batch_size)\n",
    "\n",
    "                        # unpack the batch:\n",
    "                        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                        # convert to tensors:\n",
    "                        states          = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                        actions         = tf.convert_to_tensor(actions, dtype = tf.float32)\n",
    "                        rewards         = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                        next_states     = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                        dones           = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                        # 8) GRADIENT STEP TO UPDATE NETWORKS:\n",
    "                        self.critic_update(states, rewards, actions, next_states, dones)\n",
    "                        self.actor_update(states)\n",
    "                        self.temp_update(states)\n",
    "                        self.polyak_update()\n",
    "\n",
    "            # 9) ADVANCE REWARD HISTORY:\n",
    "            reward_history[episode] = episode_reward\n",
    "\n",
    "        return reward_history\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186a112",
   "metadata": {},
   "source": [
    "define stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "683d310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters:\n",
    "lr_a = 1e-4\n",
    "lr_c = 1e-4\n",
    "alpha = 1e-2\n",
    "gamma = 0.99\n",
    "layers = 2\n",
    "neurons = 32\n",
    "batch_size = 32\n",
    "buffer_size = 5000\n",
    "gradient_steps = 1\n",
    "polyak_coefficient = 0.01\n",
    "target_update_interval = 1\n",
    "\n",
    "warmup_length = 1000\n",
    "training_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "011da8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make env:\n",
    "env = gym.make(\"InvertedPendulum-v5\")\n",
    "\n",
    "# make agent:\n",
    "agent = SAC_Agent(env = env, \n",
    "                  lr_a = lr_a, \n",
    "                  lr_c = lr_c,\n",
    "                  alpha = alpha,\n",
    "                  gamma = gamma, \n",
    "                  layers = layers, \n",
    "                  neurons = neurons, \n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size, \n",
    "                  gradient_steps = gradient_steps,\n",
    "                  polyak_coefficient = polyak_coefficient, \n",
    "                  target_update_interval = target_update_interval\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8b7be",
   "metadata": {},
   "source": [
    "need to warm up the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "1bf3f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize:\n",
    "obs, _ = agent.env.reset()\n",
    "obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "\n",
    "# generate trajectories:\n",
    "for _ in range(warmup_length):\n",
    "    # 1) SAMPLE A RANDOM ACTION:\n",
    "    mean, log_std = agent.actor(obs_tensor)             # get network output\n",
    "    log_std = tf.clip_by_value(log_std, -20, 2)         # clamp the log-std to prevent huge/small stds\n",
    "    std = tf.exp(log_std)                               # convert from log_std to std\n",
    "    z = tf.random.normal(shape = tf.shape(mean))        # sample the standard normal distribution\n",
    "    action = mean + std*z                               # use location-scaling to go from N(0,1) to N(μ, σ^2)\n",
    "    action = tf.tanh(action)[0]                         # squash action using tanh function\n",
    "    action = action * agent.env.action_space.high       # scale action into action bounds\n",
    "\n",
    "    # 2) ACT ON THE ENVIRONMENT:\n",
    "    next_obs, reward, term, trunc, _ = agent.env.step(action)\n",
    "\n",
    "    # 3) CHECK FOR COMPLETION:\n",
    "    done = term or trunc\n",
    "\n",
    "    # 4) APPEND TO BUFFER:\n",
    "    agent.experience.append((obs.copy(), action.numpy(), reward, next_obs.copy(), done))\n",
    "    obs = next_obs if not done else agent.env.reset()[0]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
