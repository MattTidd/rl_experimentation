{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826d79c2",
   "metadata": {},
   "source": [
    "# **Introduction** \n",
    "\n",
    "This notebook serves as an implementation of the Soft-Actor Critic (SAC) algorithm developed by Haarnoja et al. in the following papers [[1]](https://arxiv.org/abs/1801.01290)[[2]](https://arxiv.org/abs/1812.05905)[[3]](https://arxiv.org/abs/1812.11103). SAC is an off-policy actor-critic algorithm that is based on the maximum entropy reinforcement learning framework.\n",
    "\n",
    "The maximum entropy framework sees the actor attempting to simultaneously maximize both the expected return and the expected entropy of the policy. This leads to improvements in both exploration and robustness. The three key components of the SAC architecture are:\n",
    "\n",
    "1. an actor-critic architecture, separating policy and value function into two distinct networks,\n",
    "2. an off-policy formulation allowing the use of a replay buffer, and\n",
    "3. the use of entropy maximization to encourage both stability and exploration.\n",
    "\n",
    "A fourth feature was added in [[2]](https://arxiv.org/abs/1812.05905), which includes automatically adjusting the entropy such that the *temperature* parameter $\\alpha$ is learned.\n",
    "\n",
    "This implementation was done using the `InvertedPendulum` environment offered through `Gymnasium`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ffec7",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "9c63f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a6b11",
   "metadata": {},
   "source": [
    "function for making neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "cc69b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model based on user inputs:\n",
    "def make_model(rate : float, \n",
    "               layers : int, \n",
    "               neurons : int, \n",
    "               input_shape : int, \n",
    "               output_shape : int, \n",
    "               loss_function : str, \n",
    "               output_activation : str):\n",
    "    \"\"\" \n",
    "    this is a function for making simple Keras sequential models. these models do not have any protection\n",
    "    against vanishing or exploding gradients (lacking batch_normalization and dropout layers, namely) and are\n",
    "    simply fully connected, nonlinearly activated feedforward neural networks.\n",
    "\n",
    "    rate:                   a float representing the learning rate of the optimizer used, which is Adam\n",
    "    layers:                 an int representing the number of layers in the network\n",
    "    neurons:                an int representing the number of neurons in each layer of the network\n",
    "    input_shape:            an int representing the shape of the input data (input_shape, )\n",
    "    output_shape:           an int representing the number of outputs of the network\n",
    "    loss_function:          a string representing the desired loss function to be used in the optimizer, which is Adam\n",
    "    output_activation:      a string representing the activation function of the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "        else: \n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    # return to user:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92662a",
   "metadata": {},
   "source": [
    "object oriented function for making SAC agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29968fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class:\n",
    "class SAC_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                lr_a: float,\n",
    "                lr_c: float,\n",
    "                gamma: float,\n",
    "                layers: int,\n",
    "                neurons: int,\n",
    "                batch_size: int,\n",
    "                buffer_size: int,\n",
    "                gradient_steps: int,\n",
    "                polyak_coefficient: float,\n",
    "                target_update_interval: int,\n",
    "                ):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the soft actor-critic (SAC) algorithm to learn an optimal policy. \n",
    "        the theory behind this implementation is derived from entropy maximization reinforcement learning, which seeks to improve the robustness and the \n",
    "        exploratory nature of the agent by changing the learning objective to both maximize the expected return and the entropy. \n",
    "\n",
    "        the base SAC implementation in [1] is brittle with respect to the temperature. this is because the SAC algorithm is very sensitive to the scaling of the \n",
    "        rewards, and the reward scaling is inversely proportional to temperature, which determines the relative importance of the entropy term versus the reward.\n",
    "\n",
    "        the modified implementation in [2] addresses this delicate need to tune the temperature by having the network automatically learn the temperature. basically,\n",
    "        the learning objective is modified to include an expected entropy constraint. the learned stochastic policy therefore attempts to achieve maximal expected return, \n",
    "        satisfying a minimum expected entropy constraint. \n",
    "\n",
    "        env:                        a gymnasium environment\n",
    "        lr_a:                       a float value representing the learning rate of the actor, α_a\n",
    "        lr_c:                       a float value representing the learning rate of the critic, α_c\n",
    "        gamma:                      a float value representing the discount factor, γ\n",
    "        layers:                     an int value indicating the number of layers in a given network\n",
    "        neurons:                    an int value indicating the number of neurons in a given network\n",
    "        batch_size:                 an int value indicating the number of samples to sample from the replay buffer\n",
    "        buffer_size:                an int value indicating the size of the replay buffer\n",
    "        gradient_steps:             an int value indicating how many gradient steps to apply\n",
    "        polyak_coefficient:         a float value indicating the target smooth coefficient (polyak coefficient)\n",
    "        target_update_interval:     an int value indicating how often to apply the smooth target network update\n",
    "\n",
    "        nS:                 an int representing the number of states observed from the continuous state space\n",
    "        nA:                 an int representing the number of actions observed from the continuous action space\n",
    "        actor:              a Keras sequential neural network representing the actor network\n",
    "        critic_1:           a Keras sequential neural network representing the first critic network\n",
    "        critic_2:           a Keras sequential neural network representing the second critic network\n",
    "        experience:         an empty deque used to hold the experience history of the agent, limited by 'buffer_size'\n",
    "        entropy_target:     an int value representing the desired entropy target\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        self.gamma = gamma\n",
    "        self.layers = layers\n",
    "        self.neurons = neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gradient_steps = gradient_steps\n",
    "        self.polyak_coefficient = polyak_coefficient\n",
    "        self.target_update_interval = target_update_interval\n",
    "\n",
    "        # get the environmental dimensions (number of states and number of actions):\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.shape[0]\n",
    "        self.entropy_target = -self.nA      # see appendix D in [2]\n",
    "\n",
    "        # create networks:\n",
    "        # ACTOR NETWORK:\n",
    "        x = Input(shape = (self.nS, ))\n",
    "        for i in range(self.layers):\n",
    "            if i == 0:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(x)\n",
    "            else:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(h)\n",
    "        \n",
    "        # make heads:\n",
    "        mu = Dense(self.nA, activation = \"tanh\")(h)\n",
    "        log_sigma = Dense(self.nA, activation = \"linear\")(h)\n",
    "\n",
    "        self.actor = keras.Model(inputs = x, outputs = [mu, log_sigma])\n",
    "        self.actor.compile(optimizer = Adam(self.lr_a), loss = lambda y_true, y_pred: 0.0)\n",
    "        \n",
    "        # NETWORK PAIR 1:\n",
    "        self.critic_1 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_1 = keras.models.clone_model(self.critic_1)\n",
    "        self.target_1.set_weights(self.critic_1.get_weights())\n",
    "        \n",
    "        # NETWORK PAIR 2:\n",
    "        self.critic_2 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_2 = keras.models.clone_model(self.critic_2)\n",
    "        self.target_2.set_weights(self.critic_2.get_weights())\n",
    "        \n",
    "        # initialize the experience buffer:\n",
    "        self.experience = deque(maxlen = self.buffer_size)\n",
    "\n",
    "        # initialize the step counter:\n",
    "        self.step_counter = 0\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # training function:\n",
    "    def training(self, training_length):\n",
    "        \"\"\" \n",
    "        this is the function that executes the main SAC loop, using gymnasium to interact with the environment\n",
    "        and collect reward. this function also calls upon the tensorflow decorated training functions to update the \n",
    "        networks.\n",
    "\n",
    "        training_length:        desired number of episodes to train the agent for\n",
    "        return:                 reward_history, which is the history of reward collected over all episodes\n",
    "        \n",
    "        \"\"\"\n",
    "        # initialize reward history:\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "            # 0) INITIALIZATION:\n",
    "            obs, _ = self.env.reset(seed = 18)\n",
    "            obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while False:\n",
    "            while not done:\n",
    "                # 1) GET ACTION:\n",
    "                mean, log_std = self.actor(obs_tensor)             # get network output\n",
    "                std = tf.exp(log_std)                               # convert from log_std to std\n",
    "                z = tf.random.normal(shape = tf.shape(mean))        # sample the standard normal distribution\n",
    "                action = mean + std*z                               # use location-scaling to go from N(0,1) to N(μ, σ^2)\n",
    "                action = tf.tanh(action)[0]                         # squash action using tanh function\n",
    "\n",
    "                # 2) EXECUTE ACTION ON ENVIRONMENT:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "\n",
    "                # 3) STORE TRANSITION IN REPLAY BUFFER:\n",
    "                self.experience.append((obs, action, reward, next_obs, done))\n",
    "                # print(f\"episode: {episode} | step: {agent.step_counter} | done: {done}\")\n",
    "\n",
    "                # advance:\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # 4) RESET ENVIRONMENT STATE IF TERMINAL:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # 5) IF TIME TO UPDATE:\n",
    "                if len(self.experience) >= self.batch_size:\n",
    "                    # 6) FOR AS MANY UPDATES AS REQUIRED:\n",
    "                    for _ in range(self.gradient_steps):\n",
    "                        # 7) SAMPLE A BATCH:\n",
    "                        batch = random.sample(list(self.experience), self.batch_size)\n",
    "\n",
    "                        # unpack the batch:\n",
    "                        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                        # convert to tensors:\n",
    "                        states          = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                        actions         = tf.convert_to_tensor(actions, dtype = tf.float32)\n",
    "                        rewards         = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                        next_states     = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                        dones           = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                        # 8) GRADIENT STEP TO UPDATE NETWORKS:\n",
    "                        # need to make the decorated training functions\n",
    "\n",
    "            # advance reward history:\n",
    "            reward_history[episode] = episode_reward\n",
    "\n",
    "        return reward_history\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186a112",
   "metadata": {},
   "source": [
    "define stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "683d310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters:\n",
    "lr_a = 1e-4\n",
    "lr_c = 1e-4\n",
    "gamma = 0.99\n",
    "layers = 2\n",
    "neurons = 32\n",
    "batch_size = 32\n",
    "buffer_size = 5000\n",
    "gradient_steps = 1\n",
    "polyak_coefficient = 0.01\n",
    "target_update_interval = 1\n",
    "\n",
    "warmup_length = 1000\n",
    "training_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "011da8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make env:\n",
    "env = gym.make(\"InvertedPendulum-v5\")\n",
    "\n",
    "# make agent:\n",
    "agent = SAC_Agent(env = env, \n",
    "                  lr_a = lr_a, \n",
    "                  lr_c = lr_c, \n",
    "                  gamma = gamma, \n",
    "                  layers = layers, \n",
    "                  neurons = neurons, \n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size, \n",
    "                  gradient_steps = gradient_steps,\n",
    "                  polyak_coefficient = polyak_coefficient, \n",
    "                  target_update_interval = target_update_interval\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8b7be",
   "metadata": {},
   "source": [
    "need to warm up the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "1bf3f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize:\n",
    "obs, _ = agent.env.reset()\n",
    "obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "\n",
    "# generate trajectories:\n",
    "for _ in range(warmup_length):\n",
    "    # 1) SAMPLE A RANDOM ACTION:\n",
    "    mean, log_std = agent.actor(obs_tensor)             # get network output\n",
    "    std = tf.exp(log_std)                               # convert from log_std to std\n",
    "    z = tf.random.normal(shape = tf.shape(mean))        # sample the standard normal distribution\n",
    "    action = mean + std*z                               # use location-scaling to go from N(0,1) to N(μ, σ^2)\n",
    "    action = tf.tanh(action)[0]                         # squash action using tanh function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c0374",
   "metadata": {},
   "source": [
    "train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "78e1b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress: 100%|\u001b[38;2;51;255;0m██████████████████████████████████████████████\u001b[0m| 2/2 [00:00<00:00, 38.92it/s]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4., 21.])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.training(training_length = training_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
