{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68fe6c4",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook serves as an implementation of the Soft-Actor Critic (SAC) algorithm developed by Haarnoja et al. in the following papers [[1]](https://arxiv.org/abs/1801.01290)[[2]](https://arxiv.org/abs/1812.05905)[[3]](https://arxiv.org/abs/1812.11103). SAC is an off-policy actor-critic algorithm that is based on the maximum entropy reinforcement learning framework.\n",
    "\n",
    "Specifically, this implementation is done using the `Reacher` environment offered through `Gymnasium`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c18954",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29882ee6",
   "metadata": {},
   "source": [
    "# **Class & Function Definition**\n",
    "\n",
    "This section defines relevant functions and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aec3ee",
   "metadata": {},
   "source": [
    "##### Function for making neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ca82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model based on user inputs:\n",
    "def make_model(rate : float, \n",
    "               layers : int, \n",
    "               neurons : int, \n",
    "               input_shape : int, \n",
    "               output_shape : int, \n",
    "               loss_function : str, \n",
    "               output_activation : str):\n",
    "    \"\"\" \n",
    "    this is a function for making simple Keras sequential models. these models do not have any protection\n",
    "    against vanishing or exploding gradients (lacking batch_normalization and dropout layers, namely) and are\n",
    "    simply fully connected, nonlinearly activated feedforward neural networks.\n",
    "\n",
    "    rate:                   a float representing the learning rate of the optimizer used, which is Adam\n",
    "    layers:                 an int representing the number of layers in the network\n",
    "    neurons:                an int representing the number of neurons in each layer of the network\n",
    "    input_shape:            an int representing the shape of the input data (input_shape, )\n",
    "    output_shape:           an int representing the number of outputs of the network\n",
    "    loss_function:          a string representing the desired loss function to be used in the optimizer, which is Adam\n",
    "    output_activation:      a string representing the activation function of the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "        else: \n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    # return to user:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97c5fe",
   "metadata": {},
   "source": [
    "##### Running mean/std class for normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294bca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define running mean/std class:\n",
    "class RunningMeanStd:\n",
    "    # constructor:\n",
    "    def __init__(self, shape = (), eps = 1e-8):\n",
    "        self.mean = np.zeros(shape, dtype = np.float32)     # initialize a running mean vector\n",
    "        self.var  = np.ones(shape, dtype = np.float64)      # initialize a running std vector\n",
    "        self.count = 1e-4   # initialize a small, non-zero count to stabilize updates\n",
    "        self.eps = eps      # store an epsilon for use in division\n",
    "\n",
    "    # update stats function:\n",
    "    def update(self, x):\n",
    "        # ensure correct type on input:\n",
    "        x = np.array(x, dtype = np.float32)\n",
    "\n",
    "        # batch stats:\n",
    "        batch_mean = np.mean(x, axis = 0)       # per dimension mean of batch\n",
    "        batch_var = np.var(x, axis = 0)         # per dimension variance of batch\n",
    "        batch_count = x.shape[0]                # number of examples in the batch\n",
    "        delta = batch_mean - self.mean          # difference in new batch mean and current running mean\n",
    "        tot_count = self.count + batch_count    # total effective sample counter after merging\n",
    "\n",
    "        # compute merged sum of squared deviations, from both the previous data and the new batch:\n",
    "        m2 = (self.var * self.count) + (batch_var * batch_count) + (delta**2) * (self.count * batch_count / tot_count)\n",
    "\n",
    "        # update the running mean:\n",
    "        self.mean = self.mean + delta * (batch_count / tot_count)\n",
    "\n",
    "        # update the running variance:\n",
    "        self.var  = m2 / tot_count\n",
    "\n",
    "        # update the internal sample count:\n",
    "        self.count = tot_count\n",
    "\n",
    "    # normalization function:\n",
    "    def normalize(self, x, clip = 10.0):\n",
    "        # ensure correct type on input:\n",
    "        x = np.array(x, dtype = np.float32)\n",
    "\n",
    "        # compute per dimension standard deviation:\n",
    "        std = np.sqrt(self.var + self.eps).astype(np.float32)\n",
    "\n",
    "        # return normalized output:\n",
    "        return np.clip((x - self.mean.astype(np.float32)) / std, -clip, clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14a67a",
   "metadata": {},
   "source": [
    "##### SAC agent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SAC class:\n",
    "class SAC_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                lr_a: float,\n",
    "                lr_c: float,\n",
    "                lr_t: float,\n",
    "                alpha: float,\n",
    "                gamma: float,\n",
    "                layers: int,\n",
    "                neurons: int,\n",
    "                batch_size: int,\n",
    "                buffer_size: int,\n",
    "                action_scale: int,\n",
    "                gradient_steps: int,\n",
    "                update_interval: int,\n",
    "                polyak_coefficient: float,\n",
    "                ):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the soft actor-critic (SAC) algorithm to learn an optimal policy. \n",
    "        the theory behind this implementation is derived from entropy maximization reinforcement learning, which seeks to improve the robustness and the \n",
    "        exploratory nature of the agent by changing the learning objective to both maximize the expected return and the entropy. \n",
    "\n",
    "        the base SAC implementation in [1] is brittle with respect to the temperature. this is because the SAC algorithm is very sensitive to the scaling of the \n",
    "        rewards, and the reward scaling is inversely proportional to temperature, which determines the relative importance of the entropy term versus the reward.\n",
    "\n",
    "        the modified implementation in [2] addresses this delicate need to tune the temperature by having the network automatically learn the temperature. basically,\n",
    "        the learning objective is modified to include an expected entropy constraint. the learned stochastic policy therefore attempts to achieve maximal expected return, \n",
    "        satisfying a minimum expected entropy constraint. \n",
    "\n",
    "        env:                        a gymnasium environment\n",
    "        lr_a:                       a float value representing the learning rate of the actor, ﾎｱ_a\n",
    "        lr_c:                       a float value representing the learning rate of the critic, ﾎｱ_c\n",
    "        lr_t:                       a float value representing the learning rate of the temperature, which is given by ﾎｱ\n",
    "        alpha:                      a float value representing the initial temperature, ﾎｱ\n",
    "        gamma:                      a float value representing the discount factor, ﾎｳ\n",
    "        layers:                     an int value indicating the number of layers in a given network\n",
    "        neurons:                    an int value indicating the number of neurons in a given network\n",
    "        batch_size:                 an int value indicating the number of samples to sample from the replay buffer\n",
    "        buffer_size:                an int value indicating the size of the replay buffer\n",
    "        action_scale:               an int value indicating the desired scale of the learned actions\n",
    "        gradient_steps:             an int value indicating how many gradient steps to apply\n",
    "        update_interval:            an int value indicating how often to apply the actor and temperature gradient steps\n",
    "        polyak_coefficient:         a float value indicating the target smooth coefficient (polyak coefficient)\n",
    "\n",
    "        nS:                 an int representing the number of states observed from the continuous state space\n",
    "        nA:                 an int representing the number of actions observed from the continuous action space\n",
    "        actor:              a Keras sequential neural network representing the actor network\n",
    "        critic_1:           a Keras sequential neural network representing the first critic network\n",
    "        critic_2:           a Keras sequential neural network representing the second critic network\n",
    "        experience:         an empty deque used to hold the experience history of the agent, limited by 'buffer_size'\n",
    "        entropy_target:     an int value representing the desired entropy target\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        self.lr_t = lr_t\n",
    "        self.gamma = gamma\n",
    "        self.layers = layers\n",
    "        self.neurons = neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.action_scale = action_scale\n",
    "        self.gradient_steps = gradient_steps\n",
    "        self.update_interval = update_interval\n",
    "        self.polyak_coefficient = polyak_coefficient\n",
    "\n",
    "        # set temperature as learnable parameter:\n",
    "        self.log_alpha = tf.Variable(np.log(alpha), dtype = tf.float32)     # log alpha used to maintain positive entropy\n",
    "        self.alpha = tf.exp(self.log_alpha)\n",
    "        self.temp_optimizer = Adam(learning_rate = self.lr_t)\n",
    "\n",
    "        # get the environmental dimensions (number of states and number of actions):\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.shape[0]\n",
    "        self.entropy_target = -self.nA      # see appendix D in [2]\n",
    "\n",
    "        # initialize running stats:\n",
    "        self.obs_rms = RunningMeanStd(shape = self.env.observation_space.shape)\n",
    "        self.rew_rms = RunningMeanStd(shape = ())\n",
    "\n",
    "        # create networks:\n",
    "        # ACTOR NETWORK:\n",
    "        x = Input(shape = (self.nS, ))\n",
    "        for i in range(self.layers):\n",
    "            if i == 0:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(x)\n",
    "            else:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(h)\n",
    "        \n",
    "        # make heads:\n",
    "        mu = Dense(self.nA, activation = \"tanh\")(h)\n",
    "        log_sigma = Dense(self.nA, activation = \"linear\")(h)\n",
    "\n",
    "        self.actor = keras.Model(inputs = x, outputs = [mu, log_sigma])\n",
    "        self.actor.compile(optimizer = Adam(self.lr_a), loss = lambda y_true, y_pred: 0.0)\n",
    "        \n",
    "        # NETWORK PAIR 1:\n",
    "        self.critic_1 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS + self.nA,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_1 = keras.models.clone_model(self.critic_1)\n",
    "        self.target_1.set_weights(self.critic_1.get_weights())\n",
    "        \n",
    "        # NETWORK PAIR 2:\n",
    "        self.critic_2 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS + self.nA,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_2 = keras.models.clone_model(self.critic_2)\n",
    "        self.target_2.set_weights(self.critic_2.get_weights())\n",
    "        \n",
    "        # initialize the experience buffer:\n",
    "        self.experience = deque(maxlen = self.buffer_size)\n",
    "\n",
    "        # initialize the step counter:\n",
    "        self.step_counter = 0\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # critic updating function:\n",
    "    @tf.function\n",
    "    def critic_update(self, states, rewards, actions, next_states, dones):\n",
    "        \"\"\"\n",
    "        this function performs the critic update. it first computes the Q-network output at the next state,\n",
    "        reparameterizes that next action, computes the log-prob of that action given the state, and then \n",
    "        computes a target Q-value for use in the target calculation.\n",
    "\n",
    "        the current Q value is also computed, and this is used to compute the loss for each Q-network. in \n",
    "        SAC, two Q-networks are used to reduce bias and overestimation, much like in DDQN, and the minimum value \n",
    "        of the two Q-network outputs is taken in the target calculation to further reduce bias\n",
    "\n",
    "        states:         a tensor containing the states sampled from the buffer:          size (bS, 4)\n",
    "        rewards:        a tensor containing the rewards sampled from the buffer:         size (bS, 1)\n",
    "        actions:        a tensor containing the actions sampled from the buffer:         size (bS, 1)\n",
    "        next_states:    a tensor containing the next states sampled from the buffer:     size (bS, 4)\n",
    "        dones:          a tensor containing the dones sampled from the buffer:           size (bS, 1)\n",
    "\n",
    "        output:         loss computed for each network and backpropagated through networks to update them\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent = True) as critic_tape:\n",
    "            # 1) need to sample the next action:\n",
    "            means, log_stds = self.actor(next_states, training = True)    # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 1)                 # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                       # convert from log_std to std\n",
    "\n",
    "            # reparameterize:\n",
    "            zs = tf.random.normal(tf.shape(means))                              # sample the standard normal distribution\n",
    "            pre_tanh = means + stds * zs                                        # location-scale from N(0, 1) to N(ﾎｼ, ﾏタ2), prior to tanh squashing\n",
    "            next_actions = tf.tanh(pre_tanh)                                    # reparameterize using reparameterization trick\n",
    "\n",
    "            # scale to within the action-space:\n",
    "            scaled_next_actions = next_actions * np.array(self.action_scale, dtype = np.float32)\n",
    "\n",
    "            # 2) need to compute the log-prob with the tanh correction:\n",
    "            logp = -0.5 * (zs**2 + 2 * log_stds + np.log(2 * np.pi))    # log-probability of sampling pre-tanh\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)       # reduce dimensionality\n",
    "\n",
    "            # subtract off the jacobian correction for tanh squashing:\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "            # 3) compute target Q value:\n",
    "            target_input = tf.concat([next_states, scaled_next_actions], axis = 1)      # shape (bS, nS + nA)\n",
    "            q1_t = self.target_1(target_input, training = True)                         # shape (bS, 1)\n",
    "            q2_t = self.target_2(target_input, training = True)                         # shape (bS, 1)\n",
    "            q_t = tf.minimum(q1_t, q2_t)                                                # shape (bS, 1)\n",
    "\n",
    "            # 4) compute the target:\n",
    "            target = rewards[:, None] + self.gamma * (1 - dones[:, None]) * (q_t - self.alpha * logp)\n",
    "\n",
    "            # 5) compute current Q value:\n",
    "            current_input = tf.concat([states, actions], axis = 1)\n",
    "            q1 = self.critic_1(current_input, training = True)\n",
    "            q2 = self.critic_2(current_input, training = True)\n",
    "\n",
    "            # 6) compute losses:\n",
    "            critic_loss_1 = tf.reduce_mean((q1 - tf.stop_gradient(target))**2)\n",
    "            critic_loss_2 = tf.reduce_mean((q2 - tf.stop_gradient(target))**2)\n",
    "        \n",
    "        # 7) compute gradients:\n",
    "        grads_1 = critic_tape.gradient(critic_loss_1, self.critic_1.trainable_variables)\n",
    "        grads_2 = critic_tape.gradient(critic_loss_2, self.critic_2.trainable_variables)\n",
    "\n",
    "        # 8) backpropagate:\n",
    "        self.critic_1.optimizer.apply_gradients(zip(grads_1, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(grads_2, self.critic_2.trainable_variables))\n",
    "\n",
    "        # due to multiple tape calls, it is not automatically closed -> therefore close tape:\n",
    "        del critic_tape\n",
    "\n",
    "    @tf.function\n",
    "    def actor_update(self, states):\n",
    "        \"\"\"\n",
    "        this function performs the actor update. it first computes the action at the current state,\n",
    "        reparameterizes it, computes the log-prob of this action given the state, and then computes a \n",
    "        Q-estimate using the reparameterized action f | s. this is then used to compute loss, which is \n",
    "        backpropagated through the actor network.\n",
    "\n",
    "        states:     a tensor containing the next states sampled from the buffer:        size (bS, 4)\n",
    "\n",
    "        output:     loss computed for the actor network and backpropagated through the network to update it\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            # 1) need to sample a fresh action:\n",
    "            means, log_stds = self.actor(states, training = True)       # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 1)               # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                     # convert from log_std to std\n",
    "\n",
    "            # reparameterize:\n",
    "            zs = tf.random.normal(tf.shape(means))                      # sample the standard normal distribution\n",
    "            pre_tanh = means + stds * zs                                # location scaale from N(0, 1) to N(ﾎｼ, ﾏタ2), prior to tanh squashing\n",
    "            actions = tf.tanh(pre_tanh)                                 # reparameterize using reparameterization trick\n",
    "\n",
    "            # scale to within the action-space:\n",
    "            scaled_actions = actions * np.array(self.action_scale, dtype = np.float32) \n",
    "\n",
    "            # 2) need to compute the log-prob with the tanh correction:\n",
    "            logp = -0.5 * (zs**2 + 2*log_stds + np.log(2 * np.pi))      # log-probability of sampling pre-tanh\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)       # reduce dimensionality\n",
    "\n",
    "            # subtract off the jacobian correction for tanh squashing:\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "            # 3) compute a Q-estimate using f | s:\n",
    "            current_input = tf.concat([states, scaled_actions], axis = 1)   # shape (bS, nS + nA)\n",
    "            q1 = self.critic_1(current_input, training = True)              # shape (bS, 1)\n",
    "            q2 = self.critic_2(current_input, training = True)              # shape (bS, 1)\n",
    "            q = tf.minimum(q1, q2)                                          # shape (bS, 1)\n",
    "\n",
    "            # 4) compute the actor loss (flipped for ascent):\n",
    "            actor_loss = tf.reduce_mean(self.alpha * logp - q)\n",
    "\n",
    "        # 5) compute gradients:\n",
    "        grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "\n",
    "        # 6) backpropagate:\n",
    "        self.actor.optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def temp_update(self, states):\n",
    "        \"\"\"\n",
    "        this function updates the temperature parameter ﾎｱ. first it computes a fresh action, reparameterizes it, \n",
    "        computes the log of the policy with this action given the state. then, the temperature loss is computed and backpropagated\n",
    "        to update the temperature parameter, and ﾎｱ is updated accordingly. \n",
    "\n",
    "        the log(ﾎｱ) is what is actually learned, as this enforces positivity and helps with stability. this effectively works by driving the \n",
    "        entropy of the policy towards a desired entropy target, often given by -|搨忿.\n",
    "\n",
    "        states:     a tensor containing the next states sampled from the buffer:        size (bS, 4)\n",
    "\n",
    "        output:     loss computed for ﾎｱ and backpropagated to update the parameter\n",
    "\n",
    "        \"\"\"\n",
    "        # 1) need to sample a fresh action:\n",
    "        means, log_stds = self.actor(states, training = False)      # get the network output\n",
    "        log_stds = tf.clip_by_value(log_stds, -20, 1)               # clamp the log-std to prevent huge/small stds\n",
    "        stds = tf.exp(log_stds)                                     # convert from log_std to std\n",
    "\n",
    "        # reparameterize:\n",
    "        zs = tf.random.normal(shape = tf.shape(means))              # sample the standard normal distribution\n",
    "        pre_tanh = means + stds * zs                                # use location-scaling to go from N(0,1) to N(ﾎｼ, ﾏタ2)\n",
    "\n",
    "        # 2) compute the log of the policy:\n",
    "        logp = -0.5 * (zs**2 + 2*log_stds + np.log(2*np.pi))        # log-probability of sampling pre-tanh\n",
    "        logp = tf.reduce_sum(logp, axis = 1, keepdims = True)       # reduce dimensionality\n",
    "\n",
    "        # subtract off the jacobian correction for tanh squashing:\n",
    "        logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "        # 3) compute temperature loss:\n",
    "        with tf.GradientTape() as temp_tape:\n",
    "            temp_loss = -tf.reduce_mean(self.log_alpha * (logp + self.entropy_target))\n",
    "        \n",
    "        # 4) compute gradient:\n",
    "        temp_grad = temp_tape.gradient(temp_loss, [self.log_alpha])\n",
    "\n",
    "        # 5) backpropagate and update:\n",
    "        self.temp_optimizer.apply_gradients(zip(temp_grad, [self.log_alpha]))\n",
    "        self.alpha = (tf.exp(self.log_alpha))\n",
    "\n",
    "    @tf.function\n",
    "    def polyak_update(self):\n",
    "        \"\"\"\n",
    "        this function performs soft or polyak updates on the target network. it slowly updates the target networks\n",
    "        towards the online networks, and this method is supposed to help with stability.\n",
    "\n",
    "        output:     each target network has its weights updated toward its respective online network\n",
    "        \n",
    "        \"\"\"\n",
    "        # update the first network:\n",
    "        for target, online in zip(self.target_1.variables, self.critic_1.variables):\n",
    "            target.assign(self.polyak_coefficient * online + (1 - self.polyak_coefficient) * target)\n",
    "\n",
    "        # update the second network:\n",
    "        for target, online in zip(self.target_2.variables, self.critic_2.variables):\n",
    "            target.assign(self.polyak_coefficient * online + (1 - self.polyak_coefficient) * target)\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length, train_metrics = None):\n",
    "        \"\"\" \n",
    "        this is the function that executes the main SAC loop, using gymnasium to interact with the environment\n",
    "        and collect reward. this function also calls upon the tensorflow decorated training functions to update the \n",
    "        networks.\n",
    "\n",
    "        training_length:        desired number of episodes to train the agent for\n",
    "        training_metrics:       dict containing desired training results\n",
    "        return:                 reward_history, which is the history of reward collected over all episodes\n",
    "        \n",
    "        \"\"\"\n",
    "        # initialize reward history:\n",
    "        reward_history = []\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "            # 0) INITIALIZATION:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while False:\n",
    "            while not done:\n",
    "                # update obs stats:\n",
    "                self.obs_rms.update(np.expand_dims(obs, 0))\n",
    "                norm_obs = self.obs_rms.normalize(obs)\n",
    "\n",
    "                # convert normalized obs to tensor:\n",
    "                obs_tensor = tf.convert_to_tensor([norm_obs], dtype = tf.float32)\n",
    "\n",
    "                # 1) GET ACTION:\n",
    "                mean, log_std = self.actor(obs_tensor)                          # get network output\n",
    "                log_std = tf.clip_by_value(log_std, -20, 1)                     # clamp the log-std to prevent huge/small stds\n",
    "                std = tf.exp(log_std)                                           # convert from log_std to std\n",
    "                z = tf.random.normal(shape = tf.shape(mean))                    # sample the standard normal distribution\n",
    "                pre_tanh = mean + std*z                                         # use location-scaling to go from N(0,1) to N(ﾎｼ, ﾏタ2)\n",
    "\n",
    "                # squash using tanh, scale action into action bounds:\n",
    "                scaled_action = tf.tanh(pre_tanh)[0] * np.array(self.action_scale, dtype = np.float32)\n",
    "                \n",
    "                # 2) EXECUTE ACTION ON ENVIRONMENT:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(scaled_action.numpy().astype(np.float32))\n",
    "                done = term or trunc\n",
    "\n",
    "                # update running reward stats:\n",
    "                self.rew_rms.update(np.expand_dims(np.array(reward, dtype = np.float32), 0))\n",
    "\n",
    "                # 3) STORE TRANSITION IN REPLAY BUFFER:\n",
    "                self.experience.append((obs.copy(), scaled_action.numpy(), reward, next_obs.copy(), done))\n",
    "\n",
    "                # advance:\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # 4) RESET ENVIRONMENT STATE IF TERMINAL:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # 5) IF TIME TO UPDATE:\n",
    "                if len(self.experience) >= self.batch_size:\n",
    "                    # 6) FOR AS MANY UPDATES AS REQUIRED:\n",
    "                    for _ in range(self.gradient_steps):\n",
    "                        # 7) SAMPLE A BATCH:\n",
    "                        batch = random.sample(list(self.experience), self.batch_size)\n",
    "\n",
    "                        # unpack the batch:\n",
    "                        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                        # normalize the observations and rewards using running stats:\n",
    "                        states_norm = self.obs_rms.normalize(states)\n",
    "                        next_states_norm = self.obs_rms.normalize(next_states)\n",
    "                        rewards_norm = self.rew_rms.normalize(rewards)\n",
    "\n",
    "                        # convert to tensors:\n",
    "                        states          = tf.convert_to_tensor(states_norm, dtype = tf.float32)\n",
    "                        actions         = tf.convert_to_tensor(actions, dtype = tf.float32)\n",
    "                        rewards         = tf.convert_to_tensor(rewards_norm, dtype = tf.float32)\n",
    "                        next_states     = tf.convert_to_tensor(next_states_norm, dtype = tf.float32)\n",
    "                        dones           = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                        # 8) GRADIENT STEP TO UPDATE NETWORKS:\n",
    "                        self.critic_update(states, rewards, actions, next_states, dones)\n",
    "\n",
    "                        if int(self.step_counter) % self.update_interval == 0:\n",
    "                            self.actor_update(states)\n",
    "                            self.temp_update(states)\n",
    "                            \n",
    "                        self.polyak_update()\n",
    "\n",
    "            # 9) ADVANCE REWARD HISTORY:\n",
    "            reward_history.append(episode_reward)\n",
    "\n",
    "            # 10) CHECK FOR EARLY STOPPING:\n",
    "            if train_metrics and len(reward_history) >= train_metrics[\"min_train\"]:\n",
    "                # compute a recent average:\n",
    "                recent_average = np.mean(reward_history[-train_metrics[\"over_last\"]:]).round(3)\n",
    "\n",
    "                # if the recent average is above the desired threshold:\n",
    "                if recent_average >= train_metrics[\"desired_score\"]:\n",
    "                    print(f\"environment solved in {len(reward_history)} episodes!\")\n",
    "                    print(f\"average reward was: {recent_average}\")\n",
    "\n",
    "                    # save weights:\n",
    "                    os.makedirs(train_metrics[\"model_path\"], exist_ok = True)\n",
    "                    try:\n",
    "                        # paths:\n",
    "                        actor_path = f\"{train_metrics[\"model_path\"]}/actor_weights.weights.h5\"\n",
    "                        critic_path_1 = f\"{train_metrics[\"model_path\"]}/critic_1_weights.weights.h5\"\n",
    "                        critic_path_2 = f\"{train_metrics[\"model_path\"]}/critic_2_weights.weights.h5\"\n",
    "\n",
    "                        # save weights:\n",
    "                        self.actor.save_weights(actor_path)\n",
    "                        self.critic_1.save_weights(critic_path_1)\n",
    "                        self.critic_2.save_weights(critic_path_2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"failed to save model: {e}\")\n",
    "                    break\n",
    "\n",
    "            # 11) PRINT TO USER:\n",
    "            if episode % 5 == 0:\n",
    "                print(f\"average reward is: {np.mean(reward_history[-train_metrics[\"over_last\"]:]).round(3)}\")\n",
    "\n",
    "        return reward_history         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713f70c",
   "metadata": {},
   "source": [
    "# **Hyperparameter Definition**\n",
    "\n",
    "This section defines the hyperparameters used, as well as other important things like the model saving paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61450f0f",
   "metadata": {},
   "source": [
    "##### Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters:\n",
    "lr_a = 1e-4\n",
    "lr_c = 3e-4\n",
    "lr_t = 3e-4\n",
    "alpha = 0.001\n",
    "gamma = 0.99\n",
    "layers = 3\n",
    "neurons = 256\n",
    "batch_size = 256\n",
    "buffer_size = int(1e6) \n",
    "action_scale = 0.5\n",
    "gradient_steps = 4\n",
    "update_interval = 8\n",
    "polyak_coefficient = 0.02\n",
    "\n",
    "warmup_length = 10000\n",
    "training_length = 500\n",
    "reward_dist_weight = 1.0\n",
    "\n",
    "# training parameters:\n",
    "# desired_score = 0\n",
    "# over_last = 10\n",
    "# min_train = 50\n",
    "desired_score = 9300\n",
    "over_last = 10\n",
    "min_train = 50\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"weights\")\n",
    "model_path = os.path.join(base_path, f\"reacher_{len(os.listdir(base_path)) + 1}\")\n",
    "\n",
    "train_metrics = {\"desired_score\" : desired_score,\n",
    "                 \"over_last\"     : over_last,\n",
    "                 \"min_train\"     : min_train,\n",
    "                 \"model_path\"    : model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06ca6d",
   "metadata": {},
   "source": [
    "##### Environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear backend:\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# make env:\n",
    "# env = gym.make(\"Reacher-v5\", reward_dist_weight = reward_dist_weight)\n",
    "env = gym.make(\"InvertedDoublePendulum-v5\", healthy_reward = 10)\n",
    "\n",
    "# make agent:\n",
    "agent = SAC_Agent(env = env, \n",
    "                  lr_a = lr_a, \n",
    "                  lr_c = lr_c,\n",
    "                  lr_t = lr_t,\n",
    "                  alpha = alpha,\n",
    "                  gamma = gamma, \n",
    "                  layers = layers, \n",
    "                  neurons = neurons, \n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size, \n",
    "                  action_scale = action_scale,\n",
    "                  gradient_steps = gradient_steps,\n",
    "                  update_interval = update_interval,\n",
    "                  polyak_coefficient = polyak_coefficient, \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fb7ce",
   "metadata": {},
   "source": [
    "# **Training** \n",
    "\n",
    "This section warms up the replay buffer and trains the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e570d99",
   "metadata": {},
   "source": [
    "##### Warm up the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45266fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize:\n",
    "obs, _ = agent.env.reset()\n",
    "obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "\n",
    "# generate trajectories:\n",
    "for _ in tqdm(range(warmup_length), ncols = 100, colour = \"#33FF00\", desc = \"warmup progress\"):\n",
    "    # 1) SAMPLE A RANDOM ACTION:\n",
    "    mean, log_std = agent.actor(obs_tensor)                         # get network output\n",
    "    log_std = tf.clip_by_value(log_std, -20, 2)                     # clamp the log-std to prevent huge/small stds\n",
    "    std = tf.exp(log_std)                                           # convert from log_std to std\n",
    "    z = tf.random.normal(shape = tf.shape(mean))                    # sample the standard normal distribution\n",
    "    pre_tanh = mean + std*z                                         # use location-scaling to go from N(0,1) to N(ﾎｼ, ﾏタ2)\n",
    "\n",
    "    # squash action using tanh, scale action into action bounds:\n",
    "    scaled_action = tf.tanh(pre_tanh)[0] * np.array(agent.action_scale, dtype = np.float32)\n",
    "\n",
    "    # 2) ACT ON THE ENVIRONMENT:\n",
    "    next_obs, reward, term, trunc, _ = agent.env.step(scaled_action.numpy().astype(np.float32))\n",
    "\n",
    "    # 3) CHECK FOR COMPLETION:\n",
    "    done = term or trunc\n",
    "\n",
    "    # 4) APPEND TO BUFFER:\n",
    "    agent.experience.append((obs.copy(), scaled_action.numpy(), reward, next_obs.copy(), done))\n",
    "    obs = next_obs if not done else agent.env.reset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31d845",
   "metadata": {},
   "source": [
    "##### Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = agent.training(training_length = training_length, train_metrics = train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1109dd",
   "metadata": {},
   "source": [
    "# **Visualization**\n",
    "\n",
    "This section visualizes the results, both in the form of plotting and actually simulating the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efe988",
   "metadata": {},
   "source": [
    "##### Visualize reward vs. episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving average function:\n",
    "def moving_average(interval, window_size):\n",
    "    window = np.ones(int(window_size)) / float(window_size)\n",
    "    return np.convolve(interval, window, 'valid')\n",
    "\n",
    "filtered_data = moving_average(reward_history, 4)\n",
    "last_few = np.mean(reward_history[-over_last:]).round(3)\n",
    "\n",
    "figure = plt.figure(figsize = (10, 4))\n",
    "plt.plot(reward_history)\n",
    "plt.plot(filtered_data, 'r-')\n",
    "plt.minorticks_on\n",
    "plt.title('reward earned vs. episode')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel(f'reward earned')\n",
    "plt.figtext(x = 0.5, y = -0.05, s = fr\"$\\mathrm{{\\alpha_a}}$: {lr_a} | $\\mathrm{{\\alpha_c}}$: {lr_c} | $\\mathrm{{\\alpha_t}}$: {lr_t} | $\\mathrm{{\\alpha}}$: {alpha} | $\\mathrm{{\\gamma}}$ : {gamma} | $\\mathrm{{n_l}}$: {layers} | $\\mathrm{{n_n}}$: {neurons} | $\\mathrm{{n_{{bs}}}}$: {batch_size} | $\\mathrm{{n_{{buff}}}}$: {buffer_size} | $\\mathrm{{\\rho}}$: {polyak_coefficient}\", ha = 'center', va = 'center')\n",
    "plt.figtext(x = 0.5, y = -0.125, s = fr\"mean over last {over_last} episodes: {last_few}\", ha = 'center', va = 'center')\n",
    "\n",
    "# save figure:\n",
    "if len(model_path) != 0:\n",
    "    try:\n",
    "        plt.savefig(f\"{model_path}/reward_episode_plot.png\", bbox_inches = 'tight')\n",
    "    except Exception as e:\n",
    "        print(f\"could not save: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9bf4dc",
   "metadata": {},
   "source": [
    "##### Visualize agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fe6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True\n",
    "width = 1280\n",
    "height = 1280\n",
    "# default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 1.5, \"lookat\" : [0.0, 0.0, 0.25]}\n",
    "default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : 0.0, \"distance\" : 3.5, \"lookat\" : [0.0, 0.0, 0.25]}\n",
    "\n",
    "if visualize:\n",
    "    # env = gym.make(\"Reacher-v5\", \n",
    "    #                reward_dist_weight = reward_dist_weight,\n",
    "    #                render_mode = \"human\", \n",
    "    #                width = width,\n",
    "    #                height = height,\n",
    "    #                default_camera_config = default_camera_config)\n",
    "    env = gym.make(\"InvertedDoublePendulum-v5\",\n",
    "                   healthy_reward = 10, \n",
    "                   render_mode = \"human\", \n",
    "                   width = width,\n",
    "                   height = height,\n",
    "                   default_camera_config = default_camera_config)\n",
    "    \n",
    "\n",
    "    agent.env = env\n",
    "\n",
    "    obs, _ = agent.env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 0) RENDER:\n",
    "        agent.env.render()\n",
    "\n",
    "        # 1) SAMPLE A RANDOM ACTION:\n",
    "        norm_obs = agent.obs_rms.normalize(obs)\n",
    "        obs_tensor = tf.convert_to_tensor([norm_obs], dtype = tf.float32)\n",
    "        mean, log_std = agent.actor(obs_tensor)                         # get network output\n",
    "        log_std = tf.clip_by_value(log_std, -20, 1)                     # clamp the log-std to prevent huge/small stds\n",
    "        std = tf.exp(log_std)                                           # convert from log_std to std\n",
    "        z = tf.random.normal(shape = tf.shape(mean))                    # sample the standard normal distribution\n",
    "        pre_tanh = mean + std*z                                         # use location-scaling to go from N(0,1) to N(ﾎｼ, ﾏタ2)\n",
    "\n",
    "        # squash action using tanh, scale action into action bounds:\n",
    "        scaled_action = tf.tanh(pre_tanh)[0] * np.array(agent.action_scale, dtype = np.float32)\n",
    "\n",
    "        # add noise:\n",
    "        scaled_action += tf.random.normal(shape = scaled_action.shape, dtype = scaled_action.dtype, stddev = 0.0)\n",
    "\n",
    "        # 2) DO ACTION:\n",
    "        next_obs, reward, term, trunc, _ = agent.env.step(scaled_action.numpy().astype(np.float32))\n",
    "\n",
    "        # 3) ADVANCE:\n",
    "        obs = next_obs\n",
    "\n",
    "        # 4) CHECK FOR COMPLETION:\n",
    "        done = term or trunc\n",
    "    \n",
    "    agent.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
