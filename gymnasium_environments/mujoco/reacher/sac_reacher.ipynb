{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68fe6c4",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook serves as an implementation of the Soft-Actor Critic (SAC) algorithm developed by Haarnoja et al. in the following papers [[1]](https://arxiv.org/abs/1801.01290)[[2]](https://arxiv.org/abs/1812.05905)[[3]](https://arxiv.org/abs/1812.11103). SAC is an off-policy actor-critic algorithm that is based on the maximum entropy reinforcement learning framework.\n",
    "\n",
    "Specifically, this implementation is done using the `Reacher` environment offered through `Gymnasium`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c18954",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc3e82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29882ee6",
   "metadata": {},
   "source": [
    "# **Class & Function Definition**\n",
    "\n",
    "This section defines relevant functions and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aec3ee",
   "metadata": {},
   "source": [
    "##### Function for making neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c02ca82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model based on user inputs:\n",
    "def make_model(rate : float, \n",
    "               layers : int, \n",
    "               neurons : int, \n",
    "               input_shape : int, \n",
    "               output_shape : int, \n",
    "               loss_function : str, \n",
    "               output_activation : str):\n",
    "    \"\"\" \n",
    "    this is a function for making simple Keras sequential models. these models do not have any protection\n",
    "    against vanishing or exploding gradients (lacking batch_normalization and dropout layers, namely) and are\n",
    "    simply fully connected, nonlinearly activated feedforward neural networks.\n",
    "\n",
    "    rate:                   a float representing the learning rate of the optimizer used, which is Adam\n",
    "    layers:                 an int representing the number of layers in the network\n",
    "    neurons:                an int representing the number of neurons in each layer of the network\n",
    "    input_shape:            an int representing the shape of the input data (input_shape, )\n",
    "    output_shape:           an int representing the number of outputs of the network\n",
    "    loss_function:          a string representing the desired loss function to be used in the optimizer, which is Adam\n",
    "    output_activation:      a string representing the activation function of the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "        else: \n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    # return to user:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14a67a",
   "metadata": {},
   "source": [
    "##### SAC agent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0221c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SAC class:\n",
    "class SAC_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                lr_a: float,\n",
    "                lr_c: float,\n",
    "                lr_t: float,\n",
    "                alpha: float,\n",
    "                gamma: float,\n",
    "                layers: int,\n",
    "                neurons: int,\n",
    "                batch_size: int,\n",
    "                buffer_size: int,\n",
    "                action_scale: int,\n",
    "                gradient_steps: int,\n",
    "                update_interval: int,\n",
    "                polyak_coefficient: float,\n",
    "                ):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the soft actor-critic (SAC) algorithm to learn an optimal policy. \n",
    "        the theory behind this implementation is derived from entropy maximization reinforcement learning, which seeks to improve the robustness and the \n",
    "        exploratory nature of the agent by changing the learning objective to both maximize the expected return and the entropy. \n",
    "\n",
    "        the base SAC implementation in [1] is brittle with respect to the temperature. this is because the SAC algorithm is very sensitive to the scaling of the \n",
    "        rewards, and the reward scaling is inversely proportional to temperature, which determines the relative importance of the entropy term versus the reward.\n",
    "\n",
    "        the modified implementation in [2] addresses this delicate need to tune the temperature by having the network automatically learn the temperature. basically,\n",
    "        the learning objective is modified to include an expected entropy constraint. the learned stochastic policy therefore attempts to achieve maximal expected return, \n",
    "        satisfying a minimum expected entropy constraint. \n",
    "\n",
    "        env:                        a gymnasium environment\n",
    "        lr_a:                       a float value representing the learning rate of the actor, Î±_a\n",
    "        lr_c:                       a float value representing the learning rate of the critic, Î±_c\n",
    "        lr_t:                       a float value representing the learning rate of the temperature, which is given by Î±\n",
    "        alpha:                      a float value representing the initial temperature, Î±\n",
    "        gamma:                      a float value representing the discount factor, Î³\n",
    "        layers:                     an int value indicating the number of layers in a given network\n",
    "        neurons:                    an int value indicating the number of neurons in a given network\n",
    "        batch_size:                 an int value indicating the number of samples to sample from the replay buffer\n",
    "        buffer_size:                an int value indicating the size of the replay buffer\n",
    "        action_scale:               an int value indicating the desired scale of the learned actions\n",
    "        gradient_steps:             an int value indicating how many gradient steps to apply\n",
    "        update_interval:            an int value indicating how often to apply the actor update\n",
    "        polyak_coefficient:         a float value indicating the target smooth coefficient (polyak coefficient)\n",
    "\n",
    "        nS:                 an int representing the number of states observed from the continuous state space\n",
    "        nA:                 an int representing the number of actions observed from the continuous action space\n",
    "        actor:              a Keras sequential neural network representing the actor network\n",
    "        critic_1:           a Keras sequential neural network representing the first critic network\n",
    "        critic_2:           a Keras sequential neural network representing the second critic network\n",
    "        experience:         an empty deque used to hold the experience history of the agent, limited by 'buffer_size'\n",
    "        entropy_target:     an int value representing the desired entropy target\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        self.lr_t = lr_t\n",
    "        self.gamma = gamma\n",
    "        self.layers = layers\n",
    "        self.neurons = neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.action_scale = action_scale\n",
    "        self.gradient_steps = gradient_steps\n",
    "        self.update_interval = update_interval\n",
    "        self.polyak_coefficient = polyak_coefficient\n",
    "\n",
    "        # set temperature as learnable parameter:\n",
    "        self.log_alpha = tf.Variable(np.log(alpha), dtype = tf.float32)     # log alpha used to maintain positive entropy\n",
    "        self.alpha = tf.exp(self.log_alpha)\n",
    "        self.temp_optimizer = Adam(learning_rate = self.lr_t)\n",
    "\n",
    "        # get the environmental dimensions (number of states and number of actions):\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.shape[0]\n",
    "        self.entropy_target = -self.nA      # see appendix D in [2]\n",
    "\n",
    "        # create networks:\n",
    "        # ACTOR NETWORK:\n",
    "        x = Input(shape = (self.nS, ))\n",
    "        for i in range(self.layers):\n",
    "            if i == 0:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(x)\n",
    "            else:\n",
    "                h = Dense(self.neurons, activation = \"relu\")(h)\n",
    "        \n",
    "        # make heads:\n",
    "        mu = Dense(self.nA, activation = \"tanh\")(h)\n",
    "        log_sigma = Dense(self.nA, activation = \"linear\")(h)\n",
    "\n",
    "        self.actor = keras.Model(inputs = x, outputs = [mu, log_sigma])\n",
    "        self.actor.compile(optimizer = Adam(self.lr_a), loss = lambda y_true, y_pred: 0.0)\n",
    "        \n",
    "        # NETWORK PAIR 1:\n",
    "        self.critic_1 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS + self.nA,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_1 = keras.models.clone_model(self.critic_1)\n",
    "        self.target_1.set_weights(self.critic_1.get_weights())\n",
    "        \n",
    "        # NETWORK PAIR 2:\n",
    "        self.critic_2 = make_model(rate = self.lr_c,\n",
    "                            layers = self.layers,\n",
    "                            neurons = self.neurons,\n",
    "                            input_shape = self.nS + self.nA,\n",
    "                            output_shape = 1,\n",
    "                            loss_function = \"mse\",\n",
    "                            output_activation = \"linear\")\n",
    "        \n",
    "        self.target_2 = keras.models.clone_model(self.critic_2)\n",
    "        self.target_2.set_weights(self.critic_2.get_weights())\n",
    "        \n",
    "        # initialize the experience buffer:\n",
    "        self.experience = deque(maxlen = self.buffer_size)\n",
    "\n",
    "        # initialize the step counter:\n",
    "        self.step_counter = 0\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # critic updating function:\n",
    "    @tf.function\n",
    "    def critic_update(self, states, rewards, actions, next_states, dones):\n",
    "        \"\"\"\n",
    "        this function performs the critic update. it first computes the Q-network output at the next state,\n",
    "        reparameterizes that next action, computes the log-prob of that action given the state, and then \n",
    "        computes a target Q-value for use in the target calculation.\n",
    "\n",
    "        the current Q value is also computed, and this is used to compute the loss for each Q-network. in \n",
    "        SAC, two Q-networks are used to reduce bias and overestimation, much like in DDQN, and the minimum value \n",
    "        of the two Q-network outputs is taken in the target calculation to further reduce bias\n",
    "\n",
    "        states:         a tensor containing the states sampled from the buffer:          size (bS, 4)\n",
    "        rewards:        a tensor containing the rewards sampled from the buffer:         size (bS, 1)\n",
    "        actions:        a tensor containing the actions sampled from the buffer:         size (bS, 1)\n",
    "        next_states:    a tensor containing the next states sampled from the buffer:     size (bS, 4)\n",
    "        dones:          a tensor containing the dones sampled from the buffer:           size (bS, 1)\n",
    "\n",
    "        output:         loss computed for each network and backpropagated through networks to update them\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent = True) as critic_tape:\n",
    "            # 1) need to sample the next action:\n",
    "            means, log_stds = self.actor(next_states, training = True)    # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 1)                 # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                       # convert from log_std to std\n",
    "\n",
    "            # reparameterize:\n",
    "            zs = tf.random.normal(tf.shape(means))                              # sample the standard normal distribution\n",
    "            pre_tanh = means + stds * zs                                        # location-scale from N(0, 1) to N(Î¼, Ïƒ^2), prior to tanh squashing\n",
    "            next_actions = tf.tanh(pre_tanh)                                    # reparameterize using reparameterization trick\n",
    "\n",
    "            # scale to within the action-space:\n",
    "            scaled_next_actions = next_actions * np.array(self.action_scale, dtype = np.float32)\n",
    "\n",
    "            # 2) need to compute the log-prob with the tanh correction:\n",
    "            logp = -0.5 * (zs**2 + 2 * log_stds + np.log(2 * np.pi))    # log-probability of sampling pre-tanh\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)       # reduce dimensionality\n",
    "\n",
    "            # subtract off the jacobian correction for tanh squashing:\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "            # 3) compute target Q value:\n",
    "            target_input = tf.concat([next_states, scaled_next_actions], axis = 1)      # shape (bS, nS + nA)\n",
    "            q1_t = self.target_1(target_input, training = True)                         # shape (bS, 1)\n",
    "            q2_t = self.target_2(target_input, training = True)                         # shape (bS, 1)\n",
    "            q_t = tf.minimum(q1_t, q2_t)                                                # shape (bS, 1)\n",
    "\n",
    "            # 4) compute the target:\n",
    "            target = rewards[:, None] + self.gamma * (1 - dones[:, None]) * (q_t - self.alpha * logp)\n",
    "\n",
    "            # 5) compute current Q value:\n",
    "            current_input = tf.concat([states, actions], axis = 1)\n",
    "            q1 = self.critic_1(current_input, training = True)\n",
    "            q2 = self.critic_2(current_input, training = True)\n",
    "\n",
    "            # 6) compute losses:\n",
    "            critic_loss_1 = tf.reduce_mean((q1 - tf.stop_gradient(target))**2)\n",
    "            critic_loss_2 = tf.reduce_mean((q2 - tf.stop_gradient(target))**2)\n",
    "        \n",
    "        # 7) compute gradients:\n",
    "        grads_1 = critic_tape.gradient(critic_loss_1, self.critic_1.trainable_variables)\n",
    "        grads_2 = critic_tape.gradient(critic_loss_2, self.critic_2.trainable_variables)\n",
    "\n",
    "        # 8) backpropagate:\n",
    "        self.critic_1.optimizer.apply_gradients(zip(grads_1, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(grads_2, self.critic_2.trainable_variables))\n",
    "\n",
    "        # due to multiple tape calls, it is not automatically closed -> therefore close tape:\n",
    "        del critic_tape\n",
    "\n",
    "    # actor updating function:\n",
    "    @tf.function\n",
    "    def actor_update(self, states):\n",
    "        \"\"\"\n",
    "        this function performs the actor update. it first computes the action at the current state,\n",
    "        reparameterizes it, computes the log-prob of this action given the state, and then computes a \n",
    "        Q-estimate using the reparameterized action f | s. this is then used to compute loss, which is \n",
    "        backpropagated through the actor network.\n",
    "\n",
    "        states:     a tensor containing the next states sampled from the buffer:        size (bS, 4)\n",
    "\n",
    "        output:     loss computed for the actor network and backpropagated through the network to update it\n",
    "        \n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            # 1) need to sample a fresh action:\n",
    "            means, log_stds = self.actor(states, training = True)       # get the network output\n",
    "            log_stds = tf.clip_by_value(log_stds, -20, 1)               # clamp the log-std to prevent huge/small stds\n",
    "            stds = tf.exp(log_stds)                                     # convert from log_std to std\n",
    "\n",
    "            # reparameterize:\n",
    "            zs = tf.random.normal(tf.shape(means))                      # sample the standard normal distribution\n",
    "            pre_tanh = means + stds * zs                                # location scaale from N(0, 1) to N(Î¼, Ïƒ^2), prior to tanh squashing\n",
    "            actions = tf.tanh(pre_tanh)                                 # reparameterize using reparameterization trick\n",
    "\n",
    "            # scale to within the action-space:\n",
    "            scaled_actions = actions * np.array(self.action_scale, dtype = np.float32) \n",
    "\n",
    "            # 2) need to compute the log-prob with the tanh correction:\n",
    "            logp = -0.5 * (zs**2 + 2*log_stds + np.log(2 * np.pi))      # log-probability of sampling pre-tanh\n",
    "            logp = tf.reduce_sum(logp, axis = 1, keepdims = True)       # reduce dimensionality\n",
    "\n",
    "            # subtract off the jacobian correction for tanh squashing:\n",
    "            logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "            # 3) compute a Q-estimate using f | s:\n",
    "            current_input = tf.concat([states, scaled_actions], axis = 1)   # shape (bS, nS + nA)\n",
    "            q1 = self.critic_1(current_input, training = True)              # shape (bS, 1)\n",
    "            q2 = self.critic_2(current_input, training = True)              # shape (bS, 1)\n",
    "            q = tf.minimum(q1, q2)                                          # shape (bS, 1)\n",
    "\n",
    "            # 4) compute the actor loss (flipped for ascent):\n",
    "            actor_loss = tf.reduce_mean(self.alpha * logp - q)\n",
    "\n",
    "        # 5) compute gradients:\n",
    "        grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "\n",
    "        # 6) backpropagate:\n",
    "        self.actor.optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "\n",
    "    # automatic temperature adjustment function:\n",
    "    @tf.function\n",
    "    def temp_update(self, states):\n",
    "        \"\"\"\n",
    "        this function updates the temperature parameter Î±. first it computes a fresh action, reparameterizes it, \n",
    "        computes the log of the policy with this action given the state. then, the temperature loss is computed and backpropagated\n",
    "        to update the temperature parameter, and Î± is updated accordingly. \n",
    "\n",
    "        the log(Î±) is what is actually learned, as this enforces positivity and helps with stability. this effectively works by driving the \n",
    "        entropy of the policy towards a desired entropy target, often given by -|ð’œ|.\n",
    "\n",
    "        states:     a tensor containing the next states sampled from the buffer:        size (bS, 4)\n",
    "\n",
    "        output:     loss computed for Î± and backpropagated to update the parameter\n",
    "\n",
    "        \"\"\"\n",
    "        # 1) need to sample a fresh action:\n",
    "        means, log_stds = self.actor(states, training = False)      # get the network output\n",
    "        log_stds = tf.clip_by_value(log_stds, -20, 1)               # clamp the log-std to prevent huge/small stds\n",
    "        stds = tf.exp(log_stds)                                     # convert from log_std to std\n",
    "\n",
    "        # reparameterize:\n",
    "        zs = tf.random.normal(shape = tf.shape(means))              # sample the standard normal distribution\n",
    "        pre_tanh = means + stds * zs                                # use location-scaling to go from N(0,1) to N(Î¼, Ïƒ^2)\n",
    "\n",
    "        # 2) compute the log of the policy:\n",
    "        logp = -0.5 * (zs**2 + 2*log_stds + np.log(2*np.pi))        # log-probability of sampling pre-tanh\n",
    "        logp = tf.reduce_sum(logp, axis = 1, keepdims = True)       # reduce dimensionality\n",
    "\n",
    "        # subtract off the jacobian correction for tanh squashing:\n",
    "        logp -= tf.reduce_sum(tf.math.log(1 - tf.tanh(pre_tanh)**2 + 1e-6), axis = 1, keepdims = True)\n",
    "\n",
    "        # 3) compute temperature loss:\n",
    "        with tf.GradientTape() as temp_tape:\n",
    "            temp_loss = -tf.reduce_mean(self.log_alpha * (logp + self.entropy_target))\n",
    "        \n",
    "        # 4) compute gradient:\n",
    "        temp_grad = temp_tape.gradient(temp_loss, [self.log_alpha])\n",
    "\n",
    "        # 5) backpropagate and update:\n",
    "        self.temp_optimizer.apply_gradients(zip(temp_grad, [self.log_alpha]))\n",
    "        self.alpha = (tf.exp(self.log_alpha))\n",
    "\n",
    "    # polyak updating function:\n",
    "    @tf.function\n",
    "    def polyak_update(self):\n",
    "        \"\"\"\n",
    "        this function performs soft or polyak updates on the target network. it slowly updates the target networks\n",
    "        towards the online networks, and this method is supposed to help with stability.\n",
    "\n",
    "        output:     each target network has its weights updated toward its respective online network\n",
    "        \n",
    "        \"\"\"\n",
    "        # update the first network:\n",
    "        for target, online in zip(self.target_1.variables, self.critic_1.variables):\n",
    "            target.assign(self.polyak_coefficient * online + (1 - self.polyak_coefficient) * target)\n",
    "\n",
    "        # update the second network:\n",
    "        for target, online in zip(self.target_2.variables, self.critic_2.variables):\n",
    "            target.assign(self.polyak_coefficient * online + (1 - self.polyak_coefficient) * target)\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length, train_metrics = None):\n",
    "        \"\"\" \n",
    "        this is the function that executes the main SAC loop, using gymnasium to interact with the environment\n",
    "        and collect reward. this function also calls upon the tensorflow decorated training functions to update the \n",
    "        networks.\n",
    "\n",
    "        training_length:        desired number of episodes to train the agent for\n",
    "        training_metrics:       a dict with the desired training metrics for early stopping\n",
    "        return:                 reward_history, which is the history of reward collected over all episodes\n",
    "        \n",
    "        \"\"\"\n",
    "        # initialize reward history:\n",
    "        reward_history = []\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "            # 0) INITIALIZATION:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while False:\n",
    "            while not done:\n",
    "                # convert obs to tensor:\n",
    "                obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "\n",
    "                # 1) GET ACTION:\n",
    "                mean, log_std = self.actor(obs_tensor)                          # get network output\n",
    "                log_std = tf.clip_by_value(log_std, -20, 1)                     # clamp the log-std to prevent huge/small stds\n",
    "                std = tf.exp(log_std)                                           # convert from log_std to std\n",
    "                z = tf.random.normal(shape = tf.shape(mean))                    # sample the standard normal distribution\n",
    "                pre_tanh = mean + std*z                                         # use location-scaling to go from N(0,1) to N(Î¼, Ïƒ^2)\n",
    "\n",
    "                # squash using tanh, scale action into action bounds:\n",
    "                scaled_action = tf.tanh(pre_tanh)[0] * np.array(self.action_scale, dtype = np.float32)\n",
    "                \n",
    "                # 2) EXECUTE ACTION ON ENVIRONMENT:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(scaled_action.numpy().astype(np.float32))\n",
    "                done = term or trunc\n",
    "\n",
    "                # 3) STORE TRANSITION IN REPLAY BUFFER:\n",
    "                self.experience.append((obs.copy(), scaled_action.numpy(), reward, next_obs.copy(), done))\n",
    "\n",
    "                # advance:\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # 4) RESET ENVIRONMENT STATE IF TERMINAL:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # 5) IF TIME TO UPDATE:\n",
    "                if len(self.experience) >= self.batch_size:\n",
    "                    # 6) FOR AS MANY UPDATES AS REQUIRED:\n",
    "                    for _ in range(self.gradient_steps):\n",
    "                        # 7) SAMPLE A BATCH:\n",
    "                        batch = random.sample(list(self.experience), self.batch_size)\n",
    "\n",
    "                        # unpack the batch:\n",
    "                        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                        # convert to tensors:\n",
    "                        states          = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                        actions         = tf.convert_to_tensor(actions, dtype = tf.float32)\n",
    "                        rewards         = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                        next_states     = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                        dones           = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                        # 8) GRADIENT STEP TO UPDATE NETWORKS:\n",
    "                        self.critic_update(states, rewards, actions, next_states, dones)\n",
    "\n",
    "                        if int(self.step_counter) % self.update_interval == 0:\n",
    "                            self.actor_update(states)\n",
    "                            self.temp_update(states) \n",
    "                            \n",
    "                        self.polyak_update()\n",
    "\n",
    "            # 9) ADVANCE REWARD HISTORY:\n",
    "            reward_history.append(episode_reward)\n",
    "\n",
    "            # 10) CHECK FOR EARLY STOPPING:\n",
    "            if train_metrics and len(reward_history) >= train_metrics[\"min_train\"]:\n",
    "                # compute a recent average:\n",
    "                recent_average = np.mean(reward_history[-train_metrics[\"over_last\"]:]).round(3)\n",
    "\n",
    "                # if the recent average is above the desired threshold:\n",
    "                if recent_average >= train_metrics[\"desired_score\"]:\n",
    "                    print(f\"environment solved in {len(reward_history)} episodes!\")\n",
    "                    print(f\"average reward was: {recent_average}\")\n",
    "\n",
    "                    # save weights:\n",
    "                    os.makedirs(train_metrics[\"model_path\"], exist_ok = True)\n",
    "                    try:\n",
    "                        # paths:\n",
    "                        actor_path = f\"{train_metrics[\"model_path\"]}/actor_weights.weights.h5\"\n",
    "                        critic_path_1 = f\"{train_metrics[\"model_path\"]}/critic_1_weights.weights.h5\"\n",
    "                        critic_path_2 = f\"{train_metrics[\"model_path\"]}/critic_2_weights.weights.h5\"\n",
    "\n",
    "                        # save weights:\n",
    "                        self.actor.save_weights(actor_path)\n",
    "                        self.critic_1.save_weights(critic_path_1)\n",
    "                        self.critic_2.save_weights(critic_path_2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"failed to save model: {e}\")\n",
    "                    break\n",
    "\n",
    "            # 11) PRINT TO USER:\n",
    "            if episode % 50 == 0:\n",
    "                print(f\"average reward is: {np.mean(reward_history[-train_metrics[\"over_last\"]:]).round(3)}\")               \n",
    "\n",
    "        return reward_history         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713f70c",
   "metadata": {},
   "source": [
    "# **Hyperparameter Definition**\n",
    "\n",
    "This section defines the hyperparameters used, as well as other important things like the model saving paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61450f0f",
   "metadata": {},
   "source": [
    "##### Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "425d5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env name:\n",
    "env_name = \"Reacher-v5\"\n",
    "\n",
    "# define hyperparameters:\n",
    "lr_a = 3e-4\n",
    "lr_c = 3e-4\n",
    "lr_t = 1e-4\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "layers = 3\n",
    "neurons = 256\n",
    "\n",
    "batch_size = 32\n",
    "buffer_size = int(1e6) \n",
    "action_scale = 1.0\n",
    "\n",
    "gradient_steps = 8\n",
    "update_interval = 8\n",
    "polyak_coefficient = 0.02\n",
    "\n",
    "# durations:\n",
    "warmup_length = 10000\n",
    "training_length = 5000\n",
    "\n",
    "# training parameters:\n",
    "match env_name:\n",
    "    case \"Reacher-v5\":\n",
    "        desired_score = -3.0\n",
    "        over_last = 5\n",
    "        min_train = 1000\n",
    "    case _:\n",
    "        raise ValueError(f\"Environment does not have training parameters set!\")\n",
    "\n",
    "# set training params:\n",
    "base_path = os.path.join(os.getcwd(), \"weights\")\n",
    "model_path = os.path.join(base_path, f\"{env_name}_{len(os.listdir(base_path)) + 1}\")\n",
    "\n",
    "train_metrics = {\"desired_score\" : desired_score,\n",
    "                 \"over_last\"     : over_last,\n",
    "                 \"min_train\"     : min_train,\n",
    "                 \"model_path\"    : model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06ca6d",
   "metadata": {},
   "source": [
    "##### Environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b7d4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear backend:\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# make environment:\n",
    "match env_name:\n",
    "    case \"Reacher-v5\":\n",
    "        env = gym.make(env_name, reward_dist_weight = 1.0)\n",
    "\n",
    "# make agent:\n",
    "agent = SAC_Agent(env = env, \n",
    "                  lr_a = lr_a, \n",
    "                  lr_c = lr_c,\n",
    "                  lr_t = lr_t,\n",
    "                  alpha = alpha,\n",
    "                  gamma = gamma, \n",
    "                  layers = layers, \n",
    "                  neurons = neurons, \n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size, \n",
    "                  action_scale = action_scale,\n",
    "                  gradient_steps = gradient_steps,\n",
    "                  update_interval = update_interval,\n",
    "                  polyak_coefficient = polyak_coefficient, \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fb7ce",
   "metadata": {},
   "source": [
    "# **Training** \n",
    "\n",
    "This section warms up the replay buffer and trains the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e570d99",
   "metadata": {},
   "source": [
    "##### Warm up the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45266fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup progress: 100%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 10000/10000 [00:43<00:00, 230.63it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# initialize:\n",
    "obs, _ = agent.env.reset()\n",
    "obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "\n",
    "# generate trajectories:\n",
    "for _ in tqdm(range(warmup_length), ncols = 100, colour = \"#33FF00\", desc = \"warmup progress\"):\n",
    "    # 1) SAMPLE A RANDOM ACTION:\n",
    "    mean, log_std = agent.actor(obs_tensor)                         # get network output\n",
    "    log_std = tf.clip_by_value(log_std, -20, 1)                     # clamp the log-std to prevent huge/small stds\n",
    "    std = tf.exp(log_std)                                           # convert from log_std to std\n",
    "    z = tf.random.normal(shape = tf.shape(mean))                    # sample the standard normal distribution\n",
    "    pre_tanh = mean + std*z                                         # use location-scaling to go from N(0,1) to N(Î¼, Ïƒ^2)\n",
    "\n",
    "    # squash action using tanh, scale action into action bounds:\n",
    "    scaled_action = tf.tanh(pre_tanh)[0] * np.array(agent.action_scale, dtype = np.float32)\n",
    "\n",
    "    # 2) ACT ON THE ENVIRONMENT:\n",
    "    next_obs, reward, term, trunc, _ = agent.env.step(scaled_action.numpy().astype(np.float32))\n",
    "\n",
    "    # 3) CHECK FOR COMPLETION:\n",
    "    done = term or trunc\n",
    "\n",
    "    # 4) APPEND TO BUFFER:\n",
    "    agent.experience.append((obs.copy(), scaled_action.numpy(), reward, next_obs.copy(), done))\n",
    "    obs = next_obs if not done else agent.env.reset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31d845",
   "metadata": {},
   "source": [
    "##### Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81d6a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                         \u001b[0m| 1/5000 [00:04<5:47:00,  4.16s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -39.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   1%|\u001b[38;2;51;255;0mâ–                                       \u001b[0m| 51/5000 [01:28<2:23:55,  1.74s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   2%|\u001b[38;2;51;255;0mâ–Š                                      \u001b[0m| 101/5000 [02:58<1:54:33,  1.40s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -18.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   3%|\u001b[38;2;51;255;0mâ–ˆâ–                                     \u001b[0m| 151/5000 [04:09<1:57:54,  1.46s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -15.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   4%|\u001b[38;2;51;255;0mâ–ˆâ–Œ                                     \u001b[0m| 201/5000 [05:35<2:16:02,  1.70s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -18.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   5%|\u001b[38;2;51;255;0mâ–ˆâ–‰                                     \u001b[0m| 251/5000 [07:03<2:15:27,  1.71s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -15.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   6%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–Ž                                    \u001b[0m| 301/5000 [08:32<2:36:28,  2.00s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -16.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   7%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–‹                                    \u001b[0m| 351/5000 [10:10<2:13:48,  1.73s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -13.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   8%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–                                   \u001b[0m| 401/5000 [11:35<2:14:06,  1.75s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   9%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–Œ                                   \u001b[0m| 451/5000 [13:01<2:20:11,  1.85s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -16.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  10%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–‰                                   \u001b[0m| 501/5000 [14:24<2:07:08,  1.70s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  11%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                  \u001b[0m| 551/5000 [16:39<2:37:06,  2.12s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -14.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  12%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  \u001b[0m| 601/5000 [18:07<2:05:35,  1.71s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  13%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  \u001b[0m| 651/5000 [19:43<2:04:34,  1.72s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -16.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  14%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 \u001b[0m| 701/5000 [21:10<2:06:50,  1.77s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -13.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  15%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 \u001b[0m| 751/5000 [22:59<3:30:07,  2.97s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -14.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  16%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                \u001b[0m| 801/5000 [24:29<2:04:46,  1.78s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  17%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                \u001b[0m| 851/5000 [25:59<2:06:47,  1.83s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  18%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                \u001b[0m| 901/5000 [27:32<2:07:59,  1.87s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  19%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               \u001b[0m| 951/5000 [29:04<2:05:13,  1.86s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  20%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              \u001b[0m| 1001/5000 [30:42<2:13:39,  2.01s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  21%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              \u001b[0m| 1051/5000 [32:21<2:14:15,  2.04s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -14.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  22%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                             \u001b[0m| 1101/5000 [34:02<2:08:35,  1.98s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  23%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             \u001b[0m| 1151/5000 [35:43<2:11:44,  2.05s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  24%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            \u001b[0m| 1201/5000 [37:27<2:11:03,  2.07s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -13.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  25%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            \u001b[0m| 1251/5000 [39:13<2:11:37,  2.11s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  26%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            \u001b[0m| 1301/5000 [41:00<2:09:50,  2.11s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  27%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           \u001b[0m| 1351/5000 [42:47<2:11:51,  2.17s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  28%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           \u001b[0m| 1401/5000 [44:33<2:04:41,  2.08s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  29%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           \u001b[0m| 1451/5000 [46:27<2:12:53,  2.25s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  30%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          \u001b[0m| 1501/5000 [48:20<2:15:30,  2.32s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -13.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  31%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          \u001b[0m| 1551/5000 [50:15<2:11:33,  2.29s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -8.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  32%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         \u001b[0m| 1601/5000 [52:06<2:08:24,  2.27s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  33%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         \u001b[0m| 1651/5000 [53:59<2:06:39,  2.27s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  34%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         \u001b[0m| 1701/5000 [55:50<1:56:56,  2.13s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  35%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        \u001b[0m| 1751/5000 [57:37<1:55:43,  2.14s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  36%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        \u001b[0m| 1801/5000 [59:30<2:02:05,  2.29s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  37%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      \u001b[0m| 1851/5000 [1:01:27<2:00:41,  2.30s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  38%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      \u001b[0m| 1901/5000 [1:03:24<2:07:14,  2.46s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  39%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      \u001b[0m| 1951/5000 [1:05:23<1:56:37,  2.29s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -13.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  40%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     \u001b[0m| 2001/5000 [1:07:09<1:49:38,  2.19s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  41%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     \u001b[0m| 2051/5000 [1:08:55<1:46:10,  2.16s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  42%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    \u001b[0m| 2101/5000 [1:10:46<1:47:38,  2.23s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -8.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  43%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    \u001b[0m| 2151/5000 [1:12:52<1:59:09,  2.51s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  44%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    \u001b[0m| 2201/5000 [1:14:54<1:49:01,  2.34s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -15.106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  45%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   \u001b[0m| 2251/5000 [1:16:50<1:41:46,  2.22s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  46%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   \u001b[0m| 2301/5000 [1:18:49<1:42:39,  2.28s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  47%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   \u001b[0m| 2351/5000 [1:20:47<1:43:48,  2.35s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  48%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  \u001b[0m| 2401/5000 [1:22:45<1:40:35,  2.32s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  49%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  \u001b[0m| 2451/5000 [1:24:49<1:47:54,  2.54s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -12.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  50%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  \u001b[0m| 2501/5000 [1:26:56<1:43:50,  2.49s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  51%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 \u001b[0m| 2551/5000 [1:29:04<1:44:37,  2.56s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  52%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 \u001b[0m| 2601/5000 [1:31:14<1:46:37,  2.67s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -13.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  53%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 \u001b[0m| 2651/5000 [1:33:19<1:39:05,  2.53s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  54%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                \u001b[0m| 2701/5000 [1:35:27<1:46:41,  2.78s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  55%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                \u001b[0m| 2751/5000 [1:37:40<1:41:04,  2.70s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  56%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               \u001b[0m| 2801/5000 [1:39:54<1:37:37,  2.66s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -8.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  57%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               \u001b[0m| 2851/5000 [1:42:09<1:37:09,  2.71s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  58%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               \u001b[0m| 2901/5000 [1:44:31<1:46:57,  3.06s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -11.247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  59%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              \u001b[0m| 2951/5000 [1:46:58<1:42:12,  2.99s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -8.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  60%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              \u001b[0m| 3001/5000 [1:49:27<1:37:59,  2.94s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  61%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              \u001b[0m| 3051/5000 [1:51:59<1:39:54,  3.08s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  62%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             \u001b[0m| 3101/5000 [1:54:31<1:37:04,  3.07s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  63%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             \u001b[0m| 3151/5000 [1:57:06<1:35:40,  3.10s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -10.302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  64%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             \u001b[0m| 3201/5000 [2:00:54<4:06:04,  8.21s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is: -9.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:  64%|\u001b[38;2;51;255;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             \u001b[0m| 3207/5000 [2:01:36<1:07:59,  2.28s/it]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_metrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 386\u001b[0m, in \u001b[0;36mSAC_Agent.training\u001b[1;34m(self, training_length, train_metrics)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_counter) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_update(states)\n\u001b[1;32m--> 386\u001b[0m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    388\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolyak_update()\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# 9) ADVANCE REWARD HISTORY:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\mtidd\\OneDrive\\Desktop\\rl\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_history = agent.training(training_length = training_length, train_metrics = train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1109dd",
   "metadata": {},
   "source": [
    "# **Visualization**\n",
    "\n",
    "This section visualizes the results, both in the form of plotting and actually simulating the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efe988",
   "metadata": {},
   "source": [
    "##### Visualize reward vs. episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving average function:\n",
    "def moving_average(interval, window_size):\n",
    "    window = np.ones(int(window_size)) / float(window_size)\n",
    "    return np.convolve(interval, window, 'valid')\n",
    "\n",
    "filtered_data = moving_average(reward_history, 4)\n",
    "last_few = np.mean(reward_history[-over_last:]).round(3)\n",
    "\n",
    "figure = plt.figure(figsize = (10, 4))\n",
    "plt.plot(reward_history)\n",
    "plt.plot(filtered_data, 'r-')\n",
    "plt.minorticks_on\n",
    "plt.title('reward earned vs. episode')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel(f'reward earned')\n",
    "plt.figtext(x = 0.5, y = -0.05, s = fr\"$\\mathrm{{\\alpha_a}}$: {lr_a} | $\\mathrm{{\\alpha_c}}$: {lr_c} | $\\mathrm{{\\alpha_t}}$: {lr_t} | $\\mathrm{{\\alpha}}$: {alpha} | $\\mathrm{{\\gamma}}$ : {gamma} | $\\mathrm{{n_l}}$: {layers} | $\\mathrm{{n_n}}$: {neurons} | $\\mathrm{{n_{{bs}}}}$: {batch_size} | $\\mathrm{{n_{{buff}}}}$: {buffer_size} | $\\mathrm{{\\rho}}$: {polyak_coefficient}\", ha = 'center', va = 'center')\n",
    "plt.figtext(x = 0.5, y = -0.125, s = fr\"mean over last {over_last} episodes: {last_few}\", ha = 'center', va = 'center')\n",
    "\n",
    "# save figure:\n",
    "if len(model_path) != 0:\n",
    "    try:\n",
    "        plt.savefig(f\"{model_path}/reward_episode_plot.png\", bbox_inches = 'tight')\n",
    "    except Exception as e:\n",
    "        print(f\"could not save: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9bf4dc",
   "metadata": {},
   "source": [
    "##### Visualize agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fe6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False\n",
    "width = 1280\n",
    "height = 1280\n",
    "\n",
    "if visualize:\n",
    "    # handle the render settings based on the environment:\n",
    "    match env_name:\n",
    "            case \"Reacher-v5\":\n",
    "                default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 1.5, \"lookat\" : [0.0, 0.0, 0.25]}\n",
    "                env = gym.make(env_name,\n",
    "                            render_mode = \"human\",\n",
    "                            reward_dist_weight = 1.0,\n",
    "                            width = width, \n",
    "                            height = height,\n",
    "                            default_camera_config = default_camera_config, \n",
    "                            max_episode_steps = 50)  \n",
    "\n",
    "    # pass the environment to the agent:\n",
    "    agent.env = env\n",
    "    obs, _ = agent.env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # 0) RENDER:\n",
    "        agent.env.render()\n",
    "\n",
    "        # 1) SAMPLE A RANDOM ACTION:\n",
    "        obs_tensor = tf.convert_to_tensor([obs], dtype = tf.float32)\n",
    "        mean, log_std = agent.actor(obs_tensor)                         # get network output\n",
    "        log_std = tf.clip_by_value(log_std, -20, 1)                     # clamp the log-std to prevent huge/small stds\n",
    "        std = tf.exp(log_std)                                           # convert from log_std to std\n",
    "        z = tf.random.normal(shape = tf.shape(mean))                    # sample the standard normal distribution\n",
    "        pre_tanh = mean + std*z                                         # use location-scaling to go from N(0,1) to N(Î¼, Ïƒ^2)\n",
    "\n",
    "        # squash action using tanh, scale action into action bounds:\n",
    "        scaled_action = tf.tanh(pre_tanh)[0] * np.array(agent.action_scale, dtype = np.float32)\n",
    "\n",
    "        # add noise:\n",
    "        scaled_action += tf.random.normal(shape = scaled_action.shape, dtype = scaled_action.dtype, stddev = 0.0)\n",
    "\n",
    "        # 2) DO ACTION:\n",
    "        next_obs, reward, term, trunc, _ = agent.env.step(scaled_action.numpy().astype(np.float32))\n",
    "\n",
    "        # 3) ADVANCE:\n",
    "        obs = next_obs\n",
    "\n",
    "        # 4) CHECK FOR COMPLETION:\n",
    "        done = term or trunc\n",
    "    \n",
    "    agent.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
