{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe44c84",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is used for testing and visualizing the performance of DQN-Agents by loading the weights of the Q-Network. The environment that is used for testing is the ``cartpole`` environment offered through Gymnasium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adba098",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fea62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ec461",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5e309",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec0a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e11137",
   "metadata": {},
   "source": [
    "##### DQN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3849b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN agent class:\n",
    "class DQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, \n",
    "        env: gym.Env, \n",
    "        gamma: float, \n",
    "        lr: float,\n",
    "        epsilon: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        batch_size: int,\n",
    "        buffer_size: int,\n",
    "        target_update_freq: int, \n",
    "        train_freq: int,\n",
    "        layers = int,\n",
    "        neurons = int, \n",
    "        seed = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses a DQN to learn an optimal policy, through the use of approximator neural network \n",
    "        to approximate action-value Q, and a target network to generate a Q-target used in the updating of Q(s,a). this is done to prevent updates\n",
    "        to the network weights from changing the target, meaning that we aren't bootstrapping towards a changing target. this helps to stabilize the learning.\n",
    "\n",
    "        env:                    a gymnasium environment\n",
    "        gamma:                  a float value indicating the discount factor γ\n",
    "        lr:                     a float value indicating the learning rate α\n",
    "        epsilon:                a float value indicating the action-selection probability ε\n",
    "        epsilon_min:            a float value indicating the minimum ε value\n",
    "        epsilon_decay:          a float value indicating the decay rate of ε\n",
    "        batch_size:             an int representing the batch size sampled from the experience\n",
    "        buffer_size:            an int representing the size of the memory buffer\n",
    "        target_update_freq:     an int representing how frequently the target network weights should be updated\n",
    "        train_freq:             an int representing how frequently training should occur\n",
    "        layers:                 an int representing the number of layers in each network\n",
    "        neurons:                an int representing the number of neurons in each network\n",
    "        seed:                   an int representing the random seed of the environment, for reproducability\n",
    "\n",
    "        nS:         an int representing the number of states observed, each of which is continuous\n",
    "        nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        q_network:                  a Keras sequential neural network representing the actual function approximator\n",
    "        target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "        experience:                 an empty deque used to hold the experience history of the agent, limited to buffer_size\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "\n",
    "        # experience history and mini-batch size:\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # initialize networks:\n",
    "        self.q_network = make_model(layers = layers, neurons = neurons, rate = lr,\n",
    "                                                norm = True, drop = True,\n",
    "                                                input_shape = self.nS, output_shape = self.nA,\n",
    "                                                loss_function = 'mse')\n",
    "        self.target_network = keras.models.clone_model(self.q_network)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # set target update and training frequencies:\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_freq = train_freq\n",
    "\n",
    "        # set the seed:\n",
    "        self.seed = seed\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # define a decorated function to infer Q's from batched states (this is the implicit policy):\n",
    "    @tf.function\n",
    "    def get_qs(self, obs_batch):\n",
    "        return self.q_network(obs_batch)\n",
    "    \n",
    "    # define a decorated function to perform the DQN training step for updating Q network weights:\n",
    "    @tf.function\n",
    "    def training_step(self, states, actions, rewards, next_states, dones):\n",
    "        # track auto differentiation:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1) do a forward pass to get Q values:\n",
    "            # this is all the Q values from every state:\n",
    "            q_all = self.q_network(states)\n",
    "\n",
    "            # find relevant index of actions that will be selected:\n",
    "            index = tf.stack([tf.range(tf.shape(actions)[0]), actions], axis = 1)\n",
    "\n",
    "            # gather up the Q values that correspond to actions actually taken:\n",
    "            q_selected = tf.gather_nd(q_all, index)\n",
    "\n",
    "            # 2) compute TD-targets:\n",
    "            # TD-target is computed with S', A', w-:\n",
    "            q_next = self.target_network(next_states)\n",
    "\n",
    "            # get the Q value corresponding to the max over the actions:\n",
    "            max_q_next = tf.reduce_max(q_next, axis = 1)\n",
    "\n",
    "            # compute actual TD-targets:\n",
    "            targets = tf.stop_gradient(rewards + (1 - dones) * self.gamma * max_q_next)\n",
    "\n",
    "            # 3) MSE loss between the Qs that correspond to taken actions and the TD-target:\n",
    "            loss = tf.reduce_mean(tf.square(q_selected - targets))\n",
    "        \n",
    "        # 4) backpropagate and update the weights:\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length):\n",
    "\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), colour = \"#33FF00\", ncols = 100, desc = \"training progress\"):\n",
    "            # reset environment:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # ε-greedy policy:\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    obs_batch = tf.expand_dims(tf.convert_to_tensor(obs, dtype=tf.float32), 0)\n",
    "                    qs = self.get_qs(obs_batch)\n",
    "                    action = tf.argmax(qs[0]).numpy()\n",
    "\n",
    "                # interact with the environment:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # sample a batch of experience:\n",
    "                if len(self.replay_buffer) >= self.batch_size and self.step_counter % self.train_freq == 0:\n",
    "                    # get a batch:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "                    # unpack the batch:\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                    # convert to tensors:\n",
    "                    states = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                    actions = tf.convert_to_tensor(actions, dtype = tf.int32)\n",
    "                    rewards = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                    next_states = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                    dones = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                    # single graph call:\n",
    "                    self.training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "                    # update target network periodically:\n",
    "                    if self.step_counter % self.target_update_freq == 0:\n",
    "                        self.target_network.set_weights(self.q_network.get_weights())\n",
    "                \n",
    "            # decay epsilon:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            # advance reward history:\n",
    "            reward_history[episode] = episode_reward\n",
    "        \n",
    "        return reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc6261",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section utilizes the above DQN class to create an environment, and instantiate an agent. However, instead of training the agent, weights are loaded for the Q-Network, such that the model can be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d7651",
   "metadata": {},
   "source": [
    "##### Specify hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06319f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent hyperparameters:\n",
    "lr = 1e-5                   # learning rate α\n",
    "gamma = 0.99                # discount factor γ\n",
    "epsilon = 1.0               # starting value of ε\n",
    "epsilon_min = 0.1           # final value of ε\n",
    "epsilon_decay = 0.999       # decay rate of ε\n",
    "\n",
    "buffer_size = 2000          # size of the replay buffer\n",
    "batch_size = 32             # amount sampled from buffer\n",
    "target_update_freq = 1000   # number of elapsed steps before target network is updated\n",
    "train_freq = 1              # frequency at which the training occurs\n",
    "\n",
    "training_length = 2000      # how many episodes to train the network for\n",
    "warmup_length = 1000        # how many steps of experience to populate the replay buffer\n",
    "\n",
    "neurons = 128               # how many neurons to have in each layer of the network\n",
    "layers = 3                  # how many layers to have in each network\n",
    "\n",
    "seed = None                   # random seed for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb9c56",
   "metadata": {},
   "source": [
    "##### Initialize the environment and the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95acf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# weights path:\n",
    "weights_index = 0\n",
    "weights_folder = os.path.join(os.getcwd(), \"weights\")\n",
    "weights_path = os.path.join(weights_folder, os.listdir(weights_folder)[weights_index], \"dqn_weights.weights.h5\")\n",
    "\n",
    "# create the environment:\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# clear backend:\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# instantiate the agent:\n",
    "dqn_agent = DQN_Agent(env = env, \n",
    "                  gamma = gamma, \n",
    "                  lr = lr,\n",
    "                  epsilon = epsilon,\n",
    "                  epsilon_min = epsilon_min,\n",
    "                  epsilon_decay = epsilon_decay,\n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size,\n",
    "                  target_update_freq = target_update_freq, \n",
    "                  train_freq = train_freq,\n",
    "                  layers = layers,\n",
    "                  neurons = neurons, \n",
    "                  seed = seed)\n",
    "\n",
    "# load the weights:\n",
    "dqn_agent.q_network.load_weights(weights_path)\n",
    "\n",
    "# seed environment:\n",
    "_, _ = dqn_agent.env.reset(seed = dqn_agent.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a3673",
   "metadata": {},
   "source": [
    "# **Evaluate the Agent**\n",
    "\n",
    "This section evaluates the performance of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e44b97",
   "metadata": {},
   "source": [
    "##### Testing the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de3a5acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing progress:: 100%|\u001b[38;2;51;255;0m██████████████████████████████████████████\u001b[0m| 100/100 [00:18<00:00,  5.53it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 test episodes: 500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test parameters:\n",
    "test_episodes = 100\n",
    "episode_rewards = []\n",
    "\n",
    "# for every episode:\n",
    "for episode in tqdm(range(test_episodes), colour = \"#33FF00\", ncols = 100, desc = \"testing progress:\"):\n",
    "    # prepare for episode:\n",
    "    obs, _ = dqn_agent.env.reset()      # reset environment\n",
    "    episode_reward = 0                  # reward counter for episode\n",
    "    done = False                        # completion flag\n",
    "\n",
    "    # while false:\n",
    "    while not done:\n",
    "        # use Q-Network to determine the value of each action:\n",
    "        q_values = dqn_agent.q_network.predict_on_batch(obs.reshape(1,-1))\n",
    "\n",
    "        # pick the best action:\n",
    "        action = tf.argmax(q_values[0]).numpy()\n",
    "\n",
    "        # take the action on the environment:\n",
    "        next_obs, reward, term, trunc, _ = dqn_agent.env.step(action)\n",
    "\n",
    "        # advance reward and state:\n",
    "        episode_reward += reward\n",
    "        obs = next_obs\n",
    "\n",
    "        # check for completion:\n",
    "        done = term or trunc\n",
    "    \n",
    "    # advance reward per episode list:\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "# average out the reward over the test duration:\n",
    "average_reward = sum(episode_rewards) / test_episodes\n",
    "print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe2b39",
   "metadata": {},
   "source": [
    "##### Visualizing the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f714c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the new environment:\n",
    "env = gym.make(\"CartPole-v1\", render_mode = 'human', max_episode_steps = 1000)\n",
    "\n",
    "# give the agent the new environment:\n",
    "dqn_agent.env = env\n",
    "\n",
    "# prep the visualization:\n",
    "obs, _ = dqn_agent.env.reset()\n",
    "done = False\n",
    "\n",
    "# while false:\n",
    "while not done:\n",
    "    # start rendering the environment:\n",
    "    env.render()\n",
    "\n",
    "    # use Q-Network to determine the value of each action:\n",
    "    q_values = dqn_agent.q_network.predict_on_batch(obs.reshape(1,-1))\n",
    "\n",
    "    # pick the best action:\n",
    "    action = tf.argmax(q_values[0]).numpy()\n",
    "\n",
    "    # take the action on the environment:\n",
    "    next_obs, reward, term, trunc, _ = dqn_agent.env.step(action)\n",
    "\n",
    "    # advance state:\n",
    "    obs = next_obs\n",
    "\n",
    "    # check the completion status:\n",
    "    done = term or trunc\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
