{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbaedc1",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for testing hyperparameter optimization using `Optuna`, which is a hyperparameter optimization framework used to automate hyperparameter searching. The idea is to take the DQN implementation developed for `Cartpole`, both the base and swingup versions, and perform a hyperparameter search to optimize their performance using `Optuna`. The hyperparameters that will be optimized are:\n",
    "\n",
    "* The learning rate: α\n",
    "* The discount rate: γ\n",
    "* The epsilon decay rate: $ε_{d}$\n",
    "* The final epsilon value: $ε_{f}$\n",
    "* The replay buffer size: $n_{buff}$\n",
    "* The batch size: $n_{batch}$\n",
    "* The target update frequency: $f_{target}$\n",
    "* The training frequency: $f_{train}$\n",
    "* The warmup length: $n_{warm}$\n",
    "* The number of neurons per layer: $n_{neurons}$\n",
    "* The number of layers: $n_{layers}$\n",
    "\n",
    "With the goal being to maximize the average return per episode while training the model over a set number of episodes, for a set number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180b6c7",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ce066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "from wrappers.swingup_wrapper import SwingUpWrapper\n",
    "import numpy as np\n",
    "import optuna\n",
    "# optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import parallel_backend\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0e01a",
   "metadata": {},
   "source": [
    "# **Environment Setup** \n",
    "\n",
    "This section sets up the environment and defines all relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff35b3",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee25346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c8dd2",
   "metadata": {},
   "source": [
    "##### DQN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba039ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN agent class:\n",
    "class DQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, \n",
    "        env: gym.Env, \n",
    "        gamma: float, \n",
    "        lr: float,\n",
    "        epsilon: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        batch_size: int,\n",
    "        buffer_size: int,\n",
    "        target_update_freq: int, \n",
    "        train_freq: int,\n",
    "        layers = int,\n",
    "        neurons = int, \n",
    "        seed = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses a DQN to learn an optimal policy, through the use of approximator neural network \n",
    "        to approximate action-value Q, and a target network to generate a Q-target used in the updating of Q(s,a). this is done to prevent updates\n",
    "        to the network weights from changing the target, meaning that we aren't bootstrapping towards a changing target. this helps to stabilize the learning.\n",
    "\n",
    "        env:                    a gymnasium environment\n",
    "        gamma:                  a float value indicating the discount factor γ\n",
    "        lr:                     a float value indicating the learning rate α\n",
    "        epsilon:                a float value indicating the action-selection probability ε\n",
    "        epsilon_min:            a float value indicating the minimum ε value\n",
    "        epsilon_decay:          a float value indicating the decay rate of ε\n",
    "        batch_size:             an int representing the batch size sampled from the experience\n",
    "        buffer_size:            an int representing the size of the memory buffer\n",
    "        target_update_freq:     an int representing how frequently the target network weights should be updated\n",
    "        train_freq:             an int representing how frequently training should occur\n",
    "        layers:                 an int representing the number of layers in each network\n",
    "        neurons:                an int representing the number of neurons in each network\n",
    "        seed:                   an int representing the random seed of the environment, for reproducability\n",
    "\n",
    "        nS:         an int representing the number of states observed, each of which is continuous\n",
    "        nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        q_network:                  a Keras sequential neural network representing the actual function approximator\n",
    "        target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "        experience:                 an empty deque used to hold the experience history of the agent, limited to buffer_size\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "\n",
    "        # experience history and mini-batch size:\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # initialize networks:\n",
    "        self.q_network = make_model(layers = layers, neurons = neurons, rate = lr,\n",
    "                                                norm = True, drop = True,\n",
    "                                                input_shape = self.nS, output_shape = self.nA,\n",
    "                                                loss_function = 'mse')\n",
    "        self.target_network = keras.models.clone_model(self.q_network)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # set target update and training frequencies:\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_freq = train_freq\n",
    "\n",
    "        # set the seed:\n",
    "        self.seed = seed\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # define a decorated function to infer Q's from batched states (this is the implicit policy):\n",
    "    @tf.function\n",
    "    def get_qs(self, obs_batch):\n",
    "        return self.q_network(obs_batch)\n",
    "    \n",
    "    # define a decorated function to perform the DQN training step for updating Q network weights:\n",
    "    @tf.function\n",
    "    def training_step(self, states, actions, rewards, next_states, dones):\n",
    "        # track auto differentiation:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1) do a forward pass to get Q values:\n",
    "            # this is all the Q values from every state:\n",
    "            q_all = self.q_network(states)\n",
    "\n",
    "            # find relevant index of actions that will be selected:\n",
    "            index = tf.stack([tf.range(tf.shape(actions)[0]), actions], axis = 1)\n",
    "\n",
    "            # gather up the Q values that correspond to actions actually taken:\n",
    "            q_selected = tf.gather_nd(q_all, index)\n",
    "\n",
    "            # 2) compute TD-targets:\n",
    "            # TD-target is computed with S', A', w-:\n",
    "            q_next = self.target_network(next_states)\n",
    "\n",
    "            # get the Q value corresponding to the max over the actions:\n",
    "            max_q_next = tf.reduce_max(q_next, axis = 1)\n",
    "\n",
    "            # compute actual TD-targets:\n",
    "            targets = tf.stop_gradient(rewards + (1 - dones) * self.gamma * max_q_next)\n",
    "\n",
    "            # 3) MSE loss between the Qs that correspond to taken actions and the TD-target:\n",
    "            loss = tf.reduce_mean(tf.square(q_selected - targets))\n",
    "        \n",
    "        # 4) backpropagate and update the weights:\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length, on_episode_end = None):\n",
    "\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        # for episode in tqdm(range(training_length), colour = \"#33FF00\", ncols = 100, desc = \"training progress\"):\n",
    "        for episode in range(training_length):\n",
    "            # reset environment:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # ε-greedy policy:\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    obs_batch = tf.expand_dims(tf.convert_to_tensor(obs, dtype = tf.float32), 0)\n",
    "                    qs = self.get_qs(obs_batch)\n",
    "                    action = tf.argmax(qs[0]).numpy()\n",
    "\n",
    "                # interact with the environment:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # sample a batch of experience:\n",
    "                if len(self.replay_buffer) >= self.batch_size and self.step_counter % self.train_freq == 0:\n",
    "                    # get a batch:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "                    # unpack the batch:\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                    # convert to tensors:\n",
    "                    states = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                    actions = tf.convert_to_tensor(actions, dtype = tf.int32)\n",
    "                    rewards = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                    next_states = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                    dones = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                    # single graph call:\n",
    "                    self.training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "                    # update target network periodically:\n",
    "                    if self.step_counter % self.target_update_freq == 0:\n",
    "                        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "            # decay epsilon:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            # advance reward history:\n",
    "            reward_history[episode] = episode_reward\n",
    "\n",
    "            # callback for pruning:\n",
    "            if on_episode_end is not None:\n",
    "                on_episode_end(episode, episode_reward)\n",
    "        \n",
    "        return reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec97a1c",
   "metadata": {},
   "source": [
    "##### Objective function for Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ad328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an objective function that takes a trial object:\n",
    "def objective(trial):\n",
    "    # 0) initialization:\n",
    "    # clear the backend:\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # define parameters for moving average used in pruning:\n",
    "    window_size = 20\n",
    "    recent_rewards = deque(maxlen = window_size)\n",
    "    \n",
    "    # define a callback that can be used every episode:\n",
    "    def prune_callback(episode, episode_reward):\n",
    "        # collect episode rewards into the deque:\n",
    "        recent_rewards.append(episode_reward)\n",
    "\n",
    "        # compute average reward:\n",
    "        avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "\n",
    "        # prune report:\n",
    "        trial.report(avg_reward, step = episode)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # 1) create an environment:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    # seeding:\n",
    "    seed = trial.number\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    _, _ = env.reset(seed = seed)\n",
    "\n",
    "    # 2) create an agent:\n",
    "    # instantiate the agent:\n",
    "    dqn_agent = DQN_Agent(env                   =       env,\n",
    "                          gamma                 =       trial.suggest_categorical('gamma', [0.95, 0.99, 0.995, 0.999]),\n",
    "                          lr                    =       trial.suggest_categorical('lr', [1e-5, 1e-4, 1e-3, 1e-2]),\n",
    "                          epsilon               =       1.0,\n",
    "                          epsilon_min           =       trial.suggest_categorical('epsilon_min', [0.01, 0.05, 0.1]),\n",
    "                          epsilon_decay         =       trial.suggest_categorical('epsilon_decay', [0.99, 0.9925, 0.995]),\n",
    "                          batch_size            =       trial.suggest_categorical('batch_size', [32, 64, 128, 256]),\n",
    "                          buffer_size           =       trial.suggest_int('buffer_size', low = 1000, high = 10000, step = 1000),\n",
    "                          target_update_freq    =       trial.suggest_categorical('target_update_freq', [10, 100, 1000]),\n",
    "                          train_freq            =       trial.suggest_categorical('train_freq', [1, 2, 4, 8]), \n",
    "                          layers                =       trial.suggest_categorical('layers', [2, 3]), \n",
    "                          neurons               =       trial.suggest_categorical('neurons', [32, 64, 128])\n",
    "                          )\n",
    "    \n",
    "\n",
    "    # 3) populate the replay buffer:\n",
    "    obs, _ = dqn_agent.env.reset()\n",
    "    for _ in range(trial.suggest_int('warmup_length', low = 1000, high = 10000, step = 1000)):\n",
    "        # sample random action:\n",
    "        action = dqn_agent.env.action_space.sample()\n",
    "\n",
    "        # act on environment:\n",
    "        next_obs, reward, term, trunc, _ = dqn_agent.env.step(action)\n",
    "\n",
    "        # check for completion:\n",
    "        done = term or trunc\n",
    "\n",
    "        # append experience to agent's replay buffer:\n",
    "        dqn_agent.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs if not done else dqn_agent.env.reset()[0]\n",
    "\n",
    "    # 4) train the agent:\n",
    "    reward_history = dqn_agent.training(training_length = 1000,\n",
    "                                        on_episode_end = prune_callback)\n",
    "    \n",
    "    # 5) close environment:\n",
    "    dqn_agent.env.close()\n",
    "\n",
    "    # 6) save the seed:\n",
    "    trial.set_user_attr(\"seed\", seed)\n",
    "\n",
    "    # 7) return metric:\n",
    "    return float(np.mean(reward_history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6a933",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section performs the hyperparameter searching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8026a",
   "metadata": {},
   "source": [
    "##### Query user about mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fe78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = input(f\"Please select mode: Optimize or Analyze\").lower()\n",
    "\n",
    "if mode not in [\"optimize\", \"analyze\"]:\n",
    "    raise SystemExit(f\"Invalid mode selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5e4fe",
   "metadata": {},
   "source": [
    "##### Create study and optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.delete_study(storage = \"sqlite:///optuna_results.db\", study_name = \"DQN_HPO_2025_08_07_12_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0de687",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"optimize\":\n",
    "    # get run time:\n",
    "    time = datetime.datetime.now()\n",
    "    formatted_time = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "    # set study name:\n",
    "    # study_name = f\"DQN_HPO_{formatted_time}\"\n",
    "    study_name = \"DQN_HPO_2025_08_07_12_04\"\n",
    "\n",
    "    # create a study object and optimize the objective function:\n",
    "    study = optuna.create_study(study_name = study_name, \n",
    "                                direction = 'maximize',\n",
    "                                storage = \"sqlite:///optuna_results.db\",\n",
    "                                load_if_exists = True, \n",
    "                                pruner = optuna.pruners.MedianPruner(n_startup_trials = 10,\n",
    "                                                                    n_warmup_steps = 500,\n",
    "                                                                    interval_steps = 50))\n",
    "    \n",
    "    # run parallelized process:\n",
    "    with parallel_backend(\"loky\"):\n",
    "        study.optimize(objective, n_trials = 100, show_progress_bar = True, n_jobs = 4)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070462b",
   "metadata": {},
   "source": [
    "##### Summarize results of study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3be0ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"analyze\":\n",
    "    # manually set the study to be analyzed:\n",
    "    study_name = study_name\n",
    "\n",
    "    # load the study:\n",
    "    study = optuna.load_study(study_name = study_name,\n",
    "                              storage = \"sqlite:///optuna_results.db\")\n",
    "    \n",
    "    # summarize study:\n",
    "    trials_data = []\n",
    "    intermediate_values = []\n",
    "    for trial in study.best_trials:\n",
    "        # initialize dicts:\n",
    "        trial_data = {}\n",
    "        intermediate_value = {}\n",
    "\n",
    "        # get trial information:\n",
    "        trial_data[\"model\"] = trial.number\n",
    "        trial_data['value'] = trial.value\n",
    "        trial_data.update(trial.params.copy())\n",
    "        trials_data.append(trial_data)\n",
    "\n",
    "        # get intermediate values:\n",
    "        intermediate_value.update(trial.intermediate_values.copy())\n",
    "        intermediate_values.append(intermediate_value)\n",
    "\n",
    "    # make data frame:\n",
    "    summary = pd.DataFrame(trials_data)\n",
    "\n",
    "    # show the best 10 models:\n",
    "    summary.head(5).sort_values(by = 'value', ascending = False)\n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a456e2",
   "metadata": {},
   "source": [
    "##### Visualize results of study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"analyze\":\n",
    "    # setup grid for subplots:\n",
    "    n_plots = len(intermediate_values)\n",
    "    cols = 5\n",
    "    rows = (n_plots // cols) + (1 if n_plots % cols > 0 else 0) \n",
    "\n",
    "    # create the figure and axes:\n",
    "    fig, axes = plt.subplots(rows, cols, figsize = (15, 3 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # loop through trials and plot on each subplot:\n",
    "    for i, trial in enumerate(intermediate_values):\n",
    "        x = list(trial.keys())\n",
    "        y = list(trial.values())\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.plot(x, y)\n",
    "        ax.set_title(f\"model: {i}\")\n",
    "        ax.set_xlabel(\"episode\")\n",
    "        ax.set_ylabel(\"average reward\")\n",
    "\n",
    "    # hide unused subplots (if there are any):\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # adjust layout to prevent overlapping:\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1c803",
   "metadata": {},
   "source": [
    "##### Get params of best model based on the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1353951",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"analyze\":\n",
    "    # visually, I determined that the best looking model was model:\n",
    "    model_number = 30\n",
    "\n",
    "    # get its values:\n",
    "    summary.loc[model_number]\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
