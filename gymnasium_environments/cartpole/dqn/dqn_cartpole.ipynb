{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fcff23",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a Deep Q-Network (DQN) algorithm for the ``cartpole`` environment offered through Gymnasium. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59a22a",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a780bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ec607",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc50bb",
   "metadata": {},
   "source": [
    "##### Function for Making Keras Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5353",
   "metadata": {},
   "source": [
    "##### DQN Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9628f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DQN agent class:\n",
    "class DQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, \n",
    "        env: gym.Env, \n",
    "        gamma: float, \n",
    "        alpha: float,\n",
    "        epsilon: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        batch_size: int,\n",
    "        buffer_size: int,\n",
    "        target_update_freq: int, \n",
    "        layers = int,\n",
    "        neurons = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses a DQN to learn an optimal policy, through the use of approximator neural network \n",
    "        to approximate action-value Q, and a target network to generate a Q-target used in the updating of Q(s,a). this is done to prevent updates\n",
    "        to the network weights from changing the target, meaning that we aren't bootstrapping towards a changing target. this helps to stabilize the learning.\n",
    "\n",
    "        env:                    a gymnasium environment\n",
    "        gamma:                  a float value indicating the discount factor\n",
    "        alpha:                  a float value indicating the learning rate\n",
    "        epsilon:                a float value indicating the action-selection probability ε\n",
    "        epsilon_min:            a float value indicating the minimum ε value\n",
    "        epsilon_decay:          a float value indicating the decay rate of ε\n",
    "        batch_size:             an int representing the batch size sampled from the experience\n",
    "        buffer_size:            an int representing the size of the memory buffer\n",
    "        target_update_freq:     an int representing how frequently the target network weights should be updated\n",
    "        layers:                 an int representing the number of layers in each network\n",
    "        neurons:                an int representing the number of neurons in each network\n",
    "\n",
    "        nS:         an int representing the number of states observed, each of which is continuous\n",
    "        nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        approximator_network:       a Keras sequential neural network representing the actual function approximator\n",
    "        target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "        experience:                 an empty deque used to hold the experience history of the agent, limited to buffer_size\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # experience history and mini-batch size:\n",
    "        self.experience = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "\n",
    "        # initialize networks:\n",
    "        self.approximator_network = make_model(layers = layers, neurons = neurons, rate = alpha,\n",
    "                                                norm = False, drop = False,\n",
    "                                                input_shape = self.nS, output_shape = self.nA,\n",
    "                                                loss_function = 'mse')\n",
    "        self.target_network = keras.models.clone_model(self.approximator_network)\n",
    "        self.target_network.set_weights(self.approximator_network.get_weights())\n",
    "\n",
    "        # set target network update frequency:\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # function to store experience in the replay buffer:\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        this function populates the replay buffer from transitions in a given step:\n",
    "\n",
    "        state:          the current state\n",
    "        action:         the action taken from the current state\n",
    "        reward:         the reward received from the action taken in the state\n",
    "        next_state:     the resulting next state from the action taken in the current state\n",
    "        done:           whether or not the episode terminated\n",
    "        \n",
    "        \"\"\"\n",
    "        # append to replay buffer:\n",
    "        self.experience.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # ε-greedy action selection:\n",
    "    def greedy(self, state):\n",
    "        \"\"\" \n",
    "        this function represents the implicit \"policy\" of the network, which acts ε-greedily:\n",
    "\n",
    "        state:      a batch of states\n",
    "        returns:    the index of the action to take\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape state to be (1, nS) before feeding to net:\n",
    "        state = state.reshape(1, -1)\n",
    "\n",
    "        # predict value of each action based on current state, squeeze from (1, nA) to (nA, ):\n",
    "        q = self.approximator_network.predict(state, verbose = 0).squeeze()\n",
    "\n",
    "        # get probabilities:\n",
    "        m = len(q)  \n",
    "        probs = np.ones(m) * (self.epsilon / m)\n",
    "        probs[np.argmax(q)] += 1.0 - self.epsilon\n",
    "\n",
    "        # randomly select a probability, and return:\n",
    "        return np.random.choice(len(probs), p = probs)\n",
    "    \n",
    "    # ε-decay function:\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\" \n",
    "        this function decays epsilon according to a schedule:\n",
    "    \n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, rewards, next_states, dones):\n",
    "        # compute Q-targets using target network:\n",
    "        q_next = self.target_network(next_states)   # this is using s', a', w-\n",
    "        targets = rewards + (1 - dones) * self.gamma * tf.reduce_max(q_next, axis =1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute the current Q estimates:\n",
    "            q_vals = self.approximator_network(states)\n",
    "\n",
    "            # replace the network's current Q-estimate with the TD-target:\n",
    "            indices = tf.stack([tf.range(tf.shape(actions)[0]), actions], axis = 1)\n",
    "            q_target = tf.tensor_scatter_nd_update(q_vals, indices, targets)\n",
    "            loss = tf.reduce_mean(tf.square(q_target - q_vals))\n",
    "\n",
    "        grads = tape.gradient(loss, self.approximator_network.trainable_variables)\n",
    "        self.approximator_network.optimizer.apply_gradients(zip(grads, self.approximator_network.trainable_variables))\n",
    "\n",
    "    # training function:\n",
    "    def train(self, num_episodes):\n",
    "        \"\"\"\n",
    "        this function performs the actual training loop for the network:\n",
    "\n",
    "        num_episodes:   the desired number of training episodes\n",
    "        \n",
    "        \"\"\"\n",
    "        # pre-compute reward_history:\n",
    "        reward_history = np.zeros(num_episodes)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(num_episodes), colour = \"#33FF00\", ncols = 100):\n",
    "            # reset the environment:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            # flag for completion:\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # select action based on ε-greedy implicit \"policy\":\n",
    "                action = self.greedy(obs)\n",
    "\n",
    "                # take the action:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "\n",
    "                # check for completion:\n",
    "                done = term or trunc\n",
    "\n",
    "                # append transitions to replay buffer:\n",
    "                self.store(obs, action, reward, next_obs, done)\n",
    "\n",
    "                # advance the state:\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # sample a batch and update the network:\n",
    "                if len(self.experience) >= self.batch_size:\n",
    "                    # take a batch:\n",
    "                    batch = random.sample(self.experience, self.batch_size)\n",
    "\n",
    "                    # unpack the batch into arrays:\n",
    "                    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "                    s           =    tf.convert_to_tensor(states,          dtype = tf.float32)\n",
    "                    a           =    tf.convert_to_tensor(actions,         dtype = tf.int32)\n",
    "                    r           =    tf.convert_to_tensor(rewards,         dtype = tf.float32)\n",
    "                    s_next      =    tf.convert_to_tensor(next_states,     dtype = tf.float32)\n",
    "                    d           =    tf.convert_to_tensor(dones,           dtype = tf.float32)\n",
    "\n",
    "                    # call the @tf.function wrapped training step:\n",
    "                    self.train_step(s, a, r, s_next, d)\n",
    "\n",
    "                    # update target network periodically:\n",
    "                    if self.step_counter % self.target_update_freq == 0:\n",
    "                        self.target_network.set_weights(self.approximator_network.get_weights())\n",
    "\n",
    "            # decay epsilon:\n",
    "            self.decay_epsilon()\n",
    "\n",
    "            # append reward to history:\n",
    "            reward_history[episode] = episode_reward\n",
    "            # print(f\"average reward {sum(reward_history[:episode])/(episode+1):.2f}\", end = \"\\r\")\n",
    "        \n",
    "        return reward_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fb751",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section utilizes the above DQN to create an environment and train an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fedfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent hyperparameters:\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "alpha = 1e-3\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "buffer_size = 500\n",
    "target_update_freq = 100\n",
    "layers = 1\n",
    "neurons = 64\n",
    "\n",
    "# create the environment:\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
    "\n",
    "# instantiate the agent:\n",
    "dqn_agent = DQN_Agent(env = env, \n",
    "                  gamma = gamma, \n",
    "                  alpha = alpha,\n",
    "                  epsilon = epsilon,\n",
    "                  epsilon_min = epsilon_min,\n",
    "                  epsilon_decay = epsilon_decay,\n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size,\n",
    "                  target_update_freq = target_update_freq, \n",
    "                  layers = layers,\n",
    "                  neurons = neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97398db",
   "metadata": {},
   "source": [
    "##### **Warm up the Experience Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set minimum amount of samples we want:\n",
    "warm_length = 1000\n",
    "\n",
    "# fill buffer with some random experience to warm it up:\n",
    "obs, _ = dqn_agent.env.reset()\n",
    "for _ in tqdm(range(warm_length), colour = \"#33FF00\", ncols = 100):\n",
    "    # randomly sample the action space, equivalent to ε = 1.0:\n",
    "    action = dqn_agent.env.action_space.sample()\n",
    "\n",
    "    # take the action:\n",
    "    next_obs, reward, term, trunc, _ = dqn_agent.env.step(action)\n",
    "\n",
    "    # check completion:\n",
    "    done = term or trunc\n",
    "\n",
    "    # populate replay buffer:\n",
    "    dqn_agent.store(obs, action, reward, next_obs, done)\n",
    "\n",
    "    # advance state:\n",
    "    obs = next_obs if not done else env.reset()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624eeda",
   "metadata": {},
   "source": [
    "##### **Train the Network**\n",
    "\n",
    "This section trains the network using its training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the training method:\n",
    "reward_history = dqn_agent.train(num_episodes = num_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
