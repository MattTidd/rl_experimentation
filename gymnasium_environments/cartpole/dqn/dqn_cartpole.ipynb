{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fcff23",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a Deep Q-Network (DQN) algorithm for the ``cartpole`` environment offered through Gymnasium. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59a22a",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a780bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ec607",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc50bb",
   "metadata": {},
   "source": [
    "##### Function for Making Keras Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c569977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5353",
   "metadata": {},
   "source": [
    "##### DQN Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9628f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DQN agent class:\n",
    "class DQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, \n",
    "        env: gym.Env, \n",
    "        gamma: float, \n",
    "        alpha: float,\n",
    "        epsilon: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        batch_size: int,\n",
    "        buffer_size: int,\n",
    "        target_update_freq: int, \n",
    "        layers = int,\n",
    "        neurons = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses a DQN to learn an optimal policy, through the use of approximator neural network \n",
    "        to approximate action-value Q, and a target network to generate a Q-target used in the updating of Q(s,a). this is done to prevent updates\n",
    "        to the network weights from changing the target, meaning that we aren't bootstrapping towards a changing target. this helps to stabilize the learning.\n",
    "\n",
    "        env:                    a gymnasium environment\n",
    "        gamma:                  a float value indicating the discount factor\n",
    "        alpha:                  a float value indicating the learning rate\n",
    "        epsilon:                a float value indicating the action-selection probability ε\n",
    "        epsilon_min:            a float value indicating the minimum ε value\n",
    "        epsilon_decay:          a float value indicating the decay rate of ε\n",
    "        batch_size:             an int representing the batch size sampled from the experience\n",
    "        buffer_size:            an int representing the size of the memory buffer\n",
    "        target_update_freq:     an int representing how frequently the target network weights should be updated\n",
    "        layers:                 an int representing the number of layers in each network\n",
    "        neurons:                an int representing the number of neurons in each network\n",
    "\n",
    "        nS:         an int representing the number of states observed, each of which is continuous\n",
    "        nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        approximator_network:       a Keras sequential neural network representing the actual function approximator\n",
    "        target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "        experience:                 an empty deque used to hold the experience history of the agent, limited to buffer_size\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # experience history and mini-batch size:\n",
    "        self.experience = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "\n",
    "        # initialize networks:\n",
    "        self.approximator_network = make_model(layers = layers, neurons = neurons, rate = alpha,\n",
    "                                                norm = False, drop = False,\n",
    "                                                input_shape = self.nS, output_shape = self.nA,\n",
    "                                                loss_function = 'mse')\n",
    "        self.target_network = keras.models.clone_model(self.approximator_network)\n",
    "        self.target_network.set_weights(self.approximator_network.get_weights())\n",
    "\n",
    "        # set target network update frequency:\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # function to store experience in the replay buffer:\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        this function populates the replay buffer from transitions in a given step:\n",
    "\n",
    "        state:          the current state\n",
    "        action:         the action taken from the current state\n",
    "        reward:         the reward received from the action taken in the state\n",
    "        next_state:     the resulting next state from the action taken in the current state\n",
    "        done:           whether or not the episode terminated\n",
    "        \n",
    "        \"\"\"\n",
    "        # append to replay buffer:\n",
    "        self.experience.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # ε-greedy action selection:\n",
    "    def greedy(self, state):\n",
    "        \"\"\" \n",
    "        this function represents the implicit \"policy\" of the network, which acts ε-greedily:\n",
    "\n",
    "        state:      a batch of states\n",
    "        returns:    the index of the action to take\n",
    "\n",
    "        \"\"\"\n",
    "        # reshape state to be (1, nS) before feeding to net:\n",
    "        state = state.reshape(1, -1)\n",
    "\n",
    "        # predict value of each action based on current state, squeeze from (1, nA) to (nA, ):\n",
    "        q = self.approximator_network.predict(state, verbose = 0).squeeze()\n",
    "\n",
    "        # get probabilities:\n",
    "        m = len(q)  \n",
    "        probs = np.ones(m) * (self.epsilon / m)\n",
    "        probs[np.argmax(q)] += 1.0 - self.epsilon\n",
    "\n",
    "        # randomly select a probability, and return:\n",
    "        return np.random.choice(len(probs), p = probs)\n",
    "    \n",
    "    # ε-decay function:\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\" \n",
    "        this function decays epsilon according to a schedule:\n",
    "    \n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    # training function:\n",
    "    def train(self, num_episodes):\n",
    "        \"\"\"\n",
    "        this function performs the actual training loop for the network:\n",
    "\n",
    "        num_episodes:   the desired number of training episodes\n",
    "        \n",
    "        \"\"\"\n",
    "        # pre-compute reward history:\n",
    "        reward_history = np.zeros(num_episodes)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(num_episodes), colour = \"#33FF00\", ncols = 100):\n",
    "            # reset the environment:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            # flag for completion:\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # select action based on ε-greedy implicit \"policy\":\n",
    "                action = self.greedy(obs)\n",
    "\n",
    "                # take the action:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "\n",
    "                # check for completion:\n",
    "                done = term or trunc\n",
    "\n",
    "                # append transitions to replay buffer:\n",
    "                self.store(obs, action, reward, next_obs, done)\n",
    "\n",
    "                # advance the state:\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # sample a batch and update the network:\n",
    "                if len(self.experience) >= self.batch_size:\n",
    "                    # take a batch:\n",
    "                    batch = random.sample(self.experience, self.batch_size)\n",
    "\n",
    "                    # unpack the batch into arrays:\n",
    "                    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "                    # compute Q-targets using the target network:\n",
    "                    q_next = self.target_network.predict_on_batch(next_states)   # this is using s', a', w-\n",
    "                    targets = rewards + (1 - dones) * self.gamma * np.max(q_next, axis = 1)\n",
    "\n",
    "                    # compute the current Q estimates:\n",
    "                    q_vals = self.approximator_network.predict_on_batch(states)  # this is using s, a, w_i\n",
    "\n",
    "                    # replace the network's current Q-estimate with the TD-target:\n",
    "                    q_vals[np.arange(self.batch_size), actions] = targets\n",
    "\n",
    "                    # update the parameters of the approximator network, training towards the TD-target:\n",
    "                    self.approximator_network.train_on_batch(states, q_vals)\n",
    "\n",
    "                    # update target network periodically:\n",
    "                    if self.step_counter % self.target_update_freq == 0:\n",
    "                        self.target_network.set_weights(self.approximator_network.get_weights())\n",
    "\n",
    "            # decay epsilon:\n",
    "            self.decay_epsilon()\n",
    "\n",
    "            # append reward to history:\n",
    "            reward_history[episode] = episode_reward\n",
    "        \n",
    "        return reward_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fb751",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section utilizes the above DQN to create an environment and train an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7fedfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent hyperparameters:\n",
    "num_episodes = 5000\n",
    "gamma = 0.99\n",
    "alpha = 1e-3\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "buffer_size = 5000\n",
    "target_update_freq = 1000\n",
    "layers = 1\n",
    "neurons = 64\n",
    "\n",
    "# create the environment:\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
    "\n",
    "# instantiate the agent:\n",
    "dqn_agent = DQN_Agent(env = env, \n",
    "                  gamma = gamma, \n",
    "                  alpha = alpha,\n",
    "                  epsilon = epsilon,\n",
    "                  epsilon_min = epsilon_min,\n",
    "                  epsilon_decay = epsilon_decay,\n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size,\n",
    "                  target_update_freq = target_update_freq, \n",
    "                  layers = layers,\n",
    "                  neurons = neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97398db",
   "metadata": {},
   "source": [
    "##### **Warm up the Experience Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da1cc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m███████████████████████████████████████████████████████\u001b[0m| 1000/1000 [00:00<00:00, 125012.79it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set minimum amount of samples we want:\n",
    "warm_length = 1000\n",
    "\n",
    "# fill buffer with some random experience to warm it up:\n",
    "obs, _ = dqn_agent.env.reset()\n",
    "for _ in tqdm(range(warm_length), colour = \"#33FF00\", ncols = 100):\n",
    "    # randomly sample the action space, equivalent to ε = 1.0:\n",
    "    action = dqn_agent.env.action_space.sample()\n",
    "\n",
    "    # take the action:\n",
    "    next_obs, reward, term, trunc, _ = dqn_agent.env.step(action)\n",
    "\n",
    "    # check completion:\n",
    "    done = term or trunc\n",
    "\n",
    "    # populate replay buffer:\n",
    "    dqn_agent.store(obs, action, reward, next_obs, done)\n",
    "\n",
    "    # advance state:\n",
    "    obs = next_obs if not done else env.reset()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624eeda",
   "metadata": {},
   "source": [
    "##### **Train the Network**\n",
    "\n",
    "This section trains the network using its training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c62e16fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|\u001b[38;2;51;255;0m▋                                                            \u001b[0m| 59/5000 [00:40<56:10,  1.47it/s]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# call the training method:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m reward_history \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 142\u001b[0m, in \u001b[0;36mDQN_Agent.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# while false:\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# select action based on ε-greedy implicit \"policy\":\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# take the action:\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     next_obs, reward, term, trunc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[30], line 101\u001b[0m, in \u001b[0;36mDQN_Agent.greedy\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     98\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# predict value of each action based on current state, squeeze from (1, nA) to (nA, ):\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximator_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# get probabilities:\u001b[39;00m\n\u001b[0;32m    104\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(q)  \n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:563\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    561\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:740\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:111\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mtidd2\\Desktop\\rl_experimentation\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# call the training method:\n",
    "reward_history = dqn_agent.train(num_episodes = num_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
