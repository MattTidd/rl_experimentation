{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fcff23",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a Deep Q-Network (DQN) algorithm for the ``cartpole`` environment offered through Gymnasium. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59a22a",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a780bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ec607",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc50bb",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c569977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5353",
   "metadata": {},
   "source": [
    "##### DQN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9628f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DQN agent class:\n",
    "class DQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, \n",
    "        env: gym.Env, \n",
    "        gamma: float, \n",
    "        lr: float,\n",
    "        epsilon: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        batch_size: int,\n",
    "        buffer_size: int,\n",
    "        target_update_freq: int, \n",
    "        layers = int,\n",
    "        neurons = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses a DQN to learn an optimal policy, through the use of approximator neural network \n",
    "        to approximate action-value Q, and a target network to generate a Q-target used in the updating of Q(s,a). this is done to prevent updates\n",
    "        to the network weights from changing the target, meaning that we aren't bootstrapping towards a changing target. this helps to stabilize the learning.\n",
    "\n",
    "        env:                    a gymnasium environment\n",
    "        gamma:                  a float value indicating the discount factor γ\n",
    "        lr:                  a float value indicating the learning rate α\n",
    "        epsilon:                a float value indicating the action-selection probability ε\n",
    "        epsilon_min:            a float value indicating the minimum ε value\n",
    "        epsilon_decay:          a float value indicating the decay rate of ε\n",
    "        batch_size:             an int representing the batch size sampled from the experience\n",
    "        buffer_size:            an int representing the size of the memory buffer\n",
    "        target_update_freq:     an int representing how frequently the target network weights should be updated\n",
    "        layers:                 an int representing the number of layers in each network\n",
    "        neurons:                an int representing the number of neurons in each network\n",
    "\n",
    "        nS:         an int representing the number of states observed, each of which is continuous\n",
    "        nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        q_network:                  a Keras sequential neural network representing the actual function approximator\n",
    "        target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "        experience:                 an empty deque used to hold the experience history of the agent, limited to buffer_size\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "\n",
    "        # experience history and mini-batch size:\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # initialize networks:\n",
    "        self.q_network = make_model(layers = layers, neurons = neurons, rate = lr,\n",
    "                                                norm = False, drop = False,\n",
    "                                                input_shape = self.nS, output_shape = self.nA,\n",
    "                                                loss_function = 'mse')\n",
    "        self.target_network = keras.models.clone_model(self.q_network)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # set target network update frequency:\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # define a decorated function to infer Q's from batched states (this is the implicit policy):\n",
    "    @tf.function\n",
    "    def get_qs(self, obs_batch):\n",
    "        return self.q_network(obs_batch)\n",
    "    \n",
    "    # define a decorated function to perform the DQN training step for updating Q network weights:\n",
    "    @tf.function\n",
    "    def training_step(self, states, actions, rewards, next_states, dones):\n",
    "        # track auto differentiation:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1) do a forward pass to get Q values:\n",
    "            # this is all the Q values from every state:\n",
    "            q_all = self.q_network(states)\n",
    "\n",
    "            # find relevant index of actions that will be selected:\n",
    "            index = tf.stack([tf.range(tf.shape(actions)[0]), actions], axis = 1)\n",
    "\n",
    "            # gather up the Q values that correspond to actions actually taken:\n",
    "            q_selected = tf.gather_nd(q_all, index)\n",
    "\n",
    "            # 2) compute TD-targets:\n",
    "            # TD-target is computed with S', A', w-:\n",
    "            q_next = self.target_network(next_states)\n",
    "\n",
    "            # get the Q value corresponding to the max over the actions:\n",
    "            max_q_next = tf.reduce_max(q_next, axis = 1)\n",
    "\n",
    "            # compute actual TD-targets:\n",
    "            targets = tf.stop_gradient(rewards + (1 - dones) * self.gamma * max_q_next)\n",
    "\n",
    "            # 3) MSE loss between the Qs that correspond to taken actions and the TD-target:\n",
    "            loss = tf.reduce_mean(tf.square(q_selected - targets))\n",
    "        \n",
    "        # 4) backpropagate and update the weights:\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length):\n",
    "\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), colour = \"#33FF00\", ncols = 100, desc = \"training progress\"):\n",
    "            # reset environment:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # ε-greedy policy:\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    obs_batch = tf.expand_dims(tf.convert_to_tensor(obs, dtype=tf.float32), 0)\n",
    "                    qs = self.get_qs(obs_batch)\n",
    "                    action = tf.argmax(qs[0]).numpy()\n",
    "\n",
    "                # interact with the environment:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # sample a batch of experience:\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    # get a batch:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "                    # unpack the batch:\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                    # convert to tensors:\n",
    "                    states = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                    actions = tf.convert_to_tensor(actions, dtype = tf.int32)\n",
    "                    rewards = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                    next_states = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                    dones = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                    # single graph call:\n",
    "                    self.training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "                    # update target network periodically:\n",
    "                    if self.step_counter % self.target_update_freq == 0:\n",
    "                        self.target_network.set_weights(self.q_network.get_weights())\n",
    "                \n",
    "            # decay epsilon:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            # advance reward history:\n",
    "            reward_history[episode] = episode_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fb751",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section utilizes the above DQN to create an environment and train an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f1d3a",
   "metadata": {},
   "source": [
    "##### Specify hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fedfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent hyperparameters:\n",
    "lr = 1e-5                   # learning rate α\n",
    "gamma = 0.99                # discount factor γ\n",
    "epsilon = 1.0               # starting value of ε\n",
    "epsilon_min = 0.1           # final value of ε\n",
    "epsilon_decay = 0.999       # decay rate of ε\n",
    "\n",
    "buffer_size = 5000          # size of the replay buffer\n",
    "batch_size = 32             # amount sampled from buffer\n",
    "target_update_freq = 1000   # number of elapsed steps before target network is updated\n",
    "\n",
    "training_length = 5000      # how many episodes to train the network for\n",
    "warmup_length = 1000        # how many steps of experience to populate the replay buffer\n",
    "\n",
    "neurons = 128               # how many neurons to have in each layer of the network\n",
    "layers = 3                  # how many layers to have in each network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ae139",
   "metadata": {},
   "source": [
    "##### Initialize the environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment:\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# clear backend:\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# instantiate the agent:\n",
    "dqn_agent = DQN_Agent(env = env, \n",
    "                  gamma = gamma, \n",
    "                  lr = lr,\n",
    "                  epsilon = epsilon,\n",
    "                  epsilon_min = epsilon_min,\n",
    "                  epsilon_decay = epsilon_decay,\n",
    "                  batch_size = batch_size, \n",
    "                  buffer_size = buffer_size,\n",
    "                  target_update_freq = target_update_freq, \n",
    "                  layers = layers,\n",
    "                  neurons = neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97398db",
   "metadata": {},
   "source": [
    "##### Warm up the Experience Buffer with random examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m████████████████████████████████████████████████████████\u001b[0m| 1000/1000 [00:00<00:00, 74330.19it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# fill buffer with some random experience to warm it up:\n",
    "obs, _ = env.reset()\n",
    "for _ in range(warmup_length):\n",
    "    # sample random action:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # act on environment:\n",
    "    next_obs, reward, term, trunc, _ = env.step(action)\n",
    "\n",
    "    # check for completion:\n",
    "    done = term or trunc\n",
    "\n",
    "    # append experience to agent's replay buffer:\n",
    "    dqn_agent.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "    obs = next_obs if not done else env.reset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624eeda",
   "metadata": {},
   "source": [
    "##### Train the Network:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
