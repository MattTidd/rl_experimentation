{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fcff23",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a Deep Q-Network (DQN) algorithm for the ``cartpole`` environment offered through Gymnasium. Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms, through the use of a standardized API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59a22a",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a780bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ec607",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc50bb",
   "metadata": {},
   "source": [
    "##### Function for Making Keras Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c569977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5353",
   "metadata": {},
   "source": [
    "##### DQN Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9628f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DQN agent class:\n",
    "class DQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "        # constructor:\n",
    "        def __init__(self, \n",
    "            env: gym.Env, \n",
    "            gamma: float, \n",
    "            alpha: float,\n",
    "            epsilon: float):\n",
    "            \"\"\" \n",
    "            this is the constructor for the agent. this agent uses a DQN to learn an optimal policy, through the use of approximator neural network \n",
    "            to approximate action-value Q, and a target network to generate a Q-target used in the updating of Q(s,a). this is done to prevent updates\n",
    "            to the network weights from changing the target, meaning that we aren't bootstrapping towards a changing target. this helps to stabilize the learning.\n",
    "\n",
    "            env:        a gymnasium environment\n",
    "            gamma:      a float value indicating the discount factor\n",
    "            alpha:      a float value indicating the learning rate\n",
    "            epsilon:    a float value indicating the action-selection probability ε\n",
    "\n",
    "            nS:         an int representing the number of states observed, each of which is continuous\n",
    "            nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "            approximator_network:       a Keras sequential neural network representing the actual function approximator\n",
    "            target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "\n",
    "            experience:         an empty deque used to hold the experience history of the agent, limited to 100k buffer\n",
    "            mini_batch:         an int representing the size of the mini-batch to be sampled from the experience\n",
    "\n",
    "            epsilon_final:      a float representing the desired final ε value\n",
    "            epsilon_decay:      a float representing the desired ε decay rate\n",
    "\n",
    "            \"\"\"\n",
    "            # object parameters:\n",
    "            self.env = env\n",
    "            self.gamma = gamma\n",
    "            self.alpha = alpha\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "            # get the environment dimensions:\n",
    "            self.nS = self.env.observation_space.shape[0]\n",
    "            self.nA = self.env.action_space.n\n",
    "\n",
    "            # initialize networks:\n",
    "            self.approximator_network = make_model(layers = 2, neurons = 64, rate = 1e-3,\n",
    "                                                   norm = False, drop = False,\n",
    "                                                   input_shape = self.nS, output_shape = self.nA,\n",
    "                                                   loss_function = 'mse')\n",
    "            self.target_network = keras.models.clone_model(self.approximator_network)\n",
    "            self.target_network.set_weights(self.approximator_network.get_weights())\n",
    "\n",
    "            # experience history and mini-batch size:\n",
    "            self.experience = deque(maxlen = 100000)\n",
    "            self.mini_batch = 64\n",
    "\n",
    "            # exploration schedule:\n",
    "            self.epsilon_final = 0.01\n",
    "            self.epsilon_decay = 0.995\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
