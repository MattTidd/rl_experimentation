{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53c91de",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing the Double Deep Q-Network (DDQN) algorithm for the `cartpole` environment offered through Gymnasium. The Q-learning algorithm itself is prone to overestimation of action-values, and this overestimation propagates into the DQN algorithm. The DQN algorithm is a combination of the Q-learning algorithm and a deep neural network for action-value approximation, and was shown to suffer from substantial overestimations in certain cases, as per Hasselt, Guez & Silver. As such, they proposed the use of the Double Q-learning algorithm in the tabular case, which was then adapted into the DDQN methodology, which reduces observed overestimations and was found to lead to better performance, as stated in [its introductory paper](https://arxiv.org/abs/1509.06461). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
