{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65edd195",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing HPO using `Optuna` on the DDQN implementation developed for `Cartpole`, both in the base and swingup versions. The hyperparameters that will be optimized are:\n",
    "\n",
    "* The learning rate: α\n",
    "* The discount rate: γ\n",
    "* The epsilon decay rate: $ε_{d}$\n",
    "* The final epsilon value: $ε_{f}$\n",
    "* The replay buffer size: $n_{buff}$\n",
    "* The batch size: $n_{batch}$\n",
    "* The target update frequency: $f_{target}$\n",
    "* The training frequency: $f_{train}$\n",
    "* The warmup length: $n_{warm}$\n",
    "* The number of neurons per layer: $n_{neurons}$\n",
    "* The number of layers: $n_{layers}$\n",
    "\n",
    "With the goal being to maximize the average return per episode while training the model over a set number of episodes, for a set number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b48ad1",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "from wrappers.swingup_wrapper import SwingUpWrapper\n",
    "import numpy as np\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2db8ac",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines all relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aae1d",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80650a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = 'linear', name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee56b7d",
   "metadata": {},
   "source": [
    "##### DDQN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994956f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDQN agent class:\n",
    "class DDQN_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self, \n",
    "        env: gym.Env,\n",
    "        gamma: float, \n",
    "        lr: float, \n",
    "        epsilon: float,\n",
    "        epsilon_min: float, \n",
    "        epsilon_decay: float,\n",
    "        batch_size: int,\n",
    "        buffer_size: int, \n",
    "        target_update_freq: int,\n",
    "        train_freq: int,\n",
    "        layers = int,\n",
    "        neurons = int,\n",
    "        seed = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses a DDQN to learn an optimal \"policy\".\n",
    "\n",
    "        env:                    a gymnasium environment\n",
    "        gamma:                  a float value indicating the discount factor γ\n",
    "        lr:                     a float value indicating the learning rate α\n",
    "        epsilon:                a float value indicating the action-selection probability ε\n",
    "        epsilon_min:            a float value indicating the minimum ε value\n",
    "        epsilon_decay:          a float value indicating the decay rate of ε\n",
    "        batch_size:             an int representing the batch size sampled from the experience\n",
    "        buffer_size:            an int representing the size of the memory buffer\n",
    "        target_update_freq:     an int representing how frequently the target network weights should be updated\n",
    "        train_freq:             an int representing how frequently training should occur\n",
    "        layers:                 an int representing the number of layers in each network\n",
    "        neurons:                an int representing the number of neurons in each network\n",
    "        seed:                   an int representing the random seed of the environment, for reproducability\n",
    "\n",
    "        nS:         an int representing the number of states observed, each of which is continuous\n",
    "        nA:         an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        q_network:                  a Keras sequential neural network representing the actual function approximator\n",
    "        target_network:             a Keras sequential neural network representing responsible for generating Q-targets\n",
    "        experience:                 an empty deque used to hold the experience history of the agent, limited to buffer_size\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = self.env.observation_space.shape[0]\n",
    "        self.nA = self.env.action_space.n\n",
    "\n",
    "        # experience history and mini-batch size:\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # initialize networks:\n",
    "        self.q_network = make_model(layers = layers, neurons = neurons, rate = lr,\n",
    "                                    norm = True, drop = True,\n",
    "                                    input_shape = self.nS, output_shape = self.nA,\n",
    "                                    loss_function = \"mse\")\n",
    "        self.target_network = keras.models.clone_model(self.q_network)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # set target update and training frequencies:\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_freq = train_freq\n",
    "\n",
    "        # set the seed:\n",
    "        self.seed = seed\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # define a decorated function to infer Q's from batched states (implicit policy):\n",
    "    @tf.function\n",
    "    def get_qs(self, obs_batch):\n",
    "        return self.q_network(obs_batch)\n",
    "    \n",
    "    # define a decorated function to perform the DDQN training step for updating Q network weights:\n",
    "    @tf.function\n",
    "    def training_step(self, states, actions, rewards, next_states, dones):\n",
    "        # track auto differentiation:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1) do a forward pass to get Q values:\n",
    "            # this is all the Q values from every state:\n",
    "            q_all = self.q_network(states)\n",
    "\n",
    "            # find relevant index of actions that will be selected:\n",
    "            index = tf.stack([tf.range(tf.shape(actions)[0]), actions], axis = 1)\n",
    "\n",
    "            # gather up the Q values that correspond to actions actually taken:\n",
    "            q_selected = tf.gather_nd(q_all, index)\n",
    "            \n",
    "            # 2) compute TD-targets:\n",
    "            # a) select an action using the online network:\n",
    "            q_next_online = self.q_network(next_states)\n",
    "            a_next = tf.argmax(q_next_online, axis = 1, output_type = tf.int32)\n",
    "\n",
    "            # b) evaluate that action using the target network:\n",
    "            q_next_target = self.target_network(next_states)\n",
    "\n",
    "            # get the indices of the actions that are actually taken:\n",
    "            indices = tf.stack([tf.range(tf.shape(a_next)[0]), a_next], axis = 1)\n",
    "            \n",
    "            # pick max q:\n",
    "            max_q_next = tf.gather_nd(q_next_target, indices)\n",
    "\n",
    "            # 3) form the targets:\n",
    "            targets = tf.stop_gradient(rewards + (1 - dones) * self.gamma * max_q_next)\n",
    "\n",
    "            # 4) MSE loss between Qs that correspond to taken actions and the TD-target:\n",
    "            loss = tf.reduce_mean(tf.square(targets - q_selected))\n",
    "\n",
    "        # 5) backpropagate and update the weights:\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "    \n",
    "    # training function:\n",
    "    def training(self, training_length, on_episode_end = None):\n",
    "\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in range(training_length):\n",
    "            # reset environment:\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # ε-greedy policy:\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    obs_batch = tf.expand_dims(tf.convert_to_tensor(obs, dtype = tf.float32), 0)\n",
    "                    qs = self.get_qs(obs_batch)\n",
    "                    action = tf.argmax(qs[0]).numpy()\n",
    "\n",
    "                # interact with the environment:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                done = term or trunc\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "                self.step_counter += 1\n",
    "\n",
    "                # sample a batch of experience:\n",
    "                if len(self.replay_buffer) >= self.batch_size and self.step_counter % self.train_freq == 0:\n",
    "                    # get a batch:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "                    # unpack the batch:\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                    # convert to tensors:\n",
    "                    states = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "                    actions = tf.convert_to_tensor(actions, dtype = tf.int32)\n",
    "                    rewards = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "                    next_states = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
    "                    dones = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "                    # single graph call:\n",
    "                    self.training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "                    # update target network periodically:\n",
    "                    if self.step_counter % self.target_update_freq == 0:\n",
    "                        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "            # decay epsilon:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            # advance reward history:\n",
    "            reward_history[episode] = episode_reward\n",
    "\n",
    "            # callback for pruning:\n",
    "            if on_episode_end is not None:\n",
    "                on_episode_end(episode, episode_reward)\n",
    "        \n",
    "        return reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2006b",
   "metadata": {},
   "source": [
    "##### Objective function for Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e68b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an object function that takes a trial object:\n",
    "def objective(trial):\n",
    "    # 0) initialization:\n",
    "    # clear the backend:\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # define parameters for moving average used in pruning:\n",
    "    window_size = 20\n",
    "    recent_rewards = deque(maxlen = window_size)\n",
    "\n",
    "    # define a callback that can be used every episode:\n",
    "    def prune_callback(episode, episode_reward):\n",
    "        # collect episode rewards into the deque:\n",
    "        recent_rewards.append(episode_reward)\n",
    "\n",
    "        # compute average reward:\n",
    "        avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "\n",
    "        # prune report:\n",
    "        trial.report(avg_reward, step = episode)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "    # 1) create an environment:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    # wrap environment:\n",
    "    # env = SwingUpWrapper(env)\n",
    "\n",
    "    # seeding:\n",
    "    seed = trial.number\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    _, _ = env.reset(seed = seed)\n",
    "\n",
    "    # 2) create an agent:\n",
    "    ddqn_agent = DDQN_Agent(env                   =       env,\n",
    "                          gamma                 =       trial.suggest_categorical('gamma', [0.95, 0.99, 0.995, 0.999]),\n",
    "                          lr                    =       trial.suggest_categorical('lr', [1e-5, 1e-4, 1e-3]),\n",
    "                          epsilon               =       1.0,\n",
    "                          epsilon_min           =       trial.suggest_categorical('epsilon_min', [0.01, 0.05, 0.1]),\n",
    "                          epsilon_decay         =       trial.suggest_categorical('epsilon_decay', [0.99, 0.995, 0.999]),\n",
    "                          batch_size            =       trial.suggest_categorical('batch_size', [32, 64, 128, 256]),\n",
    "                          buffer_size           =       trial.suggest_int('buffer_size', low = 1000, high = 10000, step = 1000),\n",
    "                          target_update_freq    =       trial.suggest_categorical('target_update_freq', [10, 100, 1000]),\n",
    "                          train_freq            =       trial.suggest_categorical('train_freq', [1, 2, 4]), \n",
    "                          layers                =       trial.suggest_categorical('layers', [2, 3]), \n",
    "                          neurons               =       trial.suggest_categorical('neurons', [32, 64, 128])\n",
    "                          )\n",
    "    \n",
    "    # 3) populate the replay buffer:\n",
    "    obs, _ = ddqn_agent.env.reset()\n",
    "    for _ in range(trial.suggest_int('warmup_length', low = 1000, high = 10000, step = 1000)):\n",
    "        # sample random action:\n",
    "        action = ddqn_agent.env.action_space.sample()\n",
    "\n",
    "        # act on environment:\n",
    "        next_obs, reward, term, trunc, _ = ddqn_agent.env.step(action)\n",
    "\n",
    "        # check for completion:\n",
    "        done = term or trunc\n",
    "\n",
    "        # append experience to agent's replay buffer:\n",
    "        ddqn_agent.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "        obs = next_obs if not done else ddqn_agent.env.reset()[0]\n",
    "\n",
    "    # 4) train the agent:\n",
    "    reward_history = ddqn_agent.training(training_length = 1000,\n",
    "                                         on_episode_end = prune_callback)\n",
    "    \n",
    "    # 5) close the environment:\n",
    "    ddqn_agent.env.close()\n",
    "\n",
    "    # 6) save the seed:\n",
    "    trial.set_user_attr(\"seed\", seed)\n",
    "\n",
    "    # 7) return metric:\n",
    "    return float(np.mean(reward_history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588c3a7",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section performs the hyperparameter searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49039e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get run time:\n",
    "time = datetime.datetime.now()\n",
    "formatted_time = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "# set study name:\n",
    "study_name = f\"DDQN_HPO_{formatted_time}\"\n",
    "\n",
    "# create a study object and optimize the objective function:\n",
    "study = optuna.create_study(study_name = study_name, \n",
    "                            direction = 'maximize',\n",
    "                            storage = \"sqlite:///optuna_results.db\",\n",
    "                            load_if_exists = True, \n",
    "                            pruner = optuna.pruners.MedianPruner(n_startup_trials = 10,\n",
    "                                                                n_warmup_steps = 250,\n",
    "                                                                interval_steps = 10))\n",
    "\n",
    "# optimize:\n",
    "study.optimize(objective, n_trials = 250, show_progress_bar = True, n_jobs = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
