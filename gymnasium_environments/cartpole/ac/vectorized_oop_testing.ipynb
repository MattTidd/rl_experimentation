{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e395f8e",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing an Actor-Critic method based on the advantage function (A2C), for learning an optimal policy for the `Cartpole` environment. The Actor-Critic method utilizes two networks, one of which is responsible for mapping states to a probability distribution over the actions (actor), and another which estimates the value of a state to guide the actor (critic). The general idea is that the actor updates its policy in the direction suggested by the critic.\n",
    "\n",
    "Also, as per [this paper](https://proceedings.mlr.press/v97/ahmed19a/ahmed19a.pdf), I am also implementing entropy regularization in an attempt to reduce variance and improve performance. Similarly, I am vectorizing the environment, employing the use of `n_envs` concurrent environments, such that the implemented algorithm is more akin to [its original introduction](https://arxiv.org/pdf/1602.01783)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adedf88",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e7ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these:\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import AsyncVectorEnv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d48b7",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce1507",
   "metadata": {},
   "source": [
    "##### Function for making vectorized environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0645dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes seed and env_name:\n",
    "def make_env(env_name : str, seed):\n",
    "    # does not work without this, for some reason:\n",
    "    def intermediary():\n",
    "        # make the environment based on the provided env_name:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        # wrap environment:\n",
    "        env = SwingUpWrapper(env)\n",
    "\n",
    "        # reset and seed the environment:\n",
    "        env.reset(seed = seed)\n",
    "\n",
    "        # return env to user:\n",
    "        return env\n",
    "    return intermediary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60209aea",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0819b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model based on user inputs:\n",
    "def make_model(layers, neurons, rate, input_shape, output_shape, loss_function, output_activation):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "        else: \n",
    "            model.add(Dense(neurons, activation = 'relu', name = f\"hidden_layer_{i+1}\"))\n",
    "\n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    # return to user:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111620d",
   "metadata": {},
   "source": [
    "##### Vectorized A2C class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52993e7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3331239329.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    envs: gym.Env)\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "# A2C class:\n",
    "class A2C_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                envs: gym.AsyncVectorEnv,\n",
    "                gamma: float, \n",
    "                lr_a: float,\n",
    "                lr_c: float, \n",
    "                beta: float,\n",
    "                layers = int, \n",
    "                neurons = int,\n",
    "                n_envs = int,\n",
    "                n_steps: int = 5):\n",
    "        \"\"\"\n",
    "        this is the constructor for the agent. this agent uses the advantage actor-critic (A2C) algorithm to learn an optimal policy,\n",
    "        through the use of two approximator networks. the first network, called the actor, is responsible for providing the probabilty \n",
    "        distribution over all actions given a state. the second network, called the critic, is responsible for utilizing the advantage function\n",
    "        to guide the learning of the actor.\n",
    "\n",
    "        this implementation uses entropy regularization to encourage exploration, and utilizes a vectorized environment to increase the stabilizing\n",
    "        effect on the training procedure.\n",
    "\n",
    "        envs:               asynchronously vectorized gymnasium environments\n",
    "        gamma:              a float value indicating the discount factor, γ\n",
    "        lr_a:               a float value indicating the learning rate of the actor, α_a\n",
    "        lr_c:               a float value indicating the learning rate of the critic, α_c\n",
    "        beta:               a float value indicating the entropy regularization parameter, β\n",
    "        layers:             an int value indicating the number of layers in a network\n",
    "        neurons:            an int value indicating the number of neurons per layer\n",
    "        n_steps:            an int value indicating the number of steps to use when computing the return\n",
    "        n_envs:             an int value indicating the number of parallel environments used\n",
    "\n",
    "        nS:                 an int representing the number of states observed, each of which is continuous\n",
    "        nA:                 an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        actor:              a Keras sequential neural network representing the actor\n",
    "        critic:             a Keras sequential neural network representing the actor\n",
    "\n",
    "        buf_states:         a list used to hold the states used in the n-step return\n",
    "        buf_actions:        a list used to hold the actions used in the n-step return\n",
    "        buf_rewards:        a list used to hold the rewards used in the n-step return\n",
    "        buf_next_states:    a list used to hold the next states used in the n-step return\n",
    "        buf_next_dones:     a list used to hold the dones used in the n-step return\n",
    "        \n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.envs       = envs\n",
    "        self.gamma      = gamma\n",
    "        self.lr_a       = lr_a\n",
    "        self.lr_c       = lr_c\n",
    "        self.beta       = beta\n",
    "        self.n_steps    = n_steps\n",
    "        self.n_envs     = n_envs\n",
    "\n",
    "        # get environment dimensions:\n",
    "        self.nS = envs.single_observation_space.shape[0]\n",
    "        self.nA = envs.single_action_space.n\n",
    "\n",
    "        # initialize the networks:\n",
    "        self.actor = make_model(layers = layers,\n",
    "                                neurons = neurons,\n",
    "                                rate = lr_a,\n",
    "                                input_shape = self.nS,\n",
    "                                output_shape = self.nA,\n",
    "                                output_activation = \"softmax\",\n",
    "                                loss_function = \"categorical_crossentropy\")\n",
    "\n",
    "        self.critic = make_model(layers = layers,\n",
    "                                neurons = neurons,\n",
    "                                rate = lr_c,\n",
    "                                input_shape = self.nS,\n",
    "                                output_shape = self.nA,\n",
    "                                output_activation = \"linear\",\n",
    "                                loss_function = \"mse\")\n",
    "\n",
    "        # initialize buffers for rollout:\n",
    "        self.obs_buf = []\n",
    "        self.act_buf = []\n",
    "        self.rew_buf = []\n",
    "        self.val_buf = []\n",
    "        self.done_buf = []\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # function for calculating discounted returns:\n",
    "    def discounted_returns(rewards, dones, last_value):\n",
    "        # compute the discounted cumulative reward for a vectorized environment:\n",
    "        returns = np.zeros((self.n_steps, self.n_envs))\n",
    "        running_return = last_value\n",
    "\n",
    "        # start computing discounted return:\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            running_return = rewards[t] + gamma * running_return * (1 - dones[t])\n",
    "            returns[t] = running_return\n",
    "    \n",
    "        # return to user:\n",
    "        return returns\n",
    "\n",
    "    # decorated training step function:\n",
    "    @tf.function\n",
    "    def training_step(states, actions, returns, advantages):\n",
    "        # convert values to tensors, if not already:\n",
    "        states = tf.convert_to_tensor(states, dtype = tf.float32)\n",
    "        actions = tf.cast(actions, dtype = tf.int32)\n",
    "        returns = tf.cast(returns, dtype = tf.float32)\n",
    "        advantages = tf.cast(advantages, dtype = tf.float32)\n",
    "\n",
    "        # CRITIC UPDATE:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
