{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41baf300",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing HPO using `Optuna` on the A2C implementation developed for `Cartpole`, both in the base and swingup versions. The hyperparameters that will be optimized are:\n",
    "\n",
    "* The learning rate of the actor: $α_a$\n",
    "* The learning rate of the critic: $α_c$\n",
    "* The discount rate: γ\n",
    "* The entropy scaling parameter: β\n",
    "* The number of neurons per layer: $n_{neurons}$\n",
    "* The number of layers: $n_{layers}$\n",
    "* The number of steps to use in the return: $n_{steps}$\n",
    "\n",
    "With the goal being to maximize the average return per episode while training the model over a set number of episodes, for a set number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2dad53",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70d5a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "from wrappers.swingup_wrapper import SwingUpWrapper\n",
    "import numpy as np\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c38642",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines all relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad6e10",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ad01176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function, output_activation):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643384c3",
   "metadata": {},
   "source": [
    "##### A2C class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "042a5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C class:\n",
    "class A2C_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                gamma: float, \n",
    "                lr_a: float, \n",
    "                lr_c: float,\n",
    "                beta: float,\n",
    "                layers = int,\n",
    "                neurons = int,\n",
    "                n_steps: int = 5):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the advantage actor-critic (A2C) algorithm to learn an optimal policy,\n",
    "        through the use of two approximator networks. the first network, called the actor, is responsible for providing the probabilty \n",
    "        distribution over all actions given a state. the second network, called the critic, is responsible for utilizing the advantage function\n",
    "        to guide the learning of the actor.\n",
    "\n",
    "        env:                a gymnasium environment\n",
    "        gamma:              a float value indicating the discount factor, γ\n",
    "        lr_a:               a float value indicating the learning rate of the actor, α_a\n",
    "        lr_c:               a float value indicating the learning rate of the critic, α_c\n",
    "        beta:               a float value indicating the entropy regularization parameter, β\n",
    "        layers:             an int value indicating the number of layers in a network\n",
    "        neurons:            an int value indicating the number of neurons per layer\n",
    "        n_steps:            an int value indicating the number of steps to use when computing the return\n",
    "\n",
    "        nS:                 an int representing the number of states observed, each of which is continuous\n",
    "        nA:                 an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        actor_network:      a Keras sequential neural network representing the actor\n",
    "        critic_network:     a Keras sequential neural network representing the actor\n",
    "\n",
    "        buf_states:         a list used to hold the states used in the n-step return\n",
    "        buf_actions:        a list used to hold the actions used in the n-step return\n",
    "        buf_rewards:        a list used to hold the rewards used in the n-step return\n",
    "        buf_next_states:    a list used to hold the next states used in the n-step return\n",
    "        buf_next_dones:     a list used to hold the dones used in the n-step return\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env        = env\n",
    "        self.gamma      = gamma\n",
    "        self.lr_a       = lr_a\n",
    "        self.lr_c       = lr_c\n",
    "        self.beta       = beta\n",
    "        self.n_steps    = n_steps\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.n\n",
    "\n",
    "        # initialize the networks:\n",
    "        self.actor_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_a,\n",
    "                                        norm = False,\n",
    "                                        drop = False,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = self.nA,\n",
    "                                        loss_function = \"categorical_crossentropy\",\n",
    "                                        output_activation = \"softmax\")\n",
    "        \n",
    "        self.critic_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_c,\n",
    "                                        norm = False,\n",
    "                                        drop = False,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = 1,\n",
    "                                        loss_function = \"mse\",\n",
    "                                        output_activation = \"linear\")\n",
    "\n",
    "        # create n-step buffers:\n",
    "        self.buf_states         = []\n",
    "        self.buf_actions        = []\n",
    "        self.buf_rewards        = []\n",
    "        self.buf_next_states    = []\n",
    "        self.buf_dones          = []\n",
    "    \n",
    "    # function used to compute n-step returns, bootstrapping from the last value:\n",
    "    def discount_rewards_from(self, last_value, rewards):\n",
    "        r = np.zeros_like(rewards, dtype = np.float32)\n",
    "        running = float(last_value)\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running = rewards[t] + self.gamma * running\n",
    "            r[t] = running\n",
    "\n",
    "        return r\n",
    "\n",
    "    ####################### TRAINING #######################\n",
    "    # decorated training step function:\n",
    "    @tf.function\n",
    "    def training_step(self, states, actions, returns):\n",
    "        # convert values to tensors:\n",
    "        states = tf.convert_to_tensor(states, dtype = tf.float32)       # shape is (B, nS)\n",
    "        actions = tf.convert_to_tensor(actions, dtype = tf.int32)       # shape is (B, )\n",
    "        returns = tf.convert_to_tensor(returns, dtype = tf.float32)     # shape is (B, )\n",
    "\n",
    "        # CRITIC UPDATE:\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # pass state through critic:\n",
    "            values = tf.squeeze(self.critic_network(states, training = True), axis = 1)     # shape is (B, )\n",
    "\n",
    "            # compute critic loss:\n",
    "            critic_loss = tf.reduce_mean(tf.square(returns - values))\n",
    "\n",
    "        # backpropagate and update critic:\n",
    "        critic_grads = critic_tape.gradient(critic_loss, self.critic_network.trainable_variables)\n",
    "        self.critic_network.optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\n",
    "\n",
    "        # ACTOR UPDATE:\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            # pass state through actor to get probs:\n",
    "            probs = self.actor_network(states, training = True)             # shape is (B, nA)\n",
    "\n",
    "            # take log of policy for scoring:   \n",
    "            log_probs = tf.math.log(tf.clip_by_value(probs, 1e-8, 1.0))     # shape is (B, nA)\n",
    "\n",
    "            # get a mask of the actions chosen:\n",
    "            action_masks = tf.one_hot(actions, probs.shape[1], dtype = tf.float32)  # shape is (B, nA)\n",
    "\n",
    "            # get log probs that were actually chosen:\n",
    "            log_probs_chosen = tf.reduce_sum(action_masks * log_probs, axis = 1)    # shape is (B, )\n",
    "\n",
    "            # entropy term:\n",
    "            entropy_per_sample = -tf.reduce_sum(probs * log_probs, axis = 1)        # shape is (B, )\n",
    "\n",
    "            # compute advantage:\n",
    "            values_for_adv = tf.stop_gradient(tf.squeeze(self.critic_network(states, training = False), axis = 1))\n",
    "            advantage = returns - values_for_adv        # shape is (B, )\n",
    "\n",
    "            # normalize the advantage for stability:\n",
    "            adv_mean, adv_var = tf.nn.moments(advantage, axes = [0])\n",
    "            advantage_norm = (advantage - adv_mean) / (tf.sqrt(adv_var) + 1e-8)\n",
    "            \n",
    "            # compute actor loss, negative to maximize (ascent):\n",
    "            actor_loss = -tf.reduce_mean(log_probs_chosen * advantage_norm) - self.beta * tf.reduce_mean(entropy_per_sample)\n",
    "\n",
    "        # backpropagate and update actor:\n",
    "        actor_grads = actor_tape.gradient(actor_loss, self.actor_network.trainable_variables)\n",
    "        self.actor_network.optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
    "\n",
    "    # gymnasium training function:\n",
    "    def train(self, training_length, train_metrics, on_episode_end = None):\n",
    "        # 1) initialize reward history:\n",
    "        reward_history = []\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in range(training_length):\n",
    "            # 2) initialize per episode:\n",
    "            obs, _ = self.env.reset()   # get initial state\n",
    "            episode_reward = 0          # counter for reward earned this episode\n",
    "            done = False                # flag for completion\n",
    "\n",
    "            # clear buffers:\n",
    "            self.buf_states.clear()\n",
    "            self.buf_actions.clear()\n",
    "            self.buf_rewards.clear()\n",
    "            self.buf_next_states.clear()\n",
    "            self.buf_dones.clear()\n",
    "\n",
    "            # while not false:\n",
    "            while not done:\n",
    "                # 3) pick an action from the actor network output:\n",
    "                obs_tensor = tf.convert_to_tensor(obs[None, :], dtype = tf.float32)\n",
    "                probs = self.actor_network(obs_tensor, training = False)\n",
    "                action = np.random.choice(len(probs[0]), p = probs.numpy()[0])\n",
    "\n",
    "                # 4) step the environment:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                done = bool(term or trunc)\n",
    "\n",
    "                # 5) pass this transition to the buffers:\n",
    "                self.buf_states.append(obs.copy())\n",
    "                self.buf_actions.append(action)\n",
    "                self.buf_rewards.append(reward)\n",
    "                self.buf_next_states.append(next_obs.copy())\n",
    "                self.buf_dones.append(done)\n",
    "\n",
    "                # 6) training step if buffer is full:\n",
    "                if len(self.buf_rewards) >= self.n_steps or done:\n",
    "                    # if the last value is not terminal, bootstrap it:\n",
    "                    if done: \n",
    "                        last_value = 0\n",
    "                    else:\n",
    "                        ns_tensor = tf.convert_to_tensor(self.buf_next_states[-1][None, :], dtype = tf.float32)\n",
    "                        last_value = float(tf.squeeze(self.critic_network(ns_tensor, training = False)).numpy())\n",
    "                    \n",
    "                    # compute n-step returns with this last value:\n",
    "                    returns = self.discount_rewards_from(last_value, self.buf_rewards)\n",
    "\n",
    "                    # form batch arrays for states and actions corresponding to returns:\n",
    "                    states_batch = np.vstack(self.buf_states).astype(np.float32)\n",
    "                    actions_batch = np.array(self.buf_actions, dtype = np.int32)\n",
    "                    returns_batch = returns.astype(np.float32)\n",
    "\n",
    "                    # call the n-step training function:\n",
    "                    self.training_step(states_batch, actions_batch, returns_batch)\n",
    "\n",
    "                    # clear the buffers:\n",
    "                    self.buf_states.clear()\n",
    "                    self.buf_actions.clear()\n",
    "                    self.buf_rewards.clear()\n",
    "                    self.buf_next_states.clear()\n",
    "                    self.buf_dones.clear()\n",
    "\n",
    "                # 7) advance state:\n",
    "                obs = next_obs\n",
    "            \n",
    "            # 8) advance reward history:\n",
    "            reward_history.append(episode_reward)\n",
    "\n",
    "            # 9) early stopping:\n",
    "            if (episode + 1) >= train_metrics[\"min_train\"]:\n",
    "    \n",
    "                recent_average = np.mean(reward_history[-train_metrics[\"over_last\"]:]).round(3)\n",
    "\n",
    "                if recent_average >= train_metrics[\"desired_score\"]:\n",
    "                    print(f\"environment solved in {episode+ 1} episodes! average reward was: {recent_average}\")\n",
    "                    break\n",
    "\n",
    "            # callback for pruning:\n",
    "            if on_episode_end is not None:\n",
    "                on_episode_end(episode, episode_reward)\n",
    "            \n",
    "        # 10) return to user:\n",
    "        return reward_history, recent_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80d1ac",
   "metadata": {},
   "source": [
    "##### Objective function for Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c1dadd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define desired training performance for early stopping the training:\n",
    "train_metrics = {\"desired_score\"    : 475,\n",
    "                 \"over_last\"        : 100,\n",
    "                 \"min_train\"        : 250}\n",
    "\n",
    "# define an objective function that takes a trial object:\n",
    "def objective(trial):\n",
    "    # 0) initialization:\n",
    "    # clear the backend:\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # define parameters for moving average used in pruning:\n",
    "    window_size = 20\n",
    "    recent_rewards = deque(maxlen = window_size)\n",
    "\n",
    "    # define a callback that can be used every episode:\n",
    "    def prune_callback(episode, episode_reward):\n",
    "        # collect episode rewards into the deque:\n",
    "        recent_rewards.append(episode_reward)\n",
    "\n",
    "        # compute average reward:\n",
    "        avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "\n",
    "        # prune report:\n",
    "        trial.report(avg_reward, step = episode)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # 1) create an environment:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    # wrap environment:\n",
    "    # env = SwingUpWrapper(env)\n",
    "\n",
    "    # seeding:\n",
    "    seed = 18\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    _, _ = env.reset(seed = seed)\n",
    "\n",
    "    # 2) create an agent:\n",
    "    agent = A2C_Agent(env = env,\n",
    "                      lr_a = trial.suggest_categorical('lr_a', [1e-4, 2.5e-4, 5e-5, 1e-5, 2.5e-5, 5e-5]),\n",
    "                      lr_c = trial.suggest_categorical('lr_c', [1e-2, 2.5e-2, 5e-2, 1e-3, 2.5e-3, 5e-3]),\n",
    "                      gamma = trial.suggest_categorical('gamma', [0.95, 0.99, 0.995]),\n",
    "                      beta = trial.suggest_categorical('beta', [0.1, 1e-2, 1e-3, 1e-4]),\n",
    "                      layers = trial.suggest_categorical('layers', [2, 3]),\n",
    "                      neurons = trial.suggest_categorical('neurons', [16, 24, 32, 64]),\n",
    "                      n_steps = trial.suggest_int('n_steps', 2, 10)\n",
    "                      )\n",
    "    \n",
    "    # 3) train the agent:\n",
    "    reward_history, recent_average = agent.train(training_length = 1000, \n",
    "                                                 train_metrics = train_metrics, \n",
    "                                                 on_episode_end = prune_callback)\n",
    "\n",
    "    # 4) close the environment:\n",
    "    agent.env.close()\n",
    "\n",
    "    # 6) save the recent average:\n",
    "    trial.set_user_attr(f\"last_{train_metrics['over_last']}\", recent_average)\n",
    "\n",
    "    # 7) return metric:\n",
    "    return float(np.mean(reward_history).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8342e",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section performs the hyperparameter searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e180aaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 85476 calls to <function A2C_Agent.training_step at 0x000001C22EF380E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function A2C_Agent.training_step at 0x000001C2890D5440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "environment solved in 928 episodes! average reward was: 476.07\n"
     ]
    }
   ],
   "source": [
    "# get run time:\n",
    "time = datetime.datetime.now()\n",
    "formatted_time = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "# set study name:\n",
    "study_name = f\"A2C_HPO_{formatted_time}\"\n",
    "\n",
    "# create a study object and optimize the objective function:\n",
    "study = optuna.create_study(study_name = study_name,\n",
    "                            direction = \"maximize\",\n",
    "                            storage = \"sqlite:///optuna_results.db\",\n",
    "                            load_if_exists = True,\n",
    "                            pruner = optuna.pruners.MedianPruner(n_startup_trials = 5,\n",
    "                                                                n_warmup_steps = 0,\n",
    "                                                                interval_steps = 1))\n",
    "\n",
    "# optimize:\n",
    "study.optimize(objective, n_trials = 250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
