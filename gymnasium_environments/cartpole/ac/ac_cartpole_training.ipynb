{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dd5a3d",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a vanilla Actor-Critic method based on the advantage function (A2C), for learning an optimal policy for the `Cartpole` environment. The Actor-Critic method utilizes two networks, one of which is responsible for mapping states to a probability distribution over the actions (actor), and another which estimates the value of a state to guide the actor (critic). The general idea is that the actor updates its policy in the direction suggested by the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec306",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b6561ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61802222",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d0b87",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "16af0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function, output_activation):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f5c33",
   "metadata": {},
   "source": [
    "##### A2C class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf864ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C class:\n",
    "class A2C_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                gamma: float, \n",
    "                lr_a: float, \n",
    "                lr_c: float,\n",
    "                layers = int,\n",
    "                neurons = int,\n",
    "                seed = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the advantage actor-critic (A2C) algorithm to learn an optimal policy,\n",
    "        through the use of two approximator networks. the first network, called the actor, is responsible for providing the probabilty \n",
    "        distribution over all actions given a state. the second network, called the critic, is responsible for utilizing the advantage function\n",
    "        to guide the learning of the actor.\n",
    "\n",
    "        env:                a gymnasium environment\n",
    "        gamma:              a float value indicating the discount factor, γ\n",
    "        lr_a:               a float value indicating the learning rate of the actor, α_a\n",
    "        lr_c:               a float value indicating the learning rate of the critic, α_c\n",
    "        layers:             an int value indicating the number of layers in a network\n",
    "        neurons:            an int value indicating the number of neurons per layer\n",
    "        seed:               an int value indicating the desired seed, for use in randomization and reproducability\n",
    "\n",
    "        nS:                 an int representing the number of states observed, each of which is continuous\n",
    "        nA:                 an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        actor_network:      a Keras sequential neural network representing the actor\n",
    "        critic_network:     a Keras sequential neural network representing the actor\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.n\n",
    "\n",
    "        # initialize the networks:\n",
    "        self.actor_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_a,\n",
    "                                        norm = True,\n",
    "                                        drop = True,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = self.nA,\n",
    "                                        loss_function = \"categorical_crossentropy\",\n",
    "                                        output_activation = \"softmax\")\n",
    "        \n",
    "        self.critic_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_c,\n",
    "                                        norm = True,\n",
    "                                        drop = True,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = 1,\n",
    "                                        loss_function = \"mse\",\n",
    "                                        output_activation = \"linear\")\n",
    "        \n",
    "        # set the seed:\n",
    "        self.seed = seed\n",
    "    \n",
    "    ####################### TRAINING #######################\n",
    "    # decorated training step function:\n",
    "    @tf.function\n",
    "    def training_step(self, states, actions, rewards, next_states, dones):\n",
    "        # convert values to tensors:\n",
    "        states = tf.convert_to_tensor(states[None, :], dtype = tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype = tf.int32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states[None, :], dtype = tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype = tf.float32)\n",
    "\n",
    "        # critic update:\n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # pass state through critic:\n",
    "            value = self.critic_network(states, training = True)\n",
    "\n",
    "            # get next value:\n",
    "            next_value = self.critic_network(next_states, training = False)\n",
    "\n",
    "            # compute target:\n",
    "            target = rewards + self.gamma * next_value * (1 - dones)\n",
    "\n",
    "            # compute critic loss:\n",
    "            critic_loss = tf.reduce_mean(tf.square(target - value))\n",
    "\n",
    "        # backpropagate and update critic:\n",
    "        critic_grads = critic_tape.gradient(critic_loss, self.critic_network.trainable_variables)\n",
    "        self.critic_network.optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\n",
    "\n",
    "        # actor update:\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            # pass state through actor to get probs:\n",
    "            probs = self.actor_network(states, training = True)\n",
    "\n",
    "            # get a mask of the actions chosen:\n",
    "            action_masks = tf.one_hot(actions, probs.shape[1])\n",
    "\n",
    "            # take the log of the probs:\n",
    "            log_probs = tf.reduce_sum(action_masks * tf.math.log(probs + 1e-8), axis = 1)\n",
    "\n",
    "            # compute advantage:\n",
    "            advantage = tf.stop_gradient(target - value)\n",
    "            \n",
    "            # compute actor loss:\n",
    "            actor_loss = -tf.reduce_mean(log_probs * advantage)\n",
    "\n",
    "        # backpropagate and update actor:\n",
    "        actor_grads = actor_tape.gradient(actor_loss, self.actor_network.trainable_variables)\n",
    "        self.actor_network.optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
    "\n",
    "        # return losses:\n",
    "        # return actor_loss, critic_loss\n",
    "\n",
    "    # gymnasium training function:\n",
    "    def training(self, training_length):\n",
    "        # 1) initialize reward history:\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), colour = \"#33FF00\", ncols = 100, desc = \"training progress\"):\n",
    "            # 2) initialize per episode:\n",
    "            obs, _ = self.env.reset()   # get initial state\n",
    "            episode_reward = 0          # counter for reward earned this episode\n",
    "            done = False                # flag for completion\n",
    "\n",
    "            # while not false:\n",
    "            while not done:\n",
    "                # 3) pick an action from the actor network output:\n",
    "                obs_tensor = tf.convert_to_tensor(obs[None, :], dtype = tf.float32)\n",
    "                probs = self.actor_network(obs_tensor, training = False)\n",
    "                action = np.random.choice(len(probs[0]), p = probs.numpy()[0])\n",
    "\n",
    "                # 4) step the environment:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                done = term or trunc\n",
    "\n",
    "                # 5) training step:\n",
    "                self.training_step(obs, action, reward, next_obs, done)\n",
    "\n",
    "                # 6) advance state:\n",
    "                obs = next_obs\n",
    "            \n",
    "            # 7) advance reward history:\n",
    "            reward_history[episode] = episode_reward\n",
    "        \n",
    "        # 8) return to user:\n",
    "        return reward_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115cd0c",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section utilizes the above A2C implementation to create an environment and train an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa254a0",
   "metadata": {},
   "source": [
    "##### Specify hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent hyperparameters:\n",
    "lr_a = 1e-4             # learning rate for the actor, α_a\n",
    "lr_c = 1e-4             # learning rate for the actor, α_c\n",
    "gamma = 0.99            # discount factor γ\n",
    "train_length = 100      # number of episodes to train for\n",
    "\n",
    "layers = 3      # how many layers to have in each network\n",
    "neurons = 64    # how many neurons to have in each layer of each network\n",
    "\n",
    "seed = 18       # seed for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbec82",
   "metadata": {},
   "source": [
    "##### Initialize the environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "19aacd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment:\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# clear the backend:\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# instantiate the agent:\n",
    "ac_agent = A2C_Agent(env = env,\n",
    "                    lr_a = lr_a,\n",
    "                    lr_c = lr_c,\n",
    "                    gamma = gamma,\n",
    "                    layers = layers,\n",
    "                    neurons = neurons, \n",
    "                    seed = seed\n",
    "                    )\n",
    "\n",
    "_, _ = ac_agent.env.reset(seed = ac_agent.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ceca6f",
   "metadata": {},
   "source": [
    "##### Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ed266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is: [-0.0024639  -0.0377598  -0.01864039  0.02362086] with shape: (4,)\n",
      "tensor observation is: [[-0.0024639  -0.0377598  -0.01864039  0.02362086]] with shape: (1, 4)\n",
      "\n",
      "probabilities: [[0.49635154 0.50364846]]\n",
      "selected action is: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # WORKING THROUGH TRAINING:\n",
    "# obs, _ = ac_agent.env.reset()\n",
    "# print(f\"observation is: {obs} with shape: {obs.shape}\")\n",
    "\n",
    "# obs_tensor = tf.convert_to_tensor(obs[None, :], dtype = tf.float32)\n",
    "# print(f\"tensor observation is: {obs_tensor} with shape: {obs_tensor.shape}\\n\")\n",
    "\n",
    "# # ACTION CHOOSING:\n",
    "# probs = ac_agent.actor_network(obs_tensor, training = False)\n",
    "# print(f\"probabilities: {probs}\")\n",
    "# action = np.random.choice(ac_agent.nA, p = probs.numpy()[0])\n",
    "# print(f\"selected action is: {action}\\n\")\n",
    "\n",
    "# # STEP ENVIRONMENT:\n",
    "# next_obs, reward, term, trunc, _ = ac_agent.env.step(action)\n",
    "# done = term or trunc\n",
    "\n",
    "# TRAINING STEP:\n",
    "# ac_agent.training_step(obs, action, reward, next_obs, done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
