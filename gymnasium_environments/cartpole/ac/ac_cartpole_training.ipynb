{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dd5a3d",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a vanilla Actor-Critic method based on the advantage function (A2C), for learning an optimal policy for the `Cartpole` environment. The Actor-Critic method utilizes two networks, one of which is responsible for mapping states to a probability distribution over the actions (actor), and another which estimates the value of a state to guide the actor (critic). The general idea is that the actor updates its policy in the direction suggested by the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec306",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6561ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61802222",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d0b87",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function, output_activation):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f5c33",
   "metadata": {},
   "source": [
    "##### A2C class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf864ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C class:\n",
    "class A2C_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                gamma: float, \n",
    "                lr_a: float, \n",
    "                lr_c: float,\n",
    "                layers = int,\n",
    "                neurons = int,\n",
    "                seed = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the advantage actor-critic (A2C) algorithm to learn an optimal policy,\n",
    "        through the use of two approximator networks. the first network, called the actor, is responsible for providing the probabilty \n",
    "        distribution over all actions given a state. the second network, called the critic, is responsible for utilizing the advantage function\n",
    "        to guide the learning of the actor.\n",
    "\n",
    "        env:                a gymnasium environment\n",
    "        gamma:              a float value indicating the discount factor, γ\n",
    "        lr_a:               a float value indicating the learning rate of the actor, α_a\n",
    "        lr_c:               a float value indicating the learning rate of the critic, α_c\n",
    "        layers:             an int value indicating the number of layers in a network\n",
    "        neurons:            an int value indicating the number of neurons per layer\n",
    "        seed:               an int value indicating the desired seed, for use in randomization and reproducability\n",
    "\n",
    "        nS:                 an int representing the number of states observed, each of which is continuous\n",
    "        nA:                 an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        actor_network:      a Keras sequential neural network representing the actor\n",
    "        critic_network:     a Keras sequential neural network representing the actor\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.n\n",
    "\n",
    "        # initialize the networks:\n",
    "        self.actor_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_a,\n",
    "                                        norm = True,\n",
    "                                        drop = True,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = self.nA,\n",
    "                                        loss_function = \"categorical_crossentropy\",\n",
    "                                        output_activation = \"softmax\")\n",
    "        \n",
    "        self.critic_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_c,\n",
    "                                        norm = True,\n",
    "                                        drop = True,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = self.nA,\n",
    "                                        loss_function = \"mse\",\n",
    "                                        output_activation = \"linear\")\n",
    "        \n",
    "        # set the seed:\n",
    "        self.seed = seed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
