{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dd5a3d",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for implementing a vanilla Actor-Critic method based on the advantage function (A2C), for learning an optimal policy for the `Cartpole` environment. The Actor-Critic method utilizes two networks, one of which is responsible for mapping states to a probability distribution over the actions (actor), and another which estimates the value of a state to guide the actor (critic). The general idea is that the actor updates its policy in the direction suggested by the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ec306",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6561ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61802222",
   "metadata": {},
   "source": [
    "# **Environment Setup**\n",
    "\n",
    "This section sets up the environment and defines the relevant functions needed for this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d0b87",
   "metadata": {},
   "source": [
    "##### Function for making Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making a keras model:\n",
    "def make_model(layers, neurons, rate, norm, drop, input_shape, output_shape, loss_function, output_activation):\n",
    "    # instantiate model:\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add hidden layers:\n",
    "    for i in range(layers):\n",
    "        if i == 0:\n",
    "            model.add(Input(shape = (input_shape, )))\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation = 'relu', name = f'hidden_layer_{i+1}'))\n",
    "\n",
    "        if norm == True:\n",
    "            model.add(BatchNormalization(name = f'batch_norm_layer_{i+1}'))\n",
    "\n",
    "        if drop == True:\n",
    "            model.add(Dropout(0.2, name = f'dropout_layer_{i+1}'))\n",
    "    \n",
    "    # add output layer:\n",
    "    model.add(Dense(output_shape, activation = output_activation, name = 'output_layer'))\n",
    "\n",
    "    # compile the model:\n",
    "    model.compile(optimizer = Adam(learning_rate = rate),\n",
    "                  loss = loss_function)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f5c33",
   "metadata": {},
   "source": [
    "##### A2C class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf864ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C class:\n",
    "class A2C_Agent:\n",
    "    ####################### INITIALIZATION #######################\n",
    "    # constructor:\n",
    "    def __init__(self,\n",
    "                env: gym.Env,\n",
    "                gamma: float, \n",
    "                lr_a: float, \n",
    "                lr_c: float,\n",
    "                layers = int,\n",
    "                neurons = int,\n",
    "                seed = int):\n",
    "        \"\"\" \n",
    "        this is the constructor for the agent. this agent uses the advantage actor-critic (A2C) algorithm to learn an optimal policy,\n",
    "        through the use of two approximator networks. the first network, called the actor, is responsible for providing the probabilty \n",
    "        distribution over all actions given a state. the second network, called the critic, is responsible for utilizing the advantage function\n",
    "        to guide the learning of the actor.\n",
    "\n",
    "        env:                a gymnasium environment\n",
    "        gamma:              a float value indicating the discount factor, γ\n",
    "        lr_a:               a float value indicating the learning rate of the actor, α_a\n",
    "        lr_c:               a float value indicating the learning rate of the critic, α_c\n",
    "        layers:             an int value indicating the number of layers in a network\n",
    "        neurons:            an int value indicating the number of neurons per layer\n",
    "        seed:               an int value indicating the desired seed, for use in randomization and reproducability\n",
    "\n",
    "        nS:                 an int representing the number of states observed, each of which is continuous\n",
    "        nA:                 an int representing the number of discrete actions that can be taken\n",
    "\n",
    "        actor_network:      a Keras sequential neural network representing the actor\n",
    "        critic_network:     a Keras sequential neural network representing the actor\n",
    "\n",
    "        \"\"\"\n",
    "        # object parameters:\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "\n",
    "        # get the environment dimensions:\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.n\n",
    "\n",
    "        # initialize the networks:\n",
    "        self.actor_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_a,\n",
    "                                        norm = True,\n",
    "                                        drop = True,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = self.nA,\n",
    "                                        loss_function = \"categorical_crossentropy\",\n",
    "                                        output_activation = \"softmax\")\n",
    "        \n",
    "        self.critic_network = make_model(layers = layers,\n",
    "                                        neurons = neurons,\n",
    "                                        rate = lr_c,\n",
    "                                        norm = True,\n",
    "                                        drop = True,\n",
    "                                        input_shape = self.nS,\n",
    "                                        output_shape = 1,\n",
    "                                        loss_function = \"mse\",\n",
    "                                        output_activation = \"linear\")\n",
    "        \n",
    "        # set the seed:\n",
    "        self.seed = seed\n",
    "    \n",
    "    ####################### TRAINING #######################\n",
    "    # function for updating actor:\n",
    "    @tf.function\n",
    "    def update_actor(self, state, action, advantage):\n",
    "        # enforce dimensionality:\n",
    "        state = tf.convert_to_tensor(state, dtype = tf.float32)\n",
    "        state = tf.reshape(state, [1, self.nS])                 \n",
    "\n",
    "        action = tf.reshape(tf.cast(action, tf.int32), [-1])                \n",
    "\n",
    "        advantage = tf.stop_gradient(tf.cast(tf.reshape(advantage, [1]), tf.float32))\n",
    "\n",
    "        # track auto differentiation:\n",
    "        with tf.GradientTape() as tape_a:\n",
    "            # 1) compute the loss for the actor:\n",
    "            action_probs = self.actor_network(state)\n",
    "            index = tf.stack([tf.range(tf.shape(action)[0]), action], axis=1)\n",
    "            chosen_prob = tf.gather_nd(action_probs, index)\n",
    "\n",
    "            # 2) compute policy loss:\n",
    "            logp = tf.math.log(chosen_prob + 1e-8)\n",
    "            policy_loss = -tf.reduce_mean(logp * advantage)\n",
    "\n",
    "        # 3) backpropagate and update weights:\n",
    "        grads = tape_a.gradient(policy_loss, self.actor_network.trainable_variables)\n",
    "        self.actor_network.optimizer.apply_gradients(zip(grads, self.actor_network.trainable_variables))\n",
    "\n",
    "    # function for updating critic:\n",
    "    @tf.function\n",
    "    def update_critic(self, state, target):\n",
    "        # enforce dimensionality:\n",
    "        state = tf.convert_to_tensor(state, dtype = tf.float32)\n",
    "        state = tf.reshape(state, [1, self.nS])             \n",
    "\n",
    "        # freeze gradient on targets:\n",
    "        target = tf.stop_gradient(tf.cast(tf.reshape(target, [1, 1])), tf.float32)\n",
    "\n",
    "        # track auto differentiation:\n",
    "        with tf.GradientTape() as tape_c:\n",
    "            # 1) compute values so tape knows which weights to differentiate:\n",
    "            value = self.critic_network(state)\n",
    "\n",
    "            # 2) compute loss:\n",
    "            critic_loss = tf.reduce_mean(tf.square(target - value))\n",
    "\n",
    "        # 3) backpropagate and update the weights:\n",
    "        grads = tape_c.gradient(critic_loss, self.critic_network.trainable_variables)\n",
    "        self.critic_network.optimizer.apply_gradients(zip(grads, self.critic_network.trainable_variables))\n",
    "\n",
    "    # training function:\n",
    "    def training(self, training_length):\n",
    "        # initialize reward history:\n",
    "        reward_history = np.zeros(training_length)\n",
    "\n",
    "        # for every episode:\n",
    "        for episode in tqdm(range(training_length), colour = \"#33FF00\", ncols = 100, desc = \"training progress\"):\n",
    "            # get initial state:\n",
    "            obs, _ = self.env.reset()\n",
    "            obs = tf.convert_to_tensor(obs[None, :], dtype = tf.float32) \n",
    "\n",
    "            # counter for reward earned this episode\n",
    "            episode_reward = 0\n",
    "\n",
    "            # flag for completion:\n",
    "            done = False\n",
    "\n",
    "            # while false:\n",
    "            while not done:\n",
    "                # 1) pick an action from the actor network output:\n",
    "                action_probs = self.actor_network(obs, training = False)\n",
    "                # print(f\"action probs are: {action_probs}\")\n",
    "                action = tf.random.categorical(tf.math.log(action_probs), 1)\n",
    "                # print(f\"action is {action}\")\n",
    "\n",
    "                # 2) critic predicts value of state:\n",
    "                value = self.critic_network(obs, training = False)\n",
    "                print(f\"value of current state is: {value}\")\n",
    "\n",
    "                # 3) get next state, reward:\n",
    "                next_obs, reward, term, trunc, _ = self.env.step(action)\n",
    "                next_obs = tf.convert_to_tensor(next_obs[None, :], dtype = tf.float32) \n",
    "\n",
    "                done = term or trunc\n",
    "                episode_reward += reward\n",
    "\n",
    "                # 4) compute TD target, advantage:\n",
    "                next_value = self.critic_network(next_obs, training = False)\n",
    "                target = reward + (1-done) * self.gamma * next_value\n",
    "                advantage = target - value\n",
    "\n",
    "                # 5) update critic:\n",
    "                self.update_critic(obs, target)\n",
    "\n",
    "                # 6) update actor:\n",
    "                self.update_actor(obs, action, advantage)\n",
    "\n",
    "                # 7) advance values:\n",
    "                obs = next_obs\n",
    "            \n",
    "            # advance reward history:\n",
    "            reward_history[episode] = episode_reward\n",
    "        \n",
    "        # return to user:\n",
    "        return reward_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115cd0c",
   "metadata": {},
   "source": [
    "# **Using the Environment**\n",
    "\n",
    "This section utilizes the above A2C implementation to create an environment and train an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa254a0",
   "metadata": {},
   "source": [
    "##### Specify hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent hyperparameters:\n",
    "lr_a = 1e-3             # learning rate for the actor, α_a\n",
    "lr_c = 1e-3             # learning rate for the actor, α_c\n",
    "gamma = 0.99            # discount factor γ\n",
    "train_length = 50     # number of episodes to train for\n",
    "\n",
    "layers = 2      # how many layers to have in each network\n",
    "neurons = 32    # how many neurons to have in each layer of each network\n",
    "\n",
    "seed = 18       # seed for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbec82",
   "metadata": {},
   "source": [
    "##### Initialize the environment and agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aacd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment:\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# clear the backend:\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# instantiate the agent:\n",
    "ac_agent = A2C_Agent(env = env,\n",
    "                    lr_a = lr_a,\n",
    "                    lr_c = lr_c,\n",
    "                    gamma = gamma,\n",
    "                    layers = layers,\n",
    "                    neurons = neurons, \n",
    "                    seed = seed\n",
    "                    )\n",
    "\n",
    "_, _ = ac_agent.env.reset(seed = ac_agent.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ceca6f",
   "metadata": {},
   "source": [
    "##### Train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ed266",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = ac_agent.training(training_length = train_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
