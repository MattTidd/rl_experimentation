{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b392785",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook is for performing hyperparameter sweeps on the algorithms that have been designed thus far, for the ``cartpole`` environment in Gymnasium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbda1ae",
   "metadata": {},
   "source": [
    "# **Import Packages**\n",
    "\n",
    "This section imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fb51c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these packages:\n",
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "from cartpole_classes import DQN_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca16db",
   "metadata": {},
   "source": [
    "# **Environment Definition**\n",
    "\n",
    "This section defines the relevant objects and functions to perform the hyperparameter sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76798da",
   "metadata": {},
   "source": [
    "##### Sweep parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35b56e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to be evaluated:\n",
    "model_name = \"DQN\"\n",
    "\n",
    "# set the environment:\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# agent hyperparameters:\n",
    "grid = {\"lr\" : [1e-5, 5e-5, 1e-4],                          # learning rate α\n",
    "        \"buffer_size\" : [2000, 5000, 10000],                # size of the replay buffer\n",
    "        \"batch_size\" : [32, 64],                            # amount sampled from buffer\n",
    "        \"target_update_freq\" : [1000, 2500],                # number of steps to update target network\n",
    "        \"neurons\" : [64, 128],                              # how many neurons to have in each layer of the network\n",
    "        \"layers\" : [2, 3]}                                  # how many layers to have in each network\n",
    "\n",
    "# cartesian product of all combinations:\n",
    "keys, values = zip(*grid.items())\n",
    "combos = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# set the sweep conditions:\n",
    "training_length = 2000                          # length of training\n",
    "threshold = 450                                 # desired reward threshold\n",
    "consecutive_episodes = 5                        # desired consecutive number of episodes above threshold\n",
    "last_n = 50                                     # how many previous episodes to examine\n",
    "window_size = 20                                # window size for moving average of reward\n",
    "out_dir = f\"sweep_results/{model_name}\"         # output directory\n",
    "os.makedirs(out_dir, exist_ok = True)           # make that a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fbdf2",
   "metadata": {},
   "source": [
    "# **Hyperparameter Sweeping**\n",
    "\n",
    "This section performs the hyperparameter sweeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d11af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;51;255;0m██████████████████████████████████████████████████████████\u001b[0m| 144/144 [7:12:24<00:00, 180.17s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for every combination of params:\n",
    "for params in tqdm(combos, colour = \"#33FF00\", ncols = 100):\n",
    "    # 1) create a filename and path:\n",
    "    base = \"_\".join(f\"{k}{v}\" for k,v in params.items())\n",
    "    run_dir = os.path.join(out_dir, base)\n",
    "    os.makedirs(run_dir, exist_ok = True)\n",
    "\n",
    "    weights_path    = os.path.join(run_dir, \"model_weights.weights.h5\")\n",
    "    metrics_path    = os.path.join(run_dir, \"metrics.json\")\n",
    "    rewards_path    = os.path.join(run_dir, \"reward_history.csv\")\n",
    "\n",
    "    # 2) reset TF and the agent:\n",
    "    tf.keras.backend.clear_session()\n",
    "    dqn_agent = DQN_Agent(env, gamma = 0.99, epsilon = 1.0, epsilon_min = 0.1, epsilon_decay = 0.999, **params)\n",
    "\n",
    "    # 3) run episodes, record the rewards received:\n",
    "    reward_history = []\n",
    "    consec_counter = 0\n",
    "    stopped_ep = None\n",
    "    ep_to_thresh = None\n",
    "    ma_queue = []\n",
    "\n",
    "    for ep in range(1, training_length + 1):\n",
    "        # train the agent for a single episode:\n",
    "        reward = dqn_agent.training(1)[0]\n",
    "        reward_history.append(reward)\n",
    "\n",
    "        # maintain sliding window for moving average:\n",
    "        ma_queue.append(reward)\n",
    "        if len(ma_queue) > window_size:\n",
    "            ma_queue.pop(0)\n",
    "        if ep_to_thresh is None and len(ma_queue) == window_size:\n",
    "            if np.mean(ma_queue) >= threshold:\n",
    "                ep_to_thresh = ep\n",
    "\n",
    "        if reward >= threshold:\n",
    "            consec_counter += 1\n",
    "            if consec_counter >= consecutive_episodes:\n",
    "                # save the model, and break training loop:\n",
    "                stopped_ep = ep\n",
    "                dqn_agent.q_network.save_weights(weights_path)\n",
    "                break\n",
    "        else:\n",
    "            consec_counter = 0\n",
    "\n",
    "    # dump reward history as .csv:\n",
    "    with open(rewards_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"reward\"])\n",
    "        for r in reward_history:\n",
    "            writer.writerow([r])\n",
    "\n",
    "    # compute metrics:\n",
    "    max_reward      = max(reward_history)           # the maximum reward received\n",
    "    last_n_rewards  = reward_history[-last_n:]      # the last n rewards received\n",
    "    last_mean     = float(np.mean(last_n_rewards))  # mean of the last n rewards\n",
    "    last_std      = float(np.std(last_n_rewards))   # std of the last n rewards\n",
    "\n",
    "    # 4) dump run record:\n",
    "    run_record = {\n",
    "        \"params\"            : params,\n",
    "        \"stopped_episode\"   : stopped_ep,\n",
    "        \"episode_to_thresh\" : ep_to_thresh,\n",
    "        \"max_reward\"        : max_reward,\n",
    "        \"last_mean\"         : last_mean,\n",
    "        \"last_std\"          : last_std,\n",
    "        \"weights_path\"      : weights_path if stopped_ep else None,\n",
    "    }\n",
    "\n",
    "    # write to file:\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(run_record, f, indent = 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
